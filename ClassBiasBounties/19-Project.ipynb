{"cells":[{"cell_type":"markdown","metadata":{"id":"jTsRpEhHyo2L"},"source":["# Bounty Hunting Project Notebook"]},{"cell_type":"markdown","metadata":{"id":"D6uYV2tSyo2Q"},"source":["Author: Francesca Marini"]},{"cell_type":"markdown","metadata":{"id":"2hfpYNe5yo2Q"},"source":["# Set Up\n","\n","First, we load in some helper files."]},{"cell_type":"code","source":["# pip installs\n","!pip install folktables\n","!pip install dill"],"metadata":{"id":"Of-6r8Yq0iIS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-Vunz4OzVf7","executionInfo":{"status":"ok","timestamp":1648656356867,"user_tz":240,"elapsed":20413,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}},"outputId":"3c5ee961-149b-49ca-c1a4-8e011c10b2b8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# directory structure\n","%cd drive/MyDrive/CIS\\ 523/Bias\\ Bounty/ClassBiasBounties/\n","!pwd\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PKx44oApz7fS","executionInfo":{"status":"ok","timestamp":1648656357492,"user_tz":240,"elapsed":629,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}},"outputId":"64cb187b-5a09-4424-d79c-f955ef184ee4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CIS 523/Bias Bounty/ClassBiasBounties\n","/content/drive/MyDrive/CIS 523/Bias Bounty/ClassBiasBounties\n"," 19-Project.ipynb       \u001b[0m\u001b[01;34mdata\u001b[0m/                     \u001b[01;34m__pycache__\u001b[0m/\n"," acsData.py             \u001b[01;34mdontlook\u001b[0m/                \u001b[01;34m'__pycache__ (1)'\u001b[0m/\n"," bountyHuntData.py      groupID_ghSubmission.py   README.txt\n"," bountyHuntWrapper.py   groupSettings.py          updater.py\n"," cscUpdater.py          model.py                  verifier.py\n"]}]},{"cell_type":"code","source":["# imports \n","import numpy as np\n","from sklearn import metrics\n","from sklearn.tree import DecisionTreeClassifier\n","import pandas as pd\n","from folktables import ACSDataSource, ACSEmployment, ACSIncome, ACSPublicCoverage, ACSMobility, ACSTravelTime\n","from sklearn.model_selection import train_test_split\n","import sys\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import dill as pickle\n","from pprint import pprint\n","import json"],"metadata":{"id":"Q3uNbZOCPTIm","executionInfo":{"status":"ok","timestamp":1648677600451,"user_tz":240,"elapsed":7,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"qBpfarnvwSok"}},{"cell_type":"code","source":["# Data structure for the decision list\n","# Simple decision lists have been tested. pointer version has not been. ###\n","\n","class DecisionlistNode:\n","    \"\"\"\n","\n","        Node of decision lists are objects with the following form\n","\n","            -----------------        ------------------\n","            |   predicate   | --0--> | right_child Node|\n","            -----------------        ------------------\n","                    |\n","                    1\n","                    |\n","                    v\n","                  Leaf\n","    \"\"\"\n","\n","    def __init__(self, predicate=None, leaf=None, right_child=None):\n","        \"\"\"\n","\n","        predicate: function from X to {0,1}. If not defined in input to initialization, will be defined as function that\n","         always returns 1\n","        leaf: classification function from X to {0,1}. If not defined in the initialization, will be defined as function\n","         that always returns 1.\n","        right_child: Node object\n","        \"\"\"\n","        self.predicate = predicate\n","        self.leaf = leaf\n","        self.right_child = right_child\n","\n","        if self.predicate is None:\n","            self.predicate = lambda x: 1\n","\n","        if self.leaf is None:\n","            self.leaf = lambda x: 1\n","\n","class DecisionList:\n","    \"\"\"\n","    A simple decision list object with standard traversal\n","\n","            -----------------        ------------------                  ------------------\n","            |   predicate 1  | --0--> |   predicate 2  | --0--> ... ---> |   predicate n  |  ---> 0\n","            -----------------        ------------------                  ------------------\n","                    |\t\t\t\t\t\t  |                                  |\n","                    1                         1                                  1\n","                    |                         |                                  |\n","                    v                         v                                  v\n","                  Leaf 1                    Leaf 2                             Leaf n\n","\n","    \"\"\"\n","\n","    def __init__(self, initial_model):\n","        self.head = DecisionlistNode(leaf=initial_model)\n","        self.curr_node = self.head\n","        self.predicates = [self.head.predicate]  # list of predicate functions for ease of access\n","        self.leaves = [self.head.leaf]  # list of leaf functions for eas of access\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Predicts the label of x according to traversal of the decision list.\n","        \"\"\"\n","        if self.curr_node.predicate(x) == 1:\n","            # set output to the leaf function evaluated at x\n","            out = self.curr_node.leaf(x)\n","            # reset current node to head\n","            self.curr_node = self.head\n","            return out\n","\n","        elif self.curr_node.right_child is not None:\n","            self.curr_node = self.curr_node.right_child\n","            return self.predict(x)\n","        else:\n","            # reset current node to head and output 0 with a warning\n","            print(\"Warning: reached end of pDL with no predicate succeeding, returning 0.\")\n","            return 0\n","\n","    def prepend(self, node):\n","        \"\"\"\n","        Prepends a new node to the head of the decision list.\n","\n","        node we want to                          current\n","        prepend                               decision list\n","        ---------                              -------------      -----------------------\n","        | node | --> node.right_child = None    | self.head | ---> | self.head.right_child| --> ...\n","        --------                               -------------      -----------------------\n","            |\t\t\t\t\t\t               |\n","            1                                      1\n","            |                                      |\n","            v                                      v\n","          leaf                                self.head.leaf\n","        \"\"\"\n","        node.right_child = self.head\n","        self.predicates.append(node.predicate)\n","        self.leaves.append(node.leaf)\n","        self.head = node\n","        self.curr_node = self.head\n","\n","class PointerDecisionListNode(DecisionlistNode):\n","    \"\"\"\n","\n","        Node of pointer decision lists are objects with the following form if catch_node = False\n","\n","            -----------------        ------------------\n","            |   predicate   | --0--> | right_child Node|\n","            -----------------        ------------------\n","                    |\n","                    1\n","                    |\n","                    v\n","                  Leaf\n","\n","        Or, if catch_node = True:\n","\n","            -----------------        ------------------\n","            |   predicate   | --0--> | right_child Node|\n","            -----------------        ------------------\n","                    |\n","                    1\n","                    |\n","                    v\n","            -----------------\n","            | right_main_node |\n","            -----------------\n","    \"\"\"\n","\n","    def __init__(self, predicate=None, leaf=None, right_child=None, catch_node=False, right_main_node=None,\n","                 pred_name=None):\n","        DecisionlistNode.__init__(self, predicate=predicate, leaf=leaf, right_child=right_child)\n","\n","        self.catch_node = catch_node\n","        self.right_main_node = right_main_node\n","        self.predName = pred_name\n","\n","        if catch_node:\n","            # assert(right_main_node.right_child is not None)\n","            self.leaf = None\n","\n","class PointerDecisionList:\n","    \"\"\"\n","                     -------------------------------------------------------------------------------\n","                    |                         |                         \t\t\t\t\t\t   |\n","                    1\t\t\t\t\t\t  1                         \t\t\t\t\t\t   v\n","                      |                         |                  right_main_node\t\t    right_main_node.right_child\n","            -----------------        ------------------         ------------------          ---------------           -----------------\n","            |   predicate 1  | --0--> |   predicate 2  | --0--> |   predicate 3  |  --0-->  | predicate 3 |    ...    |  predicate n  |  ---> 0\n","            -----------------        ------------------         ------------------          ---------------           ------------------\n","                catch_node    \t\t\t catch_node\t                    |                          |                          |\n","                                                                        1                          1                          1\n","                                                                        |                          |                          |\n","                                                                        v                          v                          v\n","                                                                           Leaf                       Leaf                        Leaf\n","\n","    \"\"\"\n","\n","    def __init__(self, initial_model, all_groups=None):\n","        \"\"\"\n","        initial_model is some fit function e.g. a .fit method output by scikit.learn,\n","        which takes as input a dataframe x and outputs labels y for those values of x.\n","\n","        If you already know the groups we'll be passing in, they can be specified with all_groups\n","        \"\"\"\n","        self.all_groups = None\n","        if all_groups is None:\n","            all_groups = []\n","        self.head = PointerDecisionListNode(leaf=initial_model, pred_name='Total')\n","        self.curr_node = self.head\n","        self.predicates = [self.head.predicate]  # list of predicate functions for ease of access\n","        self.pred_names = ['Total']\n","        self.leaves = [self.head.leaf]  # list of leaf functions for ease of access.\n","\n","        # keeping track of the group errors so far here (including groups that haven't been introduced yet, which\n","        # wouldn't happen IRL!) for computational efficiency.\n","        # only relevant if we already know the groups we're computing things on.\n","        if all_groups:\n","            n = len(all_groups)\n","            self.test_errors = np.empty(shape=(n, n))\n","            self.test_errors[:] = np.NaN  # filling with NaNs to avoid confusion\n","            self.train_errors = np.empty(shape=(n, n))\n","            self.train_errors[:] = np.NaN  # filling with NaNs to avoid confusion\n","        else:\n","            self.test_errors = []\n","            self.train_errors = []\n","\n","        # keeping track of the number of rounds so far\n","        self.num_rounds = 0\n","\n","        # keeping track of the final node belonging to each update so that we can point to those\n","        self.update_nodes = [self.head]\n","        self.track_rejects = [1]\n","        self.update_node_indices_tracking_rejects = [0]\n","\n","    def init_groups(self):\n","        n = len(self.all_groups)\n","        self.test_errors = np.empty(shape=(n, n))\n","        self.test_errors[:] = np.NaN  # filling with NaNs to avoid confusion\n","        self.train_errors = np.empty(shape=(n, n))\n","        self.train_errors[:] = np.NaN  # filling with NaNs to avoid confusion\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Predicts the label of x according to traversal of the pointer decision list.\n","        Note that this is NOT a vectorized function as it is currently written: only single values of\n","        x may be passed in at a time. This is very much not ideal for a pandas dataframe.\n","        \"\"\"\n","        # if catch_node evaluates to true, move to the right child of the main node and continue traversal\n","        if self.curr_node.catch_node is True:\n","            if self.curr_node.predicate(x) == 1:\n","                self.curr_node = self.curr_node.right_main_node\n","                return self.predict(x)\n","            else:\n","                self.curr_node = self.curr_node.right_child\n","                return self.predict(x)\n","        # otherwise, traverse as usual\n","        else:\n","            if self.curr_node.predicate(x) == 1:\n","                # set output to the leaf function evaluated at x\n","                # because we are passing in single rows of a larger dataframe at a time (which is not good in long run)\n","                # have to reshape x to be something that predict will accept.\n","                out = self.curr_node.leaf(np.array(x).reshape(1, -1))\n","                # reset current node to head\n","                self.curr_node = self.head\n","                # output\n","                return out[0]\n","            elif self.curr_node.right_child is not None:\n","                self.curr_node = self.curr_node.right_child\n","                return self.predict(x)\n","            else:\n","                # reset current node to head and output 0 with a warning\n","                print(\"Warning: reached end of pDL with no predicate succeeding, returning 0.\")\n","                return 0\n","\n","    def prepend(self, node):\n","        \"\"\"\n","        Prepends a new node to the head of the decision list\n","        \"\"\"\n","        if node.catch_node is True:\n","            # if prepending to another catch_node\n","            if self.head.catch_node is True:\n","                node.right_child = self.head\n","                self.head = node\n","                self.curr_node = self.head\n","            else:\n","                node.right_child = self.head\n","                self.head = node\n","                self.curr_node = self.head\n","        else:\n","            node.right_child = self.head\n","            self.predicates.append(node.predicate)\n","            self.pred_names.append(node.predName)\n","            self.leaves.append(node.leaf)\n","            self.head = node\n","            self.curr_node = self.head\n","\n","    def pop(self):\n","        \"\"\"\n","        Removes the head node from the decision list\n","        \"\"\"\n","        self.head = self.head.right_child\n","        self.curr_node = self.head"],"metadata":{"id":"yJx2ln75wUHF","executionInfo":{"status":"ok","timestamp":1648656361770,"user_tz":240,"elapsed":646,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Verifier"],"metadata":{"id":"BuBz5I_KyH4s"}},{"cell_type":"code","source":["def verify(curr_model, test_x, test_y, h, g, alpha=0.001):\n","    \"\"\"\n","    Updates the curr_model model object to incorporate the (h,g).\n","\n","    Inputs:\n","    curr_model: model object that is to be updated\n","    holdout_data: data to test the proposed new model on\n","    h_t: new model\n","    g_t: function from X -> {0,1} which returns 1 if x is in identified group and 0 else.\n","\n","    Return: None\n","    \"\"\"\n","    # pull the x and y values that belong to g\n","    indices = test_x.apply(g, axis=1) == 1\n","    xs = test_x[indices]\n","    ys = test_y[indices]\n","\n","    # get predicted ys from current model and proposed h\n","    curr_model_preds = xs.apply(curr_model.predict, axis=1)\n","    h_preds = h(xs)\n","\n","    # measure the error of current model and proposed h\n","    curr_model_error = metrics.zero_one_loss(ys, curr_model_preds)\n","    h_error = metrics.zero_one_loss(ys, h_preds)\n","\n","    # determine if (g,h) should be accepted or not\n","    group_weight = sum(indices) / float(len(test_x))\n","    improvement = curr_model_error - h_error\n","\n","    if group_weight * improvement >= alpha:\n","        return True\n","\n","    return False\n","\n","def is_proposed_group_good(curr_model, test_x, test_y, h, g, train_x, train_y):\n","    \"\"\"\n","    Checks that the group error of g on h isn't worse than g on f.\n","    Doesn't worry about weight of group\n","    Inputs:\n","    curr_model: model object that is to be updated\n","    holdout_data: data to test the proposed new model on\n","    h_t: new model\n","    g_t: function from X -> {0,1} which returns 1 if x is in identified group and 0 else.\n","\n","    Return: None\n","    \"\"\"\n","    # pull the x and y values that belong to g\n","    indices = test_x.apply(g, axis=1) == 1\n","    xs = test_x[indices]\n","    ys = test_y[indices]\n","\n","    # get predicted ys from current model and proposed h\n","    curr_model_preds = xs.apply(curr_model.predict, axis=1)\n","    h_preds = h(xs)\n","\n","    # measure the error of current model and proposed h\n","    curr_model_error = metrics.zero_one_loss(ys, curr_model_preds)\n","    h_error = metrics.zero_one_loss(ys, h_preds)\n","\n","    print(\"Error of current model on proposed group: %s\" % curr_model_error)\n","    print(\"Error of h trained on proposed group: %s\" % h_error)\n","    print(\"Group size in test set: %s\" % len(xs))\n","    print(\"Group weight in test set: %s\" % (len(xs)/len(test_x)))\n","\n","    ## REMOVE BEFORE DEPLOYMENT\n","    t_indices = train_x.apply(g, axis=1) == 1\n","    t_xs = train_x[t_indices]\n","    t_ys = train_y[t_indices]\n","\n","    # get predicted ys from current model and proposed h\n","    t_curr_model_preds = t_xs.apply(curr_model.predict, axis=1)\n","    t_h_preds = h(t_xs)\n","\n","    # measure the error of current model and proposed h\n","    t_curr_model_error = metrics.zero_one_loss(t_ys, t_curr_model_preds)\n","    t_h_error = metrics.zero_one_loss(t_ys, t_h_preds)\n","\n","    print(\"Training Error of current model on proposed group: %s\" % t_curr_model_error)\n","    print(\"Training Error of h trained on proposed group: %s\" % t_h_error)\n","    print(\"Group size in training set: %s\" % len(t_xs))\n","    print(\"Group weight in training set: %s\" % (len(t_xs)/len(train_x)))\n","\n","    if h_error >= curr_model_error:\n","        return False\n","\n","    else:\n","        return True\n","\n","def is_proposed_group_good_csc(curr_model, test_x, test_y, h, g):\n","    \"\"\"\n","    Checks that the group error of g on h isn't worse than g on f.\n","    Doesn't worry about weight of group\n","    Inputs:\n","    curr_model: model object that is to be updated\n","    holdout_data: data to test the proposed new model on\n","    h_t: new model\n","    g_t: function from X -> {0,1} which returns 1 if x is in identified group and 0 else.\n","\n","    Return: None\n","    \"\"\"\n","    # pull the x and y values that belong to g\n","    indices = test_x.apply(g, axis=1) == 1\n","    xs = test_x[indices]\n","    ys = test_y[indices]\n","\n","    # get predicted ys from current model and proposed h\n","    curr_model_preds = xs.apply(curr_model.predict, axis=1)\n","\n","    # some dumb reshaping to mesh with how h takes inputs\n","    def _h(x):\n","        _x = np.array(x).reshape(1, -1)\n","        return h(_x)[0]\n","\n","    h_preds = xs.apply(_h, axis=1)\n","\n","    # measure the error of current model and proposed h\n","    curr_model_error = metrics.zero_one_loss(ys, curr_model_preds)\n","    h_error = metrics.zero_one_loss(ys, h_preds)\n","\n","    print(\"Error of current model on proposed group: %s\" % curr_model_error)\n","    print(\"Error of h trained on proposed group: %s\" % h_error)\n","\n","    if h_error >= curr_model_error:\n","        return False\n","\n","    else:\n","        return True\n","\n","def check_group_sizes(test_x, group):\n","    # Returns True if the group has more than 0 elements in test_x\n","    indices = test_x.apply(group, axis=1) == 1\n","    if sum(indices) >= 1:\n","        return True\n","    else:\n","        return False"],"metadata":{"id":"ozUz0De8yHfs","executionInfo":{"status":"ok","timestamp":1648656363427,"user_tz":240,"elapsed":468,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## CSC Updater"],"metadata":{"id":"ahCCHR5czKLM"}},{"cell_type":"code","source":["# Global parameter that determines depth of the decision tree trained\n","dt_depth = 7\n","\n","# noinspection SpellCheckingInspection\n","def training_r0_r1(train_x, true_ys, pred_ys):\n","    \"\"\"\n","    The cost sensitive classification approach to the bias bounties problems takes in the training data and the\n","    predictions of the current model and builds two models, one for the cost of predicting 0 for a datapoint\n","    and one for the cost of predicting 1. This function generates these two functions, r0 and r1. It also\n","    outputs some additional information for debugging and visualization purposes.\n","\n","    Inputs:\n","    train_x: the training data\n","    train_y: the labels for the training data\n","    pred_y: the current model's predictions on the training data\n","\n","    \"\"\"\n","    # training r0\n","    # initialize cost vector to 0s\n","    cost_pred0 = np.zeros(len(true_ys))\n","    # find the locations where the current model predicts 1\n","    curr_predicts_1 = np.where(pred_ys == 1)\n","    # label the points where the current model and the true labels are both 1 w cost of 1\n","    cost_pred0[np.intersect1d(curr_predicts_1, np.where(true_ys == 1))] = 1\n","    # label the points where current model predicts 1 and true label is 0 w cost of -1\n","    cost_pred0[np.intersect1d(curr_predicts_1, np.where(true_ys == 0))] = -1\n","    # learn a regression model\n","    r0 = DecisionTreeClassifier(max_depth=dt_depth, random_state=0)\n","    r0.fit(train_x, cost_pred0)\n","\n","    # training r1. Same thing but flipped\n","    cost_pred1 = np.zeros(len(true_ys))\n","    curr_predicts_0 = np.where(pred_ys == 0)\n","    cost_pred1[np.intersect1d(curr_predicts_0, np.where(true_ys == 0))] = 1\n","    cost_pred1[np.intersect1d(curr_predicts_0, np.where(true_ys == 1))] = -1\n","    r1 = DecisionTreeClassifier(max_depth=dt_depth, random_state=0)\n","    r1.fit(train_x, cost_pred1)\n","\n","    return [r0, r1, np.array([true_ys, pred_ys, cost_pred0, cost_pred1])]\n","\n","def _g(r0, r1):\n","    \"\"\"\n","    Next in the CSC approach, we build a function g which decides on input x whether or not it should be included in the\n","    new group g that our algorithmic bounty hunter will be updating the model with or if it should send that point\n","    to the previous model.It does this by checking what the values of our two cost functions are at input x, and\n","    returns 1 if these costs are both negative.Otherwise, it returns 1. This function _g constructs such a g given\n","    the two cost functions r0 and r1, and outputs the g.\n","    \"\"\"\n","\n","    def g(x):\n","        x = np.array(x).reshape(1, -1)\n","        cost0 = r0.predict(x)\n","        cost1 = r1.predict(x)\n","        if cost0 < 0 or cost1 < 0:\n","            return 1\n","        else:\n","            return 0\n","\n","    return g\n","\n","# noinspection SpellCheckingInspection\n","def _h(r0, r1):\n","    \"\"\"\n","    Next, we build a model h for the elements in g. We return a 1 or True for them if the cost of predicting True is\n","    less than the cost of predicting 1, and otherwise return 0 or False. h's return values are in brackets to match\n","    the way that scikit learn usually is done over batches of data points (which we cannot do here unless we are sure\n","    that all of the datapoints will end up with the same path through our pointer decision list.)\n","    \"\"\"\n","\n","    def h(x):\n","        cost0 = r0.predict(x)\n","        cost1 = r1.predict(x)\n","        if cost0 < cost1:\n","            return [False]  # have to stick these in brackets to match syntax of sklearn prediction functions\n","        else:\n","            return [True]\n","\n","    return h\n","\n","#################################################################################################\n","# The rest of the functions in this file are for generating the actual updates to our model.\n","# They are slight variations on the functions in Updater.py, because while there we were running\n","# experiments on groups that were pre-defined, here we are generating the groups on the fly.\n","##################################################################################################\n","\n","# noinspection SpellCheckingInspection\n","def measure_group_errors(model, X, y):\n","    \"\"\"\n","    Helper function that measures the group errors of groups defined in model over test data X with true\n","    labels y\n","\n","    Inputs:\n","    model: DecisionList or PointerDecisionList object\n","    X: n x m dataframe of test data\n","    y: dataframe of n true labels (or optimal predictions) of points in X\n","    \"\"\"\n","    indices = [X.apply(g, axis=1) == 1 for g in model.predicates]\n","    xs = [X[i] for i in indices]\n","    ys = [y[i] for i in indices]\n","    group_errors = []\n","    for i in range(len(model.predicates)):\n","        pred_ys = xs[i].apply(model.predict, axis=1)\n","        group_errors.append(metrics.zero_one_loss(np.array(ys[i]), np.array(pred_ys)))\n","    return group_errors\n","\n","# noinspection SpellCheckingInspection\n","def measure_group_error(model, group, X, y):\n","    \"\"\"\n","    Function to measure group errors of a specific group\n","\n","    NOTE THIS WILL BREAK IF YOU PASS IN AN EMPTY GROUP\n","    \"\"\"\n","\n","    indices = X.apply(group, axis=1) == 1\n","    xs = X[indices]\n","    ys = y[indices]\n","    pred_ys = xs.apply(model.predict, axis=1).to_numpy()\n","    group_errors = metrics.zero_one_loss(ys, pred_ys)\n","\n","    return group_errors\n","\n","\n","# noinspection SpellCheckingInspection\n","def all_group_errors(curr_model, group_pred, X, y):  # gets errors for each model for a given group\n","    # go through all the updates and get each group error for that group\n","    true_head = curr_model.head\n","    errs = []\n","    for i in range(len(curr_model.update_nodes)):\n","        curr_model.head = curr_model.update_nodes[i]\n","        curr_model.curr_node = curr_model.head\n","        errs.append(measure_group_error(curr_model, group_pred, X, y))\n","    curr_model.head = true_head\n","    curr_model.curr_node = true_head\n","    return errs\n","\n","def update_errors(model, group, train_x, train_y, test_x, test_y):\n","    # measure the group's train and test error over all previous versions of the PDF\n","    # then append as column to the model's train and test error arrays\n","\n","    # first, store where the current model is at\n","    true_head = model.head\n","    test_errs, train_errs = [], []\n","    # next, iterate through all previous models by changing the head node\n","    for i in range(len(model.update_nodes)):\n","        model.head = model.update_nodes[i]\n","        model.curr_node = model.head\n","        # calculate the train and test errors\n","        train_errs.append(measure_group_error(model, group, train_x, train_y))\n","        test_errs.append(measure_group_error(model, group, test_x, test_y))\n","    # reset the current node and head to the newest model\n","    model.head = true_head\n","    model.curr_node = true_head\n","\n","    # now, append the train and test errors to their trackers.\n","    [model.test_errors[i].append(test_errs[i]) for i in range(len(model.test_errors))]\n","    [model.train_errors[i].append(train_errs[i]) for i in range(len(model.train_errors))]\n","\n","    return None\n","\n","def find_next_problem_node(curr_model, new_errors):\n","    \"\"\"\n","    Finds a node in the PDL which, after an update, now has worse group error than it had previously.\n","\n","    curr_model: the current PDL\n","    new_errors: the errors introduced on each group by the newest update\n","\n","    Return: [returnIndex, return_model] where returnIndex is the index of the node that had worse error and\n","    return_model is the model that performed best on that node.\n","    \"\"\"\n","    # initialize the returns to -1 so we can easily tell if they weren't updated\n","    return_index = -1\n","    return_model = -1\n","\n","    for node_index in range(len(curr_model.update_nodes)):\n","        # pull out the column of errors that corresponds to this nodes error at each round of updates\n","        nodes_errors = [curr_model.test_errors[i][node_index] for i in range(len(curr_model.test_errors))]\n","        # find the round that minimizes the error\n","        indices_min_round = np.nanargmin(nodes_errors)\n","        # grab the value of the minimal error\n","        min_val = curr_model.test_errors[indices_min_round][node_index]\n","        # check if the minimal value is less than the new model's error on that group\n","        if min_val < new_errors[node_index]:\n","            # if the min error of a previously found group is better, report the node and\n","            # the model that was best for it, and break out of the loop.\n","            return_index = node_index\n","            return_model = indices_min_round\n","            break\n","\n","    return [return_index, return_model]\n","\n","# noinspection SpellCheckingInspection\n","def iterative_update(curr_model, h_t, g_t, train_X, train_y, test_X, test_y, group_name):\n","    \"\"\"\n","    Updates the curr_model to incorporate (g_t, h_t) in a way that preserves group error\n","    monotonicity over the sample data X with labels y\n","\n","    Inputs:\n","    curr_model: PointerDecisionList object that is to be updated\n","    h_t: new model that performs better than curr_model on points for which g_t returns 1\n","    g_t: function from X -> {0,1} which returns 1 if x is in identified group and 0 else.\n","\n","    Return: None\n","    \"\"\"\n","    # add a round to the round tracker\n","    curr_model.num_rounds += 1\n","\n","    # prepend the node\n","    new_node = PointerDecisionListNode(predicate=g_t, leaf=h_t, pred_name=group_name)\n","    curr_model.prepend(new_node)\n","\n","    # add a column to the train and test errors to track how the new group did at all previous rounds\n","    print(\"updating errors\")\n","    update_errors(curr_model, g_t, train_X, train_y, test_X, test_y)\n","\n","    # measure new group errors and compare to old\n","    print(\"getting new errors\")\n","    new_errors = measure_group_errors(curr_model, test_X, test_y)\n","\n","    # recursively check for new errors\n","\n","    [problem_node_index, problem_node_model_index] = find_next_problem_node(curr_model, new_errors)\n","    problem_node_tracking = []\n","\n","    while True:\n","        print(\"prob node\", problem_node_index)\n","        print(\"new errs\", new_errors)\n","\n","        if problem_node_model_index == -1:\n","            break\n","\n","        else:\n","            # add node to tracker so we can visualize PDL\n","            problem_node_tracking.append([curr_model.pred_names[problem_node_index], problem_node_model_index])\n","            # build a node that points to that model\n","            new_node = PointerDecisionListNode(predicate=curr_model.predicates[problem_node_index], catch_node=True,\n","                                               right_main_node=curr_model.update_nodes[problem_node_model_index])\n","            # prepend that node to the model\n","            curr_model.prepend(new_node)\n","            # the group of new model will change w new node appended so check those\n","            new_errors = measure_group_errors(curr_model, test_X, test_y)\n","            # check for further/new problem nodes\n","            [problem_node_index, problem_node_model_index] = find_next_problem_node(curr_model, new_errors)\n","\n","    if new_errors is None:\n","        curr_model.pop()  # remove the new model from the head of the pDL\n","        return \"Could not calculate all group errors and cannot update\"\n","\n","    # now that all of the updates have happened, add the final node of the update to the model\n","    curr_model.update_nodes.append(new_node)\n","\n","    curr_model.train_errors.append(measure_group_errors(curr_model, train_X, train_y))\n","    curr_model.test_errors.append(measure_group_errors(curr_model, test_X, test_y))\n","\n","    return [curr_model.train_errors, curr_model.test_errors]"],"metadata":{"id":"IpEFgkkHzJ7S","executionInfo":{"status":"ok","timestamp":1648656365336,"user_tz":240,"elapsed":1161,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Bounty Hunt Wrapper"],"metadata":{"id":"dBhFoKlbvj9_"}},{"cell_type":"code","source":["def build_model(x, y, group_function, dt_depth):\n","    print(\"building h\")\n","    # learn the indices first, since this is an inefficient operation\n","    indices = x.apply(group_function, axis=1) == 1\n","\n","    # then pull the particular rows from the dataframe\n","    training_xs = x[indices]\n","    training_ys = y[indices]\n","\n","    dt = DecisionTreeClassifier(max_depth=dt_depth, random_state=0)  # setting random state for replicability\n","    dt.fit(training_xs.values, training_ys) # added .values to get warning to not print out\n","    print(\"finished building h\")\n","    return dt.predict\n","\n","def build_initial_pdl(initial_model, train_x, train_y, validation_x, validation_y):\n","    f = PointerDecisionList(initial_model.predict) #model.PointerDecisionList(initial_model.predict)\n","    # manually stick in the train and test errors because I'm dumb and have codependencies in the files\n","    f.test_errors.append(measure_group_errors(f, validation_x, validation_y)) #cscUpdater.measure_group_errors(f, validation_x, validation_y))\n","    f.train_errors.append(measure_group_errors(f, train_x, train_y)) #cscUpdater.measure_group_errors(f, train_x, train_y))\n","    return f\n","\n","def verify_size(x, group):\n","    # helper function that checks that the discovered group isn't too small to run on\n","    indices = x.apply(group, axis=1) == 1\n","    xs = x[indices]\n","    if len(xs) == 0:\n","        return False\n","    else:\n","        return True\n","\n","def run_checks(f, validation_x, validation_y, g, h, train_x, train_y):\n","    size_check = verify_size(validation_x, g)\n","    if not size_check:  # Remove before deployment:\n","        print(\"Group has 0 weight in test set\")\n","        indices = train_x.apply(g, axis=1) == 1\n","        xs = train_x[indices]\n","        ys = train_y[indices]\n","\n","        # get predicted ys from current model and proposed h\n","        curr_model_preds = xs.apply(f.predict, axis=1)\n","        h_preds = h(xs)\n","\n","        # measure the error of current model and proposed h\n","        curr_model_error = metrics.zero_one_loss(ys, curr_model_preds)\n","        h_error = metrics.zero_one_loss(ys, h_preds)\n","\n","        print(\"Training Error of current model on proposed group: %s\" % curr_model_error)\n","        print(\"Training Error of h trained on proposed group: %s\" % h_error)\n","        print(\"Group size in training set: %s\" % len(xs))\n","        print(\"Group weight in training set: %s\" % (len(xs)/len(train_x)))\n","    if size_check:\n","        improvement_check = is_proposed_group_good(f, validation_x, validation_y, h, g, #verifier.is_proposed_group_good(f, validation_x, validation_y, h, g,\n","                                                            train_x, train_y)\n","        if improvement_check:\n","            print(\"Passed checks.\")\n","            return True\n","        else:\n","            print(\"Failed improvement check.\")\n","            return False\n","    else:\n","        print(\"Failed group size check.\")\n","        return False\n","\n","def measure_group_error(model, group, X, y):\n","    \"\"\"\n","    Function to measure group errors of a specific group\n","\n","    NOTE THIS WILL BREAK IF YOU PASS IN AN EMPTY GROUP\n","    \"\"\"\n","\n","    indices = X.apply(group, axis=1) == 1\n","    xs = X[indices]\n","    ys = y[indices]\n","    pred_ys = xs.apply(model.predict, axis=1).to_numpy()\n","    group_errors = metrics.zero_one_loss(ys, pred_ys)\n","\n","    return group_errors\n","\n","def run_updates(f, g, h, train_x, train_y, validation_x, validation_y, group_name=\"g\"):\n","    iterative_update(f, h, g, train_x, train_y, validation_x, validation_y, group_name) #cscUpdater.iterative_update(f, h, g, train_x, train_y, validation_x, validation_y, group_name)"],"metadata":{"id":"532VzGWn3Kyx","executionInfo":{"status":"ok","timestamp":1648656366733,"user_tz":240,"elapsed":300,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pMkqvN0Eyo2S"},"source":["Next, we load in the data. You should use `train_x` and `train_y` to train your models. The second set of data (`validation_x` and `validation_y`) is for testing your models, to ensure that you aren't overfitting. It is also what will be passed to the updater in order to determine if a proposed update should be accepted and if repairs are needed. Since you have access to this data, you could overfit to it and get a bunch of updates accepted. However, a) we'll be able to tell you did this and b) your updates will fail on the holdout set that only we have access to, so doing this is not in your best interest."]},{"cell_type":"markdown","source":["## Group Settings"],"metadata":{"id":"dTNz2XtH6LIs"}},{"cell_type":"code","source":["group_id = 19\n","acs_task = 'income'\n","acs_states = ['CA']"],"metadata":{"id":"JcqaegUIB2rM","executionInfo":{"status":"ok","timestamp":1648656368360,"user_tz":240,"elapsed":172,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Bounty Hunt Data"],"metadata":{"id":"tcNk7ca92Xvk"}},{"cell_type":"code","source":["sys.path.append(\"..\")\n","sys.path.append(\".\")\n","\n","# I did not edit any of the logic of the code, and I did not really look at what it was doing. \n","# for the sake of making everything work in Colab though, I did slightly edit the function headers to take in the group settings parameters passed in\n","\n","#### DO NOT LOOK AT THIS FILE PLEASE :) FOR RESEARCHY REASONS WE NEED TO PRETEND YOU DON'T HAVE ACCESS TO IT BUT\n","#### IRA COULDN'T FIGURE OUT HOW TO ENCRYPT IT IN A WAY THAT WOULD WORK ON EVERYONE'S SYSTEMS\n","\n","def get_data(acs_task, acs_states):\n","    acs_task = acs_task #groupSettings.acs_task\n","    acs_states = acs_states #groupSettings.acs_states\n","    test_size = 0.3\n","    acs_year = 2018\n","    acs_horizon = '1-Year'\n","    acs_survey = 'person'\n","    row_start = 0\n","    row_end = -1\n","    col_start = 0\n","    col_end = -1\n","    data_source = ACSDataSource(survey_year=acs_year, horizon=acs_horizon, survey=acs_survey)\n","    columns, features, label, group = [],[],[],[]\n","    # this pulls in the raw data\n","    acs_data = data_source.get_data(states=acs_states, download=True)\n","    # columns of the feature vector\n","    if acs_task == 'employment':\n","        # label is True if adult is employed\n","\n","        # columns of the feature vector\n","        columns = [\n","            'AGEP',\n","            'SCHL',\n","            'MAR',\n","            'RELP',\n","            'DIS',\n","            'ESP',\n","            'CIT',\n","            'MIG',\n","            'MIL',\n","            'ANC',\n","            'NATIVITY',\n","            'DEAR',\n","            'DEYE',\n","            'DREM',\n","            'SEX',\n","            'RAC1P',\n","        ]\n","        features, label, group = ACSEmployment.df_to_numpy(acs_data)\n","    elif acs_task == 'income':\n","        # label is True if US working adults’ yearly income is above $50,000\n","\n","        # columns of the feature vector\n","        columns = [\n","            'AGEP',\n","            'COW',\n","            'SCHL',\n","            'MAR',\n","            'OCCP',\n","            'POBP',\n","            'RELP',\n","            'WKHP',\n","            'SEX',\n","            'RAC1P',\n","        ]\n","        features, label, group = ACSIncome.df_to_numpy(acs_data)\n","    elif acs_task == 'public_coverage':\n","        # label True if low-income individual, not eligible for Medicare, has coverage from public health insurance.\n","\n","        # coluns of the feature vector\n","        columns = [\n","            'AGEP',\n","            'SCHL',\n","            'MAR',\n","            'SEX',\n","            'DIS',\n","            'ESP',\n","            'CIT',\n","            'MIG',\n","            'MIL',\n","            'ANC',\n","            'NATIVITY',\n","            'DEAR',\n","            'DEYE',\n","            'DREM',\n","            'PINCP',\n","            'ESR',\n","            'ST',\n","            'FER',\n","            'RAC1P',\n","        ]\n","        features, label, group = ACSPublicCoverage.df_to_numpy(acs_data)\n","    elif acs_task == 'mobility':\n","\n","        columns = [\n","            'AGEP',\n","            'SCHL',\n","            'MAR',\n","            'SEX',\n","            'DIS',\n","            'ESP',\n","            'CIT',\n","            'MIL',\n","            'ANC',\n","            'NATIVITY',\n","            'RELP',\n","            'DEAR',\n","            'DEYE',\n","            'DREM',\n","            'RAC1P',\n","            'GCL',\n","            'COW',\n","            'ESR',\n","            'WKHP',\n","            'JWMNP',\n","            'PINCP',\n","        ]\n","        # label True if a young adult moved addresses in the last year.\n","        features, label, group = ACSMobility.df_to_numpy(acs_data)\n","    elif acs_task == 'travel_time':\n","\n","        columns = [\n","            'AGEP',\n","            'SCHL',\n","            'MAR',\n","            'SEX',\n","            'DIS',\n","            'ESP',\n","            'MIG',\n","            'RELP',\n","            'RAC1P',\n","            'PUMA',\n","            'ST',\n","            'CIT',\n","            'OCCP',\n","            'JWTR',\n","            'POWPUMA',\n","            'POVPIP',\n","        ]\n","        # label True if a working adult has a travel time to work of greater than 20 minutes\n","        features, label, group = ACSTravelTime.df_to_numpy(acs_data)\n","    else:\n","        print(\"Invalid task\")\n","\n","    features = features[row_start:row_end, col_start:col_end]\n","    label = label[row_start:row_end]\n","    group = group[row_start:row_end]\n","    div = 2\n","    X_train, all_test_X, y_train, all_test_y, group_train, group_test = train_test_split(features, label, group,\n","                                                                                 test_size=test_size, random_state=10)\n","    X_train = np.hstack((X_train, group_train[:, np.newaxis]))\n","    all_test_X = np.hstack((all_test_X, group_test[:, np.newaxis]))\n","\n","    # making the training data into an actual pandas dataframe so that we can actually read it and see what things mean.\n","    X_train = pd.DataFrame(X_train, columns=columns)\n","    all_test_X = pd.DataFrame(all_test_X, columns=columns)\n","\n","    validation_x = all_test_X.iloc[:len(all_test_X)//div]\n","    validation_y = all_test_y[:len(all_test_y)//div]\n","\n","    return [X_train, y_train, validation_x, validation_y]"],"metadata":{"id":"w5cWFoJV2YQO","executionInfo":{"status":"ok","timestamp":1648656370805,"user_tz":240,"elapsed":408,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["[train_x, train_y, validation_x, validation_y] = get_data(acs_task=acs_task, acs_states=acs_states) #bountyHuntData.get_data()"],"metadata":{"id":"_i4I9094NfHz","executionInfo":{"status":"ok","timestamp":1648656387712,"user_tz":240,"elapsed":16909,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Build Decision Stump Initial Model"],"metadata":{"id":"xpEZOW9KMPvA"}},{"cell_type":"markdown","metadata":{"id":"eEx66mJoyo2U"},"source":["The model that you'll be building off of is a decision stump, i.e. a very stupid decision list with only one node. **Warning: do not rerun the next code block unless you want to completely restart building your PDL, as it will re-initialize it to just the decision stump!**"]},{"cell_type":"code","source":["initial_model = DecisionTreeClassifier(max_depth = 1, random_state=0)"],"metadata":{"id":"5q17a5HMIYke","executionInfo":{"status":"ok","timestamp":1648656389490,"user_tz":240,"elapsed":255,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["initial_model.fit(train_x.values, train_y) # added .values to train_x and this fixed a warning printout I was getting"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BprOSSCDIapc","executionInfo":{"status":"ok","timestamp":1648656389656,"user_tz":240,"elapsed":5,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}},"outputId":"ea5a6f16-2900-4805-d2f9-5d755905c627"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DecisionTreeClassifier(max_depth=1, random_state=0)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","execution_count":14,"metadata":{"pycharm":{"name":"#%%\n"},"id":"VABOLY0nyo2V","executionInfo":{"status":"ok","timestamp":1648656405473,"user_tz":240,"elapsed":15820,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"outputs":[],"source":["f = build_initial_pdl(initial_model, train_x, train_y, validation_x, validation_y) #bountyHuntWrapper.build_initial_pdl(initial_model, train_x, train_y, validation_x, validation_y)"]},{"cell_type":"markdown","metadata":{"id":"C_giE7Yoyo2V"},"source":["# Bounty Hunting\n","\n","Here's where the bulk of the work you'll be doing will live. Your job is to generate groups g such that there is some h that does better than the current model f on that group. Here, we generate an example group function, which identifies African American individuals."]},{"cell_type":"markdown","source":["## Defining Groups"],"metadata":{"id":"PHrsD-r0FP0x"}},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"ZUfyMH3Ryo2X"},"source":["You might also imaging making a group function that tries to learn what regions the current algorithm performs poorly on in an adaptive way, instead of just guessing ad-hoc that it will do poorly on a particular subgroup. In order to generate such a g, you will need to generate a constructor that takes as input a current model and the training data, and outputs a function g. A template for doing this is provided below. The example version returns a very silly function g which looks at the predictions the current model makes, and returns a group function where the group it has learned is all the points that the PDL labels as a 1. It completely ignores the true labels (train_y), so is probably not a very good group function."]},{"cell_type":"markdown","source":["### Adaptive Groups"],"metadata":{"id":"CxhOBtyj5kEc"}},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"c933GbD8yo2X"},"outputs":[],"source":["'''\n","def g_(f, train_x, train_y):\n","    # f is the current PDL\n","    preds = train_x.apply(f.predict, axis=1)\n","    merged_train = train_x.copy()\n","    merged_train['train_y'] = train_y\n","    merged_train['preds'] = preds\n","    # get all the values that we currently mislabel - could use this to find groups that have high error and try those\n","    mistakes = merged_train[merged_train['preds'] != merged_train['train_y']]\n","    xs = mistakes.drop(columns=['train_y', 'preds'])\n","    ys = mistakes['train_y']\n","    #xs = train_x[preds == 1] -- old lines\n","    #ys = train_y[preds == 1] -- old lines\n","    dt = DecisionTreeClassifier(max_depth = 10, random_state=0)\n","    dt.fit(xs, ys)\n","    def g(x):\n","        # g should take as input a SINGLE x and return 0 or 1 for it.\n","        # if we call dt.predict on x it will break because the dimensions of x are wrong, so we have to reshape it and reshape the output.\n","        # this is not particularly efficient, so if you have better ways of doing this go for it. :)\n","        y = dt.predict(np.array(x).reshape(1, -1))\n","        return y[0]\n","    return g\n","'''\n","\n","# if you wanted to build a particular g using the above, you could use the following line.\n","#g = g_(f, train_x, train_y)"]},{"cell_type":"code","source":["# possible group definitions\n","\n","# class of worker\n","def for_profit_worker(x):\n","  if x['COW'] == 1:\n","      return 1\n","  else:\n","      return 0\n","\n","def non_profit_worker(x):\n","  if x['COW'] == 2:\n","      return 1\n","  else:\n","      return 0\n","\n","def local_govt_worker(x):\n","  if x['COW'] == 3:\n","      return 1\n","  else:\n","      return 0\n","\n","def self_employed_worker(x):\n","  if x['COW'] == 6:\n","      return 1\n","  else:\n","      return 0\n","\n","def cow_minorities(x):\n","  if x['COW'] == 4:\n","      return 1\n","  elif x['COW'] == 5:\n","      return 1\n","  elif x['COW'] == 7:\n","      return 1\n","  elif x['COW'] == 8:\n","      return 1\n","  else:\n","      return 0\n","\n","# marital status\n","def married(x):\n","  if x['MAR'] == 1:\n","      return 1\n","  else:\n","      return 0\n","\n","def divorced(x):\n","  if x['MAR'] == 3:\n","      return 1\n","  else:\n","      return 0\n","\n","def mar_minorities(x):\n","  if x['MAR'] == 2:\n","      return 1\n","  elif x['MAR'] == 4:\n","      return 1\n","  else:\n","      return 0\n","\n","def single(x):\n","  if x['MAR'] == 5:\n","      return 1\n","  else:\n","      return 0\n","\n","# sex\n","def male(x):\n","  if x['SEX'] == 1:\n","      return 1\n","  else:\n","      return 0\n","\n","def female(x):\n","  if x['SEX'] == 2:\n","      return 1\n","  else:\n","      return 0\n","\n","# race\n","def white(x):\n","  if x['RAC1P'] == 1:\n","      return 1\n","  else:\n","      return 0\n","\n","def black(x):\n","  if x['RAC1P'] == 2:\n","      return 1\n","  else:\n","      return 0\n","\n","def other_race(x):\n","  if x['RAC1P'] == 9:\n","      return 1\n","  else:\n","      return 0\n","\n","def asian(x):\n","  if x['RAC1P'] == 6:\n","      return 1\n","  else:\n","      return 0\n","\n","def multiple_race(x):\n","  if x['RAC1P'] == 8:\n","      return 1\n","  else:\n","      return 0\n","\n","def native_american(x):\n","  if x['RAC1P'] == 3:\n","      return 1\n","  elif x['RAC1P'] == 4:\n","      return 1\n","  elif x['RAC1P'] == 5:\n","      return 1\n","  elif x['RAC1P'] == 7:\n","      return 1\n","  else:\n","      return 0\n","\n","# age groups\n","def twenties(x):\n","  if x['AGEP'] < 30:\n","      return 1\n","  else:\n","      return 0\n","\n","def thirties(x):\n","  if x['AGEP'] < 40 and x['AGEP'] >= 30:\n","      return 1\n","  else:\n","      return 0\n","\n","def forties(x):\n","  if x['AGEP'] < 50 and x['AGEP'] >= 40:\n","      return 1\n","  else:\n","      return 0\n","\n","def fifties(x):\n","  if x['AGEP'] < 60 and x['AGEP'] >= 50:\n","      return 1\n","  else:\n","      return 0\n","\n","def sixties(x):\n","  if x['AGEP'] < 70 and x['AGEP'] >= 60:\n","      return 1\n","  else:\n","      return 0\n","\n","def elderly(x):\n","  if x['AGEP'] >= 70:\n","      return 1\n","  else:\n","      return 0\n","\n","# education level\n","def no_school(x):\n","  if x['SCHL'] == 1:\n","    return 1\n","  else:\n","    return 0\n","\n","def some_school(x):\n","  if x['SCHL'] > 1 and x['SCHL'] <= 15:\n","    return 1\n","  else:\n","    return 0\n","\n","def high_school_grad(x):\n","  if x['SCHL'] > 15 and x['SCHL'] <= 19:\n","    return 1\n","  else:\n","    return 0\n","\n","def assoc_degree(x):\n","  if x['SCHL'] == 20:\n","    return 1\n","  else:\n","    return 0\n","\n","def assoc_degree(x):\n","  if x['SCHL'] == 20:\n","    return 1\n","  else:\n","    return 0\n","\n","def bachelor_degree(x):\n","  if x['SCHL'] == 21:\n","    return 1\n","  else:\n","    return 0\n","\n","def advanced_degree(x):\n","  if x['SCHL'] > 21:\n","    return 1\n","  else:\n","    return 0\n","\n","# work hours\n","def part_time(x):\n","  if x['WKHP'] < 30:\n","      return 1\n","  else:\n","      return 0\n","\n","def full_time(x):\n","  if x['WKHP'] < 60 and x['WKHP'] >= 30:\n","      return 1\n","  else:\n","      return 0\n","\n","def over_time(x):\n","  if x['WKHP'] >= 60:\n","      return 1\n","  else:\n","      return 0\n","\n","# occupation\n","def MGR(x):\n","  if x['OCCP'] <= 440:\n","    return 1\n","  else:\n","    return 0\n","\n","def BUS(x):\n","  if x['OCCP'] >= 500 and x['OCCP'] <= 750:\n","    return 1\n","  else:\n","    return 0\n","\n","def FIN(x):\n","  if x['OCCP'] >= 800 and x['OCCP'] <= 960:\n","    return 1\n","  else:\n","    return 0 \n","\n","def CMM(x):\n","  if x['OCCP'] >= 1005 and x['OCCP'] <= 1240:\n","    return 1\n","  else:\n","    return 0\n","\n","def ENG(x):\n","  if x['OCCP'] >= 1305 and x['OCCP'] <= 1560:\n","    return 1\n","  else:\n","    return 0\n","\n","def SCI(x):\n","  if x['OCCP'] >= 1600 and x['OCCP'] <= 1980:\n","    return 1\n","  else:\n","    return 0\n","\n","def CMS(x):\n","  if x['OCCP'] >= 2001 and x['OCCP'] <= 2060:\n","    return 1\n","  else:\n","    return 0\n","\n","def LGL(x):\n","  if x['OCCP'] >= 2105 and x['OCCP'] <= 2180:\n","    return 1\n","  else:\n","    return 0\n","\n","def EDU(x):\n","  if x['OCCP'] >= 2205 and x['OCCP'] <= 2555:\n","    return 1\n","  else:\n","    return 0\n","\n","def ENT(x):\n","  if x['OCCP'] >= 2600 and x['OCCP'] <= 2920:\n","    return 1\n","  else:\n","    return 0\n","\n","def MED(x):\n","  if x['OCCP'] >= 3000 and x['OCCP'] <= 3550:\n","    return 1\n","  else:\n","    return 0\n","\n","def HLS(x):\n","  if x['OCCP'] >= 3601 and x['OCCP'] <= 3655:\n","    return 1\n","  else:\n","    return 0\n","\n","def PRT(x):\n","  if x['OCCP'] >= 3700 and x['OCCP'] <= 3960:\n","    return 1\n","  else:\n","    return 0\n","\n","def EAT(x):\n","  if x['OCCP'] >= 4000 and x['OCCP'] <= 4160:\n","    return 1\n","  else:\n","    return 0\n","\n","def CLN(x):\n","  if x['OCCP'] >= 4200 and x['OCCP'] <= 4255:\n","    return 1\n","  else:\n","    return 0\n","\n","def PRS(x):\n","  if x['OCCP'] >= 4330 and x['OCCP'] <= 4655:\n","    return 1\n","  else:\n","    return 0\n","\n","def SAL(x):\n","  if x['OCCP'] >= 4700 and x['OCCP'] <= 4965:\n","    return 1\n","  else:\n","    return 0\n","\n","def OFF(x):\n","  if x['OCCP'] >= 5000 and x['OCCP'] <= 5940:\n","    return 1\n","  else:\n","    return 0\n","\n","def FFF(x):\n","  if x['OCCP'] >= 6005 and x['OCCP'] <= 6130:\n","    return 1\n","  else:\n","    return 0\n","\n","def CON(x):\n","  if x['OCCP'] >= 6200 and x['OCCP'] <= 6765:\n","    return 1\n","  else:\n","    return 0\n","\n","def EXT(x):\n","  if x['OCCP'] >= 6800 and x['OCCP'] <= 6950:\n","    return 1\n","  else:\n","    return 0\n","\n","def RPR(x):\n","  if x['OCCP'] >= 7000 and x['OCCP'] <= 7640:\n","    return 1\n","  else:\n","    return 0\n","\n","def PRD(x):\n","  if x['OCCP'] >= 7700 and x['OCCP'] <= 8990:\n","    return 1\n","  else:\n","    return 0\n","\n","def TRN(x):\n","  if x['OCCP'] >= 9005 and x['OCCP'] <= 9760:\n","    return 1\n","  else:\n","    return 0\n","\n","def MIL(x):\n","  if x['OCCP'] >= 9800 and x['OCCP'] <= 9830:\n","    return 1\n","  else:\n","    return 0\n","\n","# place of birth continents\n","def us_born(x):\n","  if x['POBP'] < 100:\n","    return 1\n","  else: \n","    return 0\n","\n","def european(x):\n","  if x['POBP'] < 200 and x['POBP'] >= 100:\n","    return 1\n","  else: \n","    return 0\n","\n","def asian(x):\n","  if x['POBP'] < 300 and x['POBP'] >= 200:\n","    return 1\n","  else: \n","    return 0\n","\n","def american(x):\n","  if x['POBP'] < 400 and x['POBP'] >= 300:\n","    return 1\n","  else: \n","    return 0\n","\n","def african(x):\n","  if x['POBP'] < 500 and x['POBP'] >= 400:\n","    return 1\n","  else: \n","    return 0\n","\n","def oceanian(x):\n","  if x['POBP'] < 600 and x['POBP'] >= 500:\n","    return 1\n","  else: \n","    return 0"],"metadata":{"id":"QjAIPYJu7d0P","executionInfo":{"status":"ok","timestamp":1648656411235,"user_tz":240,"elapsed":814,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["group_functions = [(for_profit_worker, 'for profit worker'), \n","                   (non_profit_worker, 'non profit worker'), \n","                   (local_govt_worker, 'local govt worker'), \n","                   (self_employed_worker, 'self employed worker'), \n","                   (cow_minorities, 'cow minorities'), \n","                   (married, 'married'), \n","                   (divorced, 'divorced'), \n","                   (mar_minorities, 'mar minorities'), \n","                   (single, 'single'), \n","                   (male, 'male'), \n","                   (female, 'female'), \n","                   (white, 'white'), \n","                   (black, 'black'), \n","                   (other_race, 'other race'),\n","                   (asian, 'asian'),\n","                   (multiple_race, 'multiple race'), \n","                   (native_american, 'native american')]"],"metadata":{"id":"rBiOkTH47h-m","executionInfo":{"status":"ok","timestamp":1648656419780,"user_tz":240,"elapsed":122,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["group_functions2 = [\n","          (twenties, 'twenties'),\n","          (thirties, 'thirties'),\n","          (forties, 'forties'),\n","          (fifties, 'fifties'),\n","          (sixties, 'sixties'),\n","          (elderly, 'elderly'),\n","          (part_time, 'part time'),\n","          (full_time, 'full time'),\n","          (over_time, 'over time'),\n","          (no_school, 'no school'),\n","          (some_school, 'some school'),\n","          (high_school_grad, 'high school grad'),\n","          (assoc_degree, 'associates degree'),\n","          (bachelor_degree, 'bachelors degree'),\n","          (advanced_degree, 'advanced degree')          \n","]"],"metadata":{"id":"Z90AxVaM7j6Z","executionInfo":{"status":"ok","timestamp":1648656421467,"user_tz":240,"elapsed":208,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["occupation_functions = [\n","            (MGR, 'MGR'),\n","            (BUS, 'BUS'),\n","            (FIN, 'FIN'),\n","            (CMM, 'CMM'),\n","            (ENG, 'ENG'),\n","            (SCI, 'SCI'),\n","            (CMS, 'CMS'),\n","            (LGL, 'LGL'),\n","            (EDU, 'EDU'),\n","            (ENT, 'ENT'),\n","            (MED, 'MED'),\n","            (HLS, 'HLS'),\n","            (PRT, 'PRT'),\n","            (EAT, 'EAT'),\n","            (CLN, 'CLN'),\n","            (PRS, 'PRS'),\n","            (SAL, 'SAL'),\n","            (OFF, 'OFF'),\n","            (FFF, 'FFF'),\n","            (CON, 'CON'),\n","            (EXT, 'EXT'),\n","            (RPR, 'RPR'),\n","            (PRD, 'PRD'),\n","            (TRN, 'TRN'),\n","            (MIL, 'MIL')            \n","]"],"metadata":{"id":"EuZN6Kpo7lab","executionInfo":{"status":"ok","timestamp":1648656423075,"user_tz":240,"elapsed":116,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["continent_functions = [\n","                       (us_born, 'US born'),\n","                       (european, 'european'),\n","                       (asian, 'asian'),\n","                       (american, 'american'),\n","                       (african, 'african'),\n","                       (oceanian, 'oceanian')\n","]"],"metadata":{"id":"XITNedzr7nAN","executionInfo":{"status":"ok","timestamp":1648656425407,"user_tz":240,"elapsed":121,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# rank each of these groups according to their error from the decision stump\n","funcs = [group_functions, group_functions2, occupation_functions, continent_functions]\n","ranking = []\n","for func in funcs:\n","  for fun, name in func:\n","    err = measure_group_error(f, fun, train_x, train_y)\n","    ranking.append((fun, name, err))\n","ranking.sort(key = lambda x: x[2])\n","ranking.reverse()"],"metadata":{"id":"4MxScj5r7p5M","executionInfo":{"status":"ok","timestamp":1648656645435,"user_tz":240,"elapsed":217209,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["ranking"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"weMr9gfllN57","executionInfo":{"status":"ok","timestamp":1648656645859,"user_tz":240,"elapsed":6,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}},"outputId":"6974398e-086a-4238-eee6-8c5dc3862f3d"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(<function __main__.EDU>, 'EDU', 0.5546695662679715),\n"," (<function __main__.CMS>, 'CMS', 0.5438673068529027),\n"," (<function __main__.ENT>, 'ENT', 0.5312569521690768),\n"," (<function __main__.PRT>, 'PRT', 0.5147113594040968),\n"," (<function __main__.EXT>, 'EXT', 0.4534883720930233),\n"," (<function __main__.LGL>, 'LGL', 0.437984496124031),\n"," (<function __main__.RPR>, 'RPR', 0.42611894543225015),\n"," (<function __main__.local_govt_worker>,\n","  'local govt worker',\n","  0.38288740754269013),\n"," (<function __main__.assoc_degree>, 'associates degree', 0.3556405353728489),\n"," (<function __main__.bachelor_degree>, 'bachelors degree', 0.3521507025832975),\n"," (<function __main__.SCI>, 'SCI', 0.34404761904761905),\n"," (<function __main__.cow_minorities>, 'cow minorities', 0.34098577571948396),\n"," (<function __main__.non_profit_worker>,\n","  'non profit worker',\n","  0.33964226289517474),\n"," (<function __main__.self_employed_worker>,\n","  'self employed worker',\n","  0.33831230148510605),\n"," (<function __main__.BUS>, 'BUS', 0.33788187372708756),\n"," (<function __main__.SAL>, 'SAL', 0.3360871472437491),\n"," (<function __main__.elderly>, 'elderly', 0.3360246972215626),\n"," (<function __main__.CON>, 'CON', 0.3357338820301783),\n"," (<function __main__.over_time>, 'over time', 0.328375),\n"," (<function __main__.FIN>, 'FIN', 0.3203002815139193),\n"," (<function __main__.sixties>, 'sixties', 0.3180296508847441),\n"," (<function __main__.oceanian>, 'oceanian', 0.3174224343675418),\n"," (<function __main__.divorced>, 'divorced', 0.31270122343850615),\n"," (<function __main__.european>, 'european', 0.3073569482288828),\n"," (<function __main__.married>, 'married', 0.30458968853693225),\n"," (<function __main__.african>, 'african', 0.30434782608695654),\n"," (<function __main__.black>, 'black', 0.3038952316991269),\n"," (<function __main__.us_born>, 'US born', 0.3037614639210936),\n"," (<function __main__.fifties>, 'fifties', 0.303328509406657),\n"," (<function __main__.MIL>, 'MIL', 0.2985347985347986),\n"," (<function __main__.white>, 'white', 0.2980504681946462),\n"," (<function __main__.other_race>, 'other race', 0.29288919571971006),\n"," (<function __main__.male>, 'male', 0.29157855454658543),\n"," (<function __main__.thirties>, 'thirties', 0.2888289036544851),\n"," (<function __main__.forties>, 'forties', 0.2885182998819362),\n"," (<function __main__.high_school_grad>,\n","  'high school grad',\n","  0.28396946564885495),\n"," (<function __main__.full_time>, 'full time', 0.28379486289148026),\n"," (<function __main__.OFF>, 'OFF', 0.28014854387908006),\n"," (<function __main__.MGR>, 'MGR', 0.27526577849549183),\n"," (<function __main__.female>, 'female', 0.2743410768730751),\n"," (<function __main__.part_time>, 'part time', 0.26780559180606633),\n"," (<function __main__.asian>, 'asian', 0.26774804001081376),\n"," (<function __main__.asian>, 'asian', 0.26774804001081376),\n"," (<function __main__.PRD>, 'PRD', 0.26692772067221404),\n"," (<function __main__.native_american>, 'native american', 0.2639472105578884),\n"," (<function __main__.mar_minorities>, 'mar minorities', 0.2627916496111339),\n"," (<function __main__.for_profit_worker>,\n","  'for profit worker',\n","  0.24843528611114207),\n"," (<function __main__.single>, 'single', 0.24632782507093975),\n"," (<function __main__.MED>, 'MED', 0.24571667363142502),\n"," (<function __main__.advanced_degree>, 'advanced degree', 0.24083051760442686),\n"," (<function __main__.twenties>, 'twenties', 0.2327312027173074),\n"," (<function __main__.CMM>, 'CMM', 0.2144377782078576),\n"," (<function __main__.multiple_race>, 'multiple race', 0.21245513506706126),\n"," (<function __main__.american>, 'american', 0.20259580313580594),\n"," (<function __main__.TRN>, 'TRN', 0.19896559003588765),\n"," (<function __main__.ENG>, 'ENG', 0.19300398895366677),\n"," (<function __main__.no_school>, 'no school', 0.1409563409563409),\n"," (<function __main__.some_school>, 'some school', 0.14031421716585402),\n"," (<function __main__.PRS>, 'PRS', 0.11613205338328259),\n"," (<function __main__.CLN>, 'CLN', 0.11183819155264718),\n"," (<function __main__.HLS>, 'HLS', 0.1026392961876833),\n"," (<function __main__.FFF>, 'FFF', 0.06400839454354668),\n"," (<function __main__.EAT>, 'EAT', 0.05395284327323158)]"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["### Manual Groups"],"metadata":{"id":"LjL1Spc1PJYY"}},{"cell_type":"markdown","metadata":{"id":"bbr-LNduyo2Y"},"source":["In the following cell(s), generate group functions that you think will make improvements, and then try to run the updates as explained in the subsequent section. In the final version of your code that you turn in, the groups that you generated and their corresponding models h, and the order in which you did updates, should be obvious and re-generating your final PDL should be completely reproducible just by running the code blocks in this notebook."]},{"cell_type":"code","execution_count":22,"metadata":{"pycharm":{"name":"#%%\n"},"id":"qesXg52cyo2Z","executionInfo":{"status":"ok","timestamp":1648656764987,"user_tz":240,"elapsed":415,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"outputs":[],"source":["# manually defined groups that tend to be systematically mistreated and would be educated guesses to test on the model\n","\n","def black_men(x):\n","  if x['RAC1P'] == 2 and x['SEX'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def black_women(x):\n","  if x['RAC1P'] == 2 and x['SEX'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def asian_men(x):\n","  if x['RAC1P'] == 6 and x['SEX'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def asian_women(x):\n","  if x['RAC1P'] == 6 and x['SEX'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def white_men(x):\n","  if x['RAC1P'] == 1 and x['SEX'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def white_women(x):\n","  if x['RAC1P'] == 1 and x['SEX'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def native_american_men(x):\n","  if x['RAC1P'] == 3 and x['SEX'] == 1:\n","      return 1\n","  elif x['RAC1P'] == 4 and x['SEX'] == 1:\n","      return 1\n","  elif x['RAC1P'] == 5 and x['SEX'] == 1:\n","      return 1\n","  elif x['RAC1P'] == 7 and x['SEX'] == 1:\n","      return 1\n","  else:\n","      return 0\n","\n","def native_american_women(x):\n","  if x['RAC1P'] == 3 and x['SEX'] == 2:\n","      return 1\n","  elif x['RAC1P'] == 4 and x['SEX'] == 2:\n","      return 1\n","  elif x['RAC1P'] == 5 and x['SEX'] == 2:\n","      return 1\n","  elif x['RAC1P'] == 7 and x['SEX'] == 2:\n","      return 1\n","  else:\n","      return 0\n","\n","def middle_aged_black(x):\n","  if x['RAC1P'] == 2 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  else:\n","      return 0\n","\n","def middle_aged_asian(x):\n","  if x['RAC1P'] == 6 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  else:\n","      return 0\n","\n","def middle_aged_white(x):\n","  if x['RAC1P'] == 1 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  else:\n","      return 0\n","\n","def middle_aged_native_american(x):\n","  if x['RAC1P'] == 3 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  elif x['RAC1P'] == 4 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  elif x['RAC1P'] == 5 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  elif x['RAC1P'] == 7 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  else:\n","      return 0\n","\n","def overtime_white(x):\n","  if x['RAC1P'] == 1 and x['WKHP'] >= 60:\n","      return 1\n","  else:\n","      return 0\n","\n","def overtime_black(x):\n","  if x['RAC1P'] == 2 and x['WKHP'] >= 60:\n","      return 1\n","  else:\n","      return 0\n","\n","def overtime_asian(x):\n","  if x['RAC1P'] == 6 and x['WKHP'] >= 60:\n","      return 1\n","  else:\n","      return 0\n","\n","def overtime_native_american(x):\n","  if x['RAC1P'] == 3 and x['WKHP'] >= 60:\n","      return 1\n","  elif x['RAC1P'] == 4 and x['WKHP'] >= 60:\n","      return 1\n","  elif x['RAC1P'] == 5 and x['WKHP'] >= 60:\n","      return 1\n","  elif x['RAC1P'] == 7 and x['WKHP'] >= 60:\n","      return 1\n","  else:\n","      return 0\n","\n","def married_men(x):\n","  if x['MAR'] == 1 and x['SEX'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def married_women(x):\n","  if x['MAR'] == 1 and x['SEX'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def divorced_men(x):\n","  if x['MAR'] == 3 and x['SEX'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def divorced_women(x):\n","  if x['MAR'] == 3 and x['SEX'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def single_men(x):\n","  if x['MAR'] == 5 and x['SEX'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def single_women(x):\n","  if x['MAR'] == 5 and x['SEX'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def divorced_black_women(x):\n","  if x['MAR'] == 3 and x['SEX'] == 2 and x['RAC1P'] == 2:\n","    return 1\n","  else: \n","    return 0   \n","\n","def single_black_women(x):\n","  if x['MAR'] == 5 and x['SEX'] == 2 and x['RAC1P'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def divorced_white_women(x):\n","  if x['MAR'] == 3 and x['SEX'] == 2 and x['RAC1P'] == 1:\n","    return 1\n","  else: \n","    return 0   \n","\n","def single_white_women(x):\n","  if x['MAR'] == 5 and x['SEX'] == 2 and x['RAC1P'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def divorced_asian_women(x):\n","  if x['MAR'] == 3 and x['SEX'] == 2 and x['RAC1P'] == 6:\n","    return 1\n","  else: \n","    return 0   \n","\n","def single_asian_women(x):\n","  if x['MAR'] == 5 and x['SEX'] == 2 and x['RAC1P'] == 6:\n","    return 1\n","  else: \n","    return 0\n","\n","def divorced_native_american_women(x):\n","  if x['MAR'] == 3 and x['SEX'] == 2 and (x['RAC1P'] == 3 or x['RAC1P'] == 4 or x['RAC1P'] == 5 or x['RAC1P'] == 7):\n","    return 1\n","  else: \n","    return 0   \n","\n","def single_native_american_women(x):\n","  if x['MAR'] == 5 and x['SEX'] == 2 and (x['RAC1P'] == 3 or x['RAC1P'] == 4 or x['RAC1P'] == 5 or x['RAC1P'] == 7):\n","    return 1\n","  else: \n","    return 0"]},{"cell_type":"code","source":["manual_functions = [\n","  (black_men, 'black men'),\n","  (black_women, 'black women'),\n","  (white_men, 'white men'),\n","  (white_women, 'white women'),\n","  (asian_men, 'asian men'),\n","  (asian_women, 'asian_women'),\n","  (native_american_men, 'native american men'),\n","  (native_american_women, 'natie american women'),\n","  (middle_aged_black, 'middle aged black'),\n","  (middle_aged_white, 'middle aged white'),\n","  (middle_aged_asian, 'middle aged asian'),\n","  (middle_aged_native_american, 'middle aged native american'),\n","  (overtime_white, 'overtime white'),\n","  (overtime_black, 'overtime black'),\n","  (overtime_asian, 'overtime asian'),\n","  (overtime_native_american, 'overtime native american'),\n","  (married_men, 'married men'),\n","  (married_women, 'married women'),\n","  (divorced_men, 'divorced women'),\n","  (single_men, 'single men'),\n","  (single_women, 'single women'),\n","  (divorced_black_women, 'divorced black women'),\n","  (single_black_women, 'single black women'),\n","  (divorced_white_women, 'divorced white women'),\n","  (single_white_women, 'single white women'),\n","  (divorced_asian_women, 'divorced asian women'),\n","  (single_asian_women, 'single asian women'),\n","  (divorced_native_american_women, 'divorced_native_american_women'),\n","  (single_native_american_women, 'single native american women')\n","]"],"metadata":{"id":"UCWcOQ-h75iV","executionInfo":{"status":"ok","timestamp":1648656775408,"user_tz":240,"elapsed":130,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["## Updating the Function"],"metadata":{"id":"ZRwL2pCZFV3q"}},{"cell_type":"markdown","metadata":{"id":"a-gMuujzyo2Z"},"source":["Once you've found a promising group g, you can run the following updater code. Here, we define two different update functions. The first, `simple_updater`, only requires that you find some group g that you think f might do poorly on. Then, it automatically trains a decision list of depth 10 on the training data restricted to your g, and it passes that model and g along to the updater.\n","\n","You might want to do something a bit fancier than a decision tree to make your model, in which case you can run the second updater, which takes as input a group g and model h, and then updates f accordingly.\n","\n","Every time you run the update function, it will tell you if your (g,h) passed the validation checks, i.e. if a) your group existed in the validation data and b) it made an improvement compared to f. If it did pass the validation checks, then the model f is updated to include your g and h. **Note that this means that as you run updates, it will be increasingly difficult to find groups that make improvements.**"]},{"cell_type":"code","execution_count":24,"metadata":{"pycharm":{"name":"#%%\n"},"id":"qr8Hm4KByo2Z","executionInfo":{"status":"ok","timestamp":1648656778596,"user_tz":240,"elapsed":127,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"outputs":[],"source":["def simple_updater(g, group_name=\"g\"):\n","    # if you want to change how h is trained, you can edit the below line.\n","    print('Testing Group: ' + group_name)\n","    h = build_model(train_x, train_y, g, dt_depth=10) #bountyHuntWrapper.build_model(train_x, train_y, g, dt_depth=10)\n","    # do not change anything beyond this point.\n","    if run_checks(f, validation_x, validation_y, g, h, train_x=train_x, train_y=train_y): #bountyHuntWrapper.run_checks(f, validation_x, validation_y, g, h, train_x=train_x, train_y=train_y):\n","        print(\"Running Update\")\n","        run_updates(f, g, h, train_x, train_y, validation_x, validation_y, group_name=group_name) #bountyHuntWrapper.run_updates(f, g, h, train_x, train_y, validation_x, validation_y, group_name=group_name)\n","\n","def updater(g, h, group_name=\"g\"):\n","    # do not alter this code\n","    if run_checks(f, validation_x, validation_y, g, h, train_x=train_x, train_y=train_y): #bountyHuntWrapper.run_checks(f, validation_x, validation_y, g, h, train_x=train_x, train_y=train_y):\n","        print(\"Running Update\")\n","        run_updates(f, g, h, train_x, train_y, validation_x, validation_y, group_name=group_name) #bountyHuntWrapper.run_updates(f, g, h, train_x, train_y, validation_x, validation_y, group_name=group_name)"]},{"cell_type":"markdown","metadata":{"id":"WKFZCqBCyo2a"},"source":["In the below block, provide a script that builds *the entire final PDL that you come up with*. We will run this on the initial version of f (the decision stump) in order to evaluate your code. (Note: it is fine for the group functions g and the hs to be defined in above code blocks, just make sure that everything runs as you expect if you run everything from a clean kernel)"]},{"cell_type":"code","source":["# updating with algorithmically ordered defined groups\n","for func, name, _ in ranking:\n","  simple_updater(func, name)\n","\n","# updating with manually defined groups\n","for func, name in manual_functions:\n","  simple_updater(func, name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mdQeEr6s9q-o","executionInfo":{"status":"ok","timestamp":1648666932027,"user_tz":240,"elapsed":10147541,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}},"outputId":"81d00212-5041-4d11-9a56-95c1b6230052"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing Group: EDU\n","building h\n","finished building h\n","Error of current model on proposed group: 0.5519516217702034\n","Error of h trained on proposed group: 0.18856514568444205\n","Group size in test set: 1819\n","Group weight in test set: 0.06197614991482112\n","Training Error of current model on proposed group: 0.5546695662679715\n","Training Error of h trained on proposed group: 0.11537996858765254\n","Group size in training set: 8277\n","Group weight in training set: 0.060431938319558426\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node -1\n","new errs [0.2571039182282794, 0.18856514568444205]\n","Testing Group: CMS\n","building h\n","finished building h\n","Error of current model on proposed group: 0.5603271983640081\n","Error of h trained on proposed group: 0.32515337423312884\n","Group size in test set: 489\n","Group weight in test set: 0.01666098807495741\n","Training Error of current model on proposed group: 0.5438673068529027\n","Training Error of h trained on proposed group: 0.12701876909646448\n","Group size in training set: 2291\n","Group weight in training set: 0.016727023159370344\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node -1\n","new errs [0.2531856899488927, 0.18856514568444205, 0.32515337423312884]\n","Testing Group: ENT\n","building h\n","finished building h\n","Error of current model on proposed group: 0.5544768069039914\n","Error of h trained on proposed group: 0.2632146709816613\n","Group size in test set: 927\n","Group weight in test set: 0.031584327086882455\n","Training Error of current model on proposed group: 0.5312569521690768\n","Training Error of h trained on proposed group: 0.1526140155728587\n","Group size in training set: 4495\n","Group weight in training set: 0.032818842907625365\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node -1\n","new errs [0.24398637137989776, 0.18856514568444205, 0.32515337423312884, 0.2632146709816613]\n","Testing Group: PRT\n","building h\n","finished building h\n","Error of current model on proposed group: 0.48484848484848486\n","Error of h trained on proposed group: 0.18855218855218858\n","Group size in test set: 594\n","Group weight in test set: 0.020238500851788757\n","Training Error of current model on proposed group: 0.5147113594040968\n","Training Error of h trained on proposed group: 0.07076350093109873\n","Group size in training set: 2685\n","Group weight in training set: 0.0196036914809731\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node -1\n","new errs [0.2379897785349233, 0.18856514568444205, 0.32515337423312884, 0.2632146709816613, 0.18855218855218858]\n","Testing Group: EXT\n","building h\n","finished building h\n","Error of current model on proposed group: 0.41666666666666663\n","Error of h trained on proposed group: 0.41666666666666663\n","Group size in test set: 12\n","Group weight in test set: 0.0004088586030664395\n","Training Error of current model on proposed group: 0.4534883720930233\n","Training Error of h trained on proposed group: 0.0\n","Group size in training set: 86\n","Group weight in training set: 0.00062790222248182\n","Failed improvement check.\n","Testing Group: LGL\n","building h\n","finished building h\n","Error of current model on proposed group: 0.40384615384615385\n","Error of h trained on proposed group: 0.3173076923076923\n","Group size in test set: 104\n","Group weight in test set: 0.0035434412265758094\n","Training Error of current model on proposed group: 0.437984496124031\n","Training Error of h trained on proposed group: 0.08333333333333337\n","Group size in training set: 516\n","Group weight in training set: 0.0037674133348909204\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node -1\n","new errs [0.23768313458262347, 0.18856514568444205, 0.32515337423312884, 0.2632146709816613, 0.18855218855218858, 0.3173076923076923]\n","Testing Group: RPR\n","building h\n","finished building h\n","Error of current model on proposed group: 0.431314623338257\n","Error of h trained on proposed group: 0.3057607090103397\n","Group size in test set: 677\n","Group weight in test set: 0.023066439522998295\n","Training Error of current model on proposed group: 0.42611894543225015\n","Training Error of h trained on proposed group: 0.1621704475781729\n","Group size in training set: 3262\n","Group weight in training set: 0.023816477322508103\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node -1\n","new errs [0.23478705281090284, 0.18856514568444205, 0.32515337423312884, 0.2632146709816613, 0.18855218855218858, 0.3173076923076923, 0.3057607090103397]\n","Testing Group: local govt worker\n","building h\n","finished building h\n","Error of current model on proposed group: 0.23546391752577323\n","Error of h trained on proposed group: 0.2243298969072165\n","Group size in test set: 2425\n","Group weight in test set: 0.08262350936967632\n","Training Error of current model on proposed group: 0.19258515204090954\n","Training Error of h trained on proposed group: 0.1324993151310383\n","Group size in training set: 10951\n","Group weight in training set: 0.07995531672556293\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 3\n","new errs [0.23386712095400342, 0.18856514568444205, 0.3210633946830266, 0.2642934196332255, 0.19865319865319864, 0.28846153846153844, 0.30871491875923196, 0.2243298969072165]\n","prob node 4\n","new errs [0.23383304940374783, 0.18856514568444205, 0.3210633946830266, 0.2632146709816613, 0.19865319865319864, 0.28846153846153844, 0.30871491875923196, 0.2239175257731959]\n","prob node 6\n","new errs [0.23362862010221463, 0.18856514568444205, 0.3210633946830266, 0.2632146709816613, 0.18855218855218858, 0.28846153846153844, 0.30871491875923196, 0.2214432989690721]\n","prob node -1\n","new errs [0.23356047700170357, 0.18856514568444205, 0.3210633946830266, 0.2632146709816613, 0.18855218855218858, 0.28846153846153844, 0.3057607090103397, 0.22061855670103092]\n","Testing Group: associates degree\n","building h\n","finished building h\n","Error of current model on proposed group: 0.28749480681346073\n","Error of h trained on proposed group: 0.25384295803905277\n","Group size in test set: 2407\n","Group weight in test set: 0.08201022146507667\n","Training Error of current model on proposed group: 0.25694254757352275\n","Training Error of h trained on proposed group: 0.1690794864791041\n","Group size in training set: 10983\n","Group weight in training set: 0.08018895476183523\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 2\n","new errs [0.23080068143100507, 0.1869158878504673, 0.33333333333333337, 0.2664509169363538, 0.19191919191919193, 0.29807692307692313, 0.30132939438700146, 0.2235051546391753, 0.25384295803905277]\n","prob node 3\n","new errs [0.23059625212947188, 0.1869158878504673, 0.3210633946830266, 0.2664509169363538, 0.19191919191919193, 0.29807692307692313, 0.30132939438700146, 0.22061855670103092, 0.2513502285002077]\n","prob node 4\n","new errs [0.23049403747870523, 0.1869158878504673, 0.3210633946830266, 0.2632146709816613, 0.19191919191919193, 0.29807692307692313, 0.30132939438700146, 0.22103092783505152, 0.2501038637307852]\n","prob node 5\n","new errs [0.23042589437819416, 0.1869158878504673, 0.3210633946830266, 0.2632146709816613, 0.18855218855218858, 0.29807692307692313, 0.30132939438700146, 0.21896907216494843, 0.24927295388450355]\n","prob node -1\n","new errs [0.23039182282793869, 0.1869158878504673, 0.3210633946830266, 0.2632146709816613, 0.18855218855218858, 0.28846153846153844, 0.30132939438700146, 0.21896907216494843, 0.24885749896136267]\n","Testing Group: bachelors degree\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2862851434280006\n","Error of h trained on proposed group: 0.22919937205651486\n","Group size in test set: 7007\n","Group weight in test set: 0.23873935264054513\n","Training Error of current model on proposed group: 0.26366815978400937\n","Training Error of h trained on proposed group: 0.18874639504203228\n","Group size in training set: 32594\n","Group weight in training set: 0.23797494232060978\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 3\n","new errs [0.21676320272572402, 0.18031885651456847, 0.31492842535787324, 0.2642934196332255, 0.17845117845117842, 0.33653846153846156, 0.30132939438700146, 0.20412371134020624, 0.24885749896136267, 0.22919937205651486]\n","prob node 5\n","new errs [0.21672913117546844, 0.18031885651456847, 0.31492842535787324, 0.2632146709816613, 0.17845117845117842, 0.33653846153846156, 0.30132939438700146, 0.20412371134020624, 0.24885749896136267, 0.22905665762808625]\n","prob node -1\n","new errs [0.21655877342419083, 0.18031885651456847, 0.31492842535787324, 0.2632146709816613, 0.17845117845117842, 0.28846153846153844, 0.30132939438700146, 0.20412371134020624, 0.24885749896136267, 0.22834308548594262]\n","Testing Group: SCI\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2774566473988439\n","Error of h trained on proposed group: 0.2601156069364162\n","Group size in test set: 346\n","Group weight in test set: 0.011788756388415672\n","Training Error of current model on proposed group: 0.24464285714285716\n","Training Error of h trained on proposed group: 0.07916666666666672\n","Group size in training set: 1680\n","Group weight in training set: 0.01226599690429602\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 7\n","new errs [0.21635434412265753, 0.18031885651456847, 0.31492842535787324, 0.2632146709816613, 0.17845117845117842, 0.28846153846153844, 0.30132939438700146, 0.20577319587628862, 0.2484420440382219, 0.22891394319965752, 0.2601156069364162]\n","prob node -1\n","new errs [0.2162180579216354, 0.18031885651456847, 0.31492842535787324, 0.2632146709816613, 0.17845117845117842, 0.28846153846153844, 0.30132939438700146, 0.20412371134020624, 0.2484420440382219, 0.22834308548594262, 0.24855491329479773]\n","Testing Group: cow minorities\n","building h\n","finished building h\n","Error of current model on proposed group: 0.26388453090680886\n","Error of h trained on proposed group: 0.2372136805773455\n","Group size in test set: 3187\n","Group weight in test set: 0.10858603066439523\n","Training Error of current model on proposed group: 0.21812768772742308\n","Training Error of h trained on proposed group: 0.17168375785643397\n","Group size in training set: 15115\n","Group weight in training set: 0.11035746619549662\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 1\n","new errs [0.21332197614991477, 0.1808686091258933, 0.3271983640081799, 0.26860841423948223, 0.18013468013468015, 0.29807692307692313, 0.2983751846381093, 0.20412371134020624, 0.24968840880764442, 0.22734408448694166, 0.2225433526011561, 0.2372136805773455]\n","prob node 2\n","new errs [0.2132879045996593, 0.18031885651456847, 0.3271983640081799, 0.26860841423948223, 0.18013468013468015, 0.29807692307692313, 0.2983751846381093, 0.20412371134020624, 0.24927295388450355, 0.22734408448694166, 0.2225433526011561, 0.23689990586758702]\n","prob node 3\n","new errs [0.2130834752981261, 0.18031885651456847, 0.31492842535787324, 0.26860841423948223, 0.18013468013468015, 0.29807692307692313, 0.2983751846381093, 0.20412371134020624, 0.24927295388450355, 0.22720137005851293, 0.2225433526011561, 0.2350172576090367]\n","prob node 4\n","new errs [0.21291311754684838, 0.18031885651456847, 0.31492842535787324, 0.2632146709816613, 0.18013468013468015, 0.29807692307692313, 0.2983751846381093, 0.20412371134020624, 0.2484420440382219, 0.22748679891537038, 0.2225433526011561, 0.2334483840602447]\n","prob node 5\n","new errs [0.2128790459965928, 0.18031885651456847, 0.31492842535787324, 0.2632146709816613, 0.17845117845117842, 0.29807692307692313, 0.2983751846381093, 0.20412371134020624, 0.24761113419194014, 0.2276295133437991, 0.2225433526011561, 0.23313460935048635]\n","prob node -1\n","new errs [0.21284497444633732, 0.18031885651456847, 0.31492842535787324, 0.2632146709816613, 0.17845117845117842, 0.28846153846153844, 0.2983751846381093, 0.20412371134020624, 0.24719567926879937, 0.2276295133437991, 0.2225433526011561, 0.232820834640728]\n","Testing Group: non profit worker\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2236328125\n","Error of h trained on proposed group: 0.21923828125\n","Group size in test set: 2048\n","Group weight in test set: 0.069778534923339\n","Training Error of current model on proposed group: 0.18479617304492513\n","Training Error of h trained on proposed group: 0.14652662229617308\n","Group size in training set: 9616\n","Group weight in training set: 0.0702082298998277\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 1\n","new errs [0.21253833049403748, 0.18251786695986805, 0.29038854805725967, 0.2621359223300971, 0.1767676767676768, 0.29807692307692313, 0.30132939438700146, 0.20412371134020624, 0.25259659326963024, 0.22919937205651486, 0.2341040462427746, 0.232820834640728, 0.21923828125]\n","prob node 5\n","new errs [0.21240204429301535, 0.18031885651456847, 0.29038854805725967, 0.2621359223300971, 0.1767676767676768, 0.29807692307692313, 0.30132939438700146, 0.20412371134020624, 0.2530120481927711, 0.22919937205651486, 0.2341040462427746, 0.232820834640728, 0.21728515625]\n","prob node 6\n","new errs [0.21236797274275976, 0.18031885651456847, 0.29038854805725967, 0.2621359223300971, 0.1767676767676768, 0.28846153846153844, 0.30132939438700146, 0.20412371134020624, 0.2530120481927711, 0.22905665762808625, 0.2341040462427746, 0.232820834640728, 0.216796875]\n","prob node 8\n","new errs [0.2122998296422487, 0.18031885651456847, 0.29038854805725967, 0.2621359223300971, 0.1767676767676768, 0.28846153846153844, 0.2983751846381093, 0.20412371134020624, 0.2530120481927711, 0.22905665762808625, 0.2341040462427746, 0.232820834640728, 0.2158203125]\n","prob node 9\n","new errs [0.21182282793867124, 0.18031885651456847, 0.29038854805725967, 0.2621359223300971, 0.1767676767676768, 0.28846153846153844, 0.2983751846381093, 0.20412371134020624, 0.24719567926879937, 0.22905665762808625, 0.23121387283236994, 0.232820834640728, 0.208984375]\n","prob node -1\n","new errs [0.2114821124361158, 0.18031885651456847, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.2983751846381093, 0.20412371134020624, 0.24719567926879937, 0.2276295133437991, 0.2167630057803468, 0.232820834640728, 0.2041015625]\n","Testing Group: self employed worker\n","building h\n","finished building h\n","Error of current model on proposed group: 0.28296067848882034\n","Error of h trained on proposed group: 0.264070932922128\n","Group size in test set: 2594\n","Group weight in test set: 0.08838160136286201\n","Training Error of current model on proposed group: 0.22482616533608035\n","Training Error of h trained on proposed group: 0.1844793544510258\n","Group size in training set: 11649\n","Group weight in training set: 0.08505154639175258\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 2\n","new errs [0.20981260647359457, 0.17757009345794394, 0.3087934560327198, 0.267529665587918, 0.18013468013468015, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24096385542168675, 0.22605965463108324, 0.2109826589595376, 0.232820834640728, 0.2041015625, 0.264070932922128]\n","prob node 3\n","new errs [0.2097103918228279, 0.17757009345794394, 0.30265848670756645, 0.267529665587918, 0.18013468013468015, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22591694020265451, 0.2109826589595376, 0.232820834640728, 0.2041015625, 0.2629144178874325]\n","prob node 4\n","new errs [0.20947189097103913, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.18013468013468015, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22620236905951197, 0.2109826589595376, 0.232820834640728, 0.2041015625, 0.26021588280647645]\n","prob node -1\n","new errs [0.20940374787052807, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22605965463108324, 0.2109826589595376, 0.232820834640728, 0.2041015625, 0.25944487278334616]\n","Testing Group: BUS\n","building h\n","finished building h\n","Error of current model on proposed group: 0.26374650512581543\n","Error of h trained on proposed group: 0.2534948741845293\n","Group size in test set: 1073\n","Group weight in test set: 0.0365587734241908\n","Training Error of current model on proposed group: 0.23340122199592672\n","Training Error of h trained on proposed group: 0.14052953156822812\n","Group size in training set: 4910\n","Group weight in training set: 0.03584883619053182\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 7\n","new errs [0.20902896081771716, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20453608247422683, 0.24137931034482762, 0.22805765662908517, 0.2109826589595376, 0.23470348289927834, 0.2060546875, 0.26252891287586744, 0.2534948741845293]\n","prob node 8\n","new errs [0.20899488926746168, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.2417947652679684, 0.22777222777222772, 0.2109826589595376, 0.23470348289927834, 0.2060546875, 0.26252891287586744, 0.25256290773532153]\n","prob node 9\n","new errs [0.20889267461669503, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22777222777222772, 0.2109826589595376, 0.23376215877000317, 0.20556640625, 0.26175790285273703, 0.24976700838769805]\n","prob node 11\n","new errs [0.20848381601362864, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22605965463108324, 0.2109826589595376, 0.23313460935048635, 0.2041015625, 0.26175790285273703, 0.23858341099720415]\n","prob node 13\n","new errs [0.20844974446337305, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22605965463108324, 0.2109826589595376, 0.232820834640728, 0.2041015625, 0.26175790285273703, 0.23765144454799625]\n","prob node -1\n","new errs [0.20824531516183986, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22605965463108324, 0.2109826589595376, 0.232820834640728, 0.2041015625, 0.25944487278334616, 0.2320596458527493]\n","Testing Group: SAL\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2265571526351814\n","Error of h trained on proposed group: 0.20636550308008217\n","Group size in test set: 2922\n","Group weight in test set: 0.09955706984667802\n","Training Error of current model on proposed group: 0.22408246819710487\n","Training Error of h trained on proposed group: 0.12743091095189352\n","Group size in training set: 13678\n","Group weight in training set: 0.09986565812914343\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 7\n","new errs [0.20623509369676318, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20453608247422683, 0.24221022019110927, 0.2282003710575139, 0.2109826589595376, 0.23376215877000317, 0.20361328125, 0.26214340786430224, 0.2320596458527493, 0.20636550308008217]\n","prob node 8\n","new errs [0.2062010221465077, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24221022019110927, 0.22805765662908517, 0.2109826589595376, 0.23376215877000317, 0.20361328125, 0.26214340786430224, 0.2320596458527493, 0.20602327173169066]\n","prob node 9\n","new errs [0.20606473594548547, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22805765662908517, 0.2109826589595376, 0.23407593347976152, 0.20361328125, 0.2629144178874325, 0.2320596458527493, 0.20465434633812463]\n","prob node 13\n","new errs [0.20558773424190802, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22605965463108324, 0.2109826589595376, 0.232820834640728, 0.203125, 0.2644564379336931, 0.2320596458527493, 0.1998631074606434]\n","prob node -1\n","new errs [0.20514480408858604, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22605965463108324, 0.2109826589595376, 0.232820834640728, 0.203125, 0.25944487278334616, 0.2320596458527493, 0.19541409993155368]\n","Testing Group: elderly\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2867647058823529\n","Error of h trained on proposed group: 0.31092436974789917\n","Group size in test set: 952\n","Group weight in test set: 0.03243611584327087\n","Training Error of current model on proposed group: 0.17668012348610784\n","Training Error of h trained on proposed group: 0.14580859653289002\n","Group size in training set: 4211\n","Group weight in training set: 0.030745305335708652\n","Failed improvement check.\n","Testing Group: CON\n","building h\n","finished building h\n","Error of current model on proposed group: 0.28537549407114626\n","Error of h trained on proposed group: 0.2640316205533597\n","Group size in test set: 1265\n","Group weight in test set: 0.043100511073253835\n","Training Error of current model on proposed group: 0.2772633744855967\n","Training Error of h trained on proposed group: 0.18021262002743488\n","Group size in training set: 5832\n","Group weight in training set: 0.04258053211062761\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 7\n","new errs [0.2042248722316865, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.2070103092783505, 0.24096385542168675, 0.2264877979163693, 0.2109826589595376, 0.2356448070285535, 0.20263671875, 0.2636854279105628, 0.2320596458527493, 0.19541409993155368, 0.2640316205533597]\n","prob node 9\n","new errs [0.20398637137989784, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22620236905951197, 0.2109826589595376, 0.2356448070285535, 0.20263671875, 0.2636854279105628, 0.2320596458527493, 0.19541409993155368, 0.25849802371541497]\n","prob node 11\n","new errs [0.20395229982964225, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22605965463108324, 0.2109826589595376, 0.23533103231879515, 0.2021484375, 0.2636854279105628, 0.2320596458527493, 0.19541409993155368, 0.2577075098814229]\n","prob node 13\n","new errs [0.203679727427598, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.24054840049854587, 0.22605965463108324, 0.2109826589595376, 0.232820834640728, 0.2021484375, 0.2636854279105628, 0.2320596458527493, 0.19541409993155368, 0.25138339920948616]\n","prob node -1\n","new errs [0.20330494037478708, 0.17757009345794394, 0.30265848670756645, 0.25997842502696866, 0.1767676767676768, 0.28846153846153844, 0.29542097488921715, 0.20412371134020624, 0.2401329455754051, 0.22605965463108324, 0.2109826589595376, 0.232820834640728, 0.2021484375, 0.25944487278334616, 0.2320596458527493, 0.19541409993155368, 0.24268774703557316]\n","Testing Group: over time\n","building h\n","finished building h\n","Error of current model on proposed group: 0.24319727891156462\n","Error of h trained on proposed group: 0.22789115646258506\n","Group size in test set: 1764\n","Group weight in test set: 0.06010221465076661\n","Training Error of current model on proposed group: 0.17862500000000003\n","Training Error of h trained on proposed group: 0.141625\n","Group size in training set: 8000\n","Group weight in training set: 0.058409509068076286\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 3\n","new errs [0.20238500851788754, 0.17647058823529416, 0.2985685071574642, 0.2610571736785329, 0.1902356902356902, 0.2692307692307693, 0.29542097488921715, 0.20371134020618553, 0.24262567511425015, 0.22506065363208216, 0.21387283236994215, 0.23031063696266085, 0.20166015625, 0.26137239784117194, 0.2301957129543336, 0.1923340177960301, 0.24584980237154153, 0.22789115646258506]\n","prob node 4\n","new errs [0.20235093696763207, 0.17647058823529416, 0.2985685071574642, 0.25997842502696866, 0.1902356902356902, 0.2692307692307693, 0.29542097488921715, 0.20371134020618553, 0.24262567511425015, 0.2246325103467961, 0.21387283236994215, 0.23031063696266085, 0.20166015625, 0.26137239784117194, 0.2301957129543336, 0.1923340177960301, 0.24584980237154153, 0.2273242630385488]\n","prob node 8\n","new errs [0.2020783645655877, 0.17647058823529416, 0.2985685071574642, 0.25997842502696866, 0.1767676767676768, 0.2692307692307693, 0.29542097488921715, 0.20247422680412375, 0.24137931034482762, 0.2246325103467961, 0.21387283236994215, 0.22968308754314404, 0.20166015625, 0.26098689282960674, 0.2301957129543336, 0.1923340177960301, 0.24584980237154153, 0.22278911564625847]\n","prob node 10\n","new errs [0.20197614991482116, 0.17647058823529416, 0.2985685071574642, 0.25997842502696866, 0.1767676767676768, 0.2692307692307693, 0.29542097488921715, 0.20247422680412375, 0.2401329455754051, 0.2246325103467961, 0.21387283236994215, 0.22905553812362722, 0.20068359375, 0.26060138781804165, 0.2301957129543336, 0.1923340177960301, 0.24426877470355735, 0.22108843537414968]\n","prob node 13\n","new errs [0.20194207836456557, 0.17647058823529416, 0.2985685071574642, 0.25997842502696866, 0.1767676767676768, 0.2692307692307693, 0.29542097488921715, 0.20247422680412375, 0.2401329455754051, 0.22434708148993865, 0.2109826589595376, 0.22936931283338557, 0.20068359375, 0.26060138781804165, 0.2301957129543336, 0.1923340177960301, 0.24426877470355735, 0.22052154195011342]\n","prob node -1\n","new errs [0.20183986371379903, 0.1759208356239692, 0.2985685071574642, 0.25997842502696866, 0.1767676767676768, 0.2692307692307693, 0.29394387001477107, 0.20247422680412375, 0.2401329455754051, 0.22477522477522482, 0.2109826589595376, 0.22936931283338557, 0.20068359375, 0.25944487278334616, 0.22926374650512582, 0.1926762491444216, 0.24268774703557316, 0.21882086167800452]\n","Testing Group: FIN\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2987987987987988\n","Error of h trained on proposed group: 0.28978978978978975\n","Group size in test set: 666\n","Group weight in test set: 0.022691652470187392\n","Training Error of current model on proposed group: 0.22489834219580862\n","Training Error of h trained on proposed group: 0.13575226775101656\n","Group size in training set: 3197\n","Group weight in training set: 0.023341900061329985\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 7\n","new errs [0.20163543441226572, 0.1759208356239692, 0.2985685071574642, 0.25997842502696866, 0.1767676767676768, 0.2692307692307693, 0.29394387001477107, 0.20329896907216494, 0.24262567511425015, 0.2252033680605109, 0.2109826589595376, 0.23031063696266085, 0.20068359375, 0.25944487278334616, 0.22926374650512582, 0.1926762491444216, 0.24268774703557316, 0.21995464852607705, 0.28978978978978975]\n","prob node 8\n","new errs [0.20156729131175466, 0.1759208356239692, 0.2985685071574642, 0.25997842502696866, 0.1767676767676768, 0.2692307692307693, 0.29394387001477107, 0.20247422680412375, 0.24221022019110927, 0.2252033680605109, 0.2109826589595376, 0.23031063696266085, 0.20068359375, 0.25944487278334616, 0.22926374650512582, 0.1926762491444216, 0.24268774703557316, 0.21995464852607705, 0.2867867867867868]\n","prob node 9\n","new errs [0.20139693356047705, 0.1759208356239692, 0.2985685071574642, 0.25997842502696866, 0.1767676767676768, 0.2692307692307693, 0.29394387001477107, 0.20247422680412375, 0.2401329455754051, 0.2252033680605109, 0.2109826589595376, 0.2299968622529024, 0.20068359375, 0.25944487278334616, 0.22926374650512582, 0.1926762491444216, 0.24268774703557316, 0.21995464852607705, 0.2792792792792793]\n","prob node 11\n","new errs [0.2012947189097104, 0.1759208356239692, 0.2985685071574642, 0.25997842502696866, 0.1767676767676768, 0.2692307692307693, 0.29394387001477107, 0.20247422680412375, 0.2401329455754051, 0.22477522477522482, 0.2109826589595376, 0.2299968622529024, 0.201171875, 0.25983037779491136, 0.22926374650512582, 0.1926762491444216, 0.24268774703557316, 0.21825396825396826, 0.27477477477477474]\n","prob node 12\n","new errs [0.20122657580919934, 0.1759208356239692, 0.2985685071574642, 0.25997842502696866, 0.1767676767676768, 0.2692307692307693, 0.29394387001477107, 0.20247422680412375, 0.2401329455754051, 0.22477522477522482, 0.2109826589595376, 0.22936931283338557, 0.201171875, 0.25983037779491136, 0.22926374650512582, 0.1926762491444216, 0.24268774703557316, 0.217687074829932, 0.2717717717717718]\n","prob node 13\n","new errs [0.20119250425894375, 0.1759208356239692, 0.2985685071574642, 0.25997842502696866, 0.1767676767676768, 0.2692307692307693, 0.29394387001477107, 0.20247422680412375, 0.2401329455754051, 0.22477522477522482, 0.2109826589595376, 0.22936931283338557, 0.20068359375, 0.25983037779491136, 0.22926374650512582, 0.1926762491444216, 0.24268774703557316, 0.217687074829932, 0.2702702702702703]\n","prob node -1\n","new errs [0.20115843270868827, 0.1759208356239692, 0.2985685071574642, 0.25997842502696866, 0.1767676767676768, 0.2692307692307693, 0.29394387001477107, 0.20247422680412375, 0.2401329455754051, 0.22477522477522482, 0.2109826589595376, 0.22936931283338557, 0.20068359375, 0.25944487278334616, 0.22926374650512582, 0.1926762491444216, 0.24268774703557316, 0.217687074829932, 0.26876876876876876]\n","Testing Group: sixties\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2560132706662981\n","Error of h trained on proposed group: 0.2618191871716893\n","Group size in test set: 3617\n","Group weight in test set: 0.12323679727427599\n","Training Error of current model on proposed group: 0.189143950263032\n","Training Error of h trained on proposed group: 0.18669296987087514\n","Group size in training set: 16728\n","Group weight in training set: 0.12213428346134751\n","Failed improvement check.\n","Testing Group: oceanian\n","building h\n","finished building h\n","Error of current model on proposed group: 0.3222222222222222\n","Error of h trained on proposed group: 0.2666666666666667\n","Group size in test set: 90\n","Group weight in test set: 0.0030664395229982964\n","Training Error of current model on proposed group: 0.13365155131264916\n","Training Error of h trained on proposed group: 0.026252983293556076\n","Group size in training set: 419\n","Group weight in training set: 0.003059198037440495\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 1\n","new errs [0.20098807495741056, 0.177020340846619, 0.2965235173824131, 0.2610571736785329, 0.17508417508417506, 0.2692307692307693, 0.292466765140325, 0.20164948453608245, 0.2401329455754051, 0.2246325103467961, 0.2109826589595376, 0.2299968622529024, 0.201171875, 0.25905936777178107, 0.2301957129543336, 0.1919917864476386, 0.24268774703557316, 0.2159863945578231, 0.26876876876876876, 0.2666666666666667]\n","prob node 3\n","new errs [0.2009199318568995, 0.1759208356239692, 0.2965235173824131, 0.2610571736785329, 0.17508417508417506, 0.2692307692307693, 0.292466765140325, 0.20164948453608245, 0.2401329455754051, 0.22434708148993865, 0.2109826589595376, 0.22968308754314404, 0.20068359375, 0.25905936777178107, 0.2301957129543336, 0.1919917864476386, 0.24268774703557316, 0.2159863945578231, 0.26876876876876876, 0.24444444444444446]\n","prob node 11\n","new errs [0.2008858603066439, 0.1759208356239692, 0.2965235173824131, 0.25997842502696866, 0.17508417508417506, 0.2692307692307693, 0.292466765140325, 0.20164948453608245, 0.2401329455754051, 0.22434708148993865, 0.2109826589595376, 0.22968308754314404, 0.20068359375, 0.25905936777178107, 0.2301957129543336, 0.1919917864476386, 0.24268774703557316, 0.2159863945578231, 0.26876876876876876, 0.23333333333333328]\n","prob node -1\n","new errs [0.20085178875638843, 0.1759208356239692, 0.2965235173824131, 0.25997842502696866, 0.17508417508417506, 0.2692307692307693, 0.292466765140325, 0.20164948453608245, 0.23971749065226422, 0.22448979591836737, 0.2109826589595376, 0.22936931283338557, 0.20068359375, 0.25905936777178107, 0.22926374650512582, 0.1923340177960301, 0.24268774703557316, 0.2159863945578231, 0.26876876876876876, 0.2222222222222222]\n","Testing Group: divorced\n","building h\n","finished building h\n","Error of current model on proposed group: 0.24437781109445278\n","Error of h trained on proposed group: 0.2773613193403298\n","Group size in test set: 2668\n","Group weight in test set: 0.09090289608177173\n","Training Error of current model on proposed group: 0.19518673535093367\n","Training Error of h trained on proposed group: 0.18230843528654217\n","Group size in training set: 12424\n","Group weight in training set: 0.09070996758272247\n","Failed improvement check.\n","Testing Group: european\n","building h\n","finished building h\n","Error of current model on proposed group: 0.19818652849740936\n","Error of h trained on proposed group: 0.24870466321243523\n","Group size in test set: 772\n","Group weight in test set: 0.026303236797274275\n","Training Error of current model on proposed group: 0.16730245231607632\n","Training Error of h trained on proposed group: 0.1054495912806539\n","Group size in training set: 3670\n","Group weight in training set: 0.026795362284979994\n","Failed improvement check.\n","Testing Group: married\n","building h\n","finished building h\n","Error of current model on proposed group: 0.21823402727925345\n","Error of h trained on proposed group: 0.2186255955100176\n","Group size in test set: 15323\n","Group weight in test set: 0.5220783645655878\n","Training Error of current model on proposed group: 0.1785963249031035\n","Training Error of h trained on proposed group: 0.19007054624543396\n","Group size in training set: 71726\n","Group weight in training set: 0.5236850559271049\n","Failed improvement check.\n","Testing Group: african\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2125603864734299\n","Error of h trained on proposed group: 0.2560386473429952\n","Group size in test set: 207\n","Group weight in test set: 0.007052810902896082\n","Training Error of current model on proposed group: 0.1515527950310559\n","Training Error of h trained on proposed group: 0.05093167701863355\n","Group size in training set: 805\n","Group weight in training set: 0.005877456849975176\n","Failed improvement check.\n","Testing Group: black\n","building h\n","finished building h\n","Error of current model on proposed group: 0.23082822085889576\n","Error of h trained on proposed group: 0.21855828220858897\n","Group size in test set: 1304\n","Group weight in test set: 0.044429301533219764\n","Training Error of current model on proposed group: 0.1774680993955675\n","Training Error of h trained on proposed group: 0.1435527199462726\n","Group size in training set: 5956\n","Group weight in training set: 0.043485879501182795\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 3\n","new errs [0.2003066439522998, 0.17482133040131942, 0.29243353783231085, 0.2621359223300971, 0.1717171717171717, 0.28846153846153844, 0.29542097488921715, 0.20082474226804126, 0.2417947652679684, 0.22548879691736834, 0.2109826589595376, 0.23125196109193602, 0.20068359375, 0.25867386276021587, 0.2320596458527493, 0.19336071184120462, 0.24347826086956526, 0.21541950113378683, 0.26726726726726724, 0.2222222222222222, 0.21855828220858897]\n","prob node 5\n","new errs [0.20023850085178874, 0.17482133040131942, 0.29243353783231085, 0.25997842502696866, 0.1717171717171717, 0.28846153846153844, 0.29542097488921715, 0.20082474226804126, 0.2417947652679684, 0.22548879691736834, 0.2109826589595376, 0.2306244116724192, 0.20068359375, 0.2582883577486508, 0.2320596458527493, 0.19336071184120462, 0.24347826086956526, 0.21541950113378683, 0.26726726726726724, 0.2222222222222222, 0.21702453987730064]\n","prob node 6\n","new errs [0.20017035775127767, 0.17482133040131942, 0.29243353783231085, 0.25997842502696866, 0.1717171717171717, 0.2692307692307693, 0.29542097488921715, 0.20082474226804126, 0.24137931034482762, 0.22548879691736834, 0.2109826589595376, 0.23031063696266085, 0.2001953125, 0.2582883577486508, 0.2320596458527493, 0.19336071184120462, 0.24347826086956526, 0.21485260770975056, 0.26726726726726724, 0.2222222222222222, 0.2154907975460123]\n","prob node 8\n","new errs [0.2001022146507666, 0.17482133040131942, 0.29243353783231085, 0.25997842502696866, 0.1717171717171717, 0.2692307692307693, 0.292466765140325, 0.20082474226804126, 0.24137931034482762, 0.22548879691736834, 0.2109826589595376, 0.23031063696266085, 0.2001953125, 0.2579028527370856, 0.2320596458527493, 0.19336071184120462, 0.24347826086956526, 0.21485260770975056, 0.26726726726726724, 0.2222222222222222, 0.21395705521472397]\n","prob node 9\n","new errs [0.19996592844974448, 0.17482133040131942, 0.29038854805725967, 0.25997842502696866, 0.1717171717171717, 0.2692307692307693, 0.292466765140325, 0.20082474226804126, 0.23971749065226422, 0.22548879691736834, 0.21387283236994215, 0.2299968622529024, 0.2001953125, 0.2582883577486508, 0.2311276794035415, 0.19301848049281312, 0.24268774703557316, 0.21485260770975056, 0.2657657657657657, 0.2222222222222222, 0.2108895705521472]\n","prob node 10\n","new errs [0.1997274275979557, 0.17372182517866963, 0.29243353783231085, 0.25997842502696866, 0.17340067340067344, 0.2692307692307693, 0.292466765140325, 0.19876288659793817, 0.23971749065226422, 0.22448979591836737, 0.21387283236994215, 0.2299968622529024, 0.19970703125, 0.2579028527370856, 0.22926374650512582, 0.1926762491444216, 0.24268774703557316, 0.2142857142857143, 0.26876876876876876, 0.2222222222222222, 0.20552147239263807]\n","prob node 11\n","new errs [0.19969335604770022, 0.17372182517866963, 0.29243353783231085, 0.25997842502696866, 0.17340067340067344, 0.2692307692307693, 0.292466765140325, 0.19876288659793817, 0.23971749065226422, 0.22448979591836737, 0.2109826589595376, 0.22968308754314404, 0.19970703125, 0.2579028527370856, 0.22926374650512582, 0.1926762491444216, 0.24268774703557316, 0.2142857142857143, 0.26876876876876876, 0.2222222222222222, 0.20475460122699385]\n","prob node -1\n","new errs [0.19965928449744463, 0.17372182517866963, 0.29243353783231085, 0.25997842502696866, 0.1717171717171717, 0.2692307692307693, 0.292466765140325, 0.19876288659793817, 0.23971749065226422, 0.22448979591836737, 0.2109826589595376, 0.22936931283338557, 0.19970703125, 0.2579028527370856, 0.22926374650512582, 0.1923340177960301, 0.24268774703557316, 0.21485260770975056, 0.26876876876876876, 0.2222222222222222, 0.20398773006134974]\n","Testing Group: US born\n","building h\n","finished building h\n","Error of current model on proposed group: 0.20401776700989305\n","Error of h trained on proposed group: 0.19563900666262868\n","Group size in test set: 19812\n","Group weight in test set: 0.6750255536626917\n","Training Error of current model on proposed group: 0.16177106765876448\n","Training Error of h trained on proposed group: 0.17760425679183245\n","Group size in training set: 92464\n","Group weight in training set: 0.6750971058088256\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 1\n","new errs [0.1940034071550255, 0.17647058823529416, 0.32515337423312884, 0.24163969795037754, 0.19528619528619529, 0.23076923076923073, 0.2703101920236337, 0.2078350515463917, 0.2451184046530951, 0.22092193520764947, 0.2225433526011561, 0.22623156573580172, 0.2060546875, 0.25250578257517353, 0.21435228331780054, 0.1923340177960301, 0.2553359683794466, 0.22278911564625847, 0.2657657657657657, 0.2222222222222222, 0.21319018404907975, 0.19563900666262868]\n","prob node 2\n","new errs [0.1938330494037479, 0.17372182517866963, 0.32515337423312884, 0.24163969795037754, 0.19528619528619529, 0.23076923076923073, 0.2703101920236337, 0.20659793814432992, 0.2451184046530951, 0.22177822177822182, 0.2225433526011561, 0.22685911515531854, 0.203125, 0.25212027756360833, 0.21435228331780054, 0.1923340177960301, 0.2553359683794466, 0.2222222222222222, 0.2657657657657657, 0.2222222222222222, 0.2108895705521472, 0.1953866343630123]\n","prob node 4\n","new errs [0.19328790459965928, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.19528619528619529, 0.23076923076923073, 0.2703101920236337, 0.20536082474226802, 0.24470294972995432, 0.2210646496360782, 0.2225433526011561, 0.22748666457483524, 0.19970703125, 0.25212027756360833, 0.21435228331780054, 0.1923340177960301, 0.2553359683794466, 0.2222222222222222, 0.2657657657657657, 0.2222222222222222, 0.21012269938650308, 0.1945790430042399]\n","prob node 7\n","new errs [0.19281090289608183, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.1717171717171717, 0.23076923076923073, 0.2703101920236337, 0.20082474226804126, 0.24262567511425015, 0.22120736406450692, 0.2225433526011561, 0.22403514276749292, 0.19970703125, 0.25212027756360833, 0.21435228331780054, 0.1923340177960301, 0.2553359683794466, 0.22278911564625847, 0.2657657657657657, 0.2222222222222222, 0.20858895705521474, 0.19387240056531396]\n","prob node 8\n","new errs [0.1926405451448041, 0.17372182517866963, 0.29243353783231085, 0.24056094929881333, 0.1717171717171717, 0.23076923076923073, 0.2703101920236337, 0.19876288659793817, 0.24262567511425015, 0.21978021978021978, 0.21965317919075145, 0.22403514276749292, 0.19970703125, 0.25212027756360833, 0.21435228331780054, 0.19336071184120462, 0.25217391304347825, 0.22052154195011342, 0.2642642642642643, 0.2222222222222222, 0.20858895705521474, 0.19362002826569757]\n","prob node 10\n","new errs [0.19240204429301533, 0.17372182517866963, 0.29243353783231085, 0.24487594390507017, 0.1717171717171717, 0.23076923076923073, 0.27474150664697194, 0.19876288659793817, 0.23971749065226422, 0.21978021978021978, 0.2167630057803468, 0.22246626921870094, 0.19677734375, 0.2536622976098689, 0.21621621621621623, 0.19096509240246407, 0.24980237154150198, 0.217687074829932, 0.2642642642642643, 0.2222222222222222, 0.20782208588957052, 0.1932667070462346]\n","prob node 16\n","new errs [0.19233390119250426, 0.17372182517866963, 0.29243353783231085, 0.24487594390507017, 0.1717171717171717, 0.23076923076923073, 0.27474150664697194, 0.19876288659793817, 0.23971749065226422, 0.21949479092336233, 0.2109826589595376, 0.2221524945089426, 0.1962890625, 0.2536622976098689, 0.21621621621621623, 0.19096509240246407, 0.24980237154150198, 0.21541950113378683, 0.2642642642642643, 0.2222222222222222, 0.2070552147239264, 0.19316575812638803]\n","prob node 17\n","new errs [0.19202725724020442, 0.17372182517866963, 0.29243353783231085, 0.24487594390507017, 0.1717171717171717, 0.23076923076923073, 0.27474150664697194, 0.19876288659793817, 0.23971749065226422, 0.21906664763807626, 0.2109826589595376, 0.22121117037966742, 0.19677734375, 0.2540478026214341, 0.21621621621621623, 0.19096509240246407, 0.24268774703557316, 0.21541950113378683, 0.2642642642642643, 0.2222222222222222, 0.20628834355828218, 0.19261053906723202]\n","prob node 7\n","new errs [0.19199318568994894, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.1717171717171717, 0.22115384615384615, 0.276218611521418, 0.19917525773195877, 0.23971749065226422, 0.21835307549593264, 0.2109826589595376, 0.22246626921870094, 0.19677734375, 0.25212027756360833, 0.21155638397017706, 0.19130732375085557, 0.24347826086956526, 0.21485260770975056, 0.2627627627627628, 0.2222222222222222, 0.2070552147239264, 0.19261053906723202]\n","prob node 20\n","new errs [0.19195911413969335, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.1717171717171717, 0.22115384615384615, 0.276218611521418, 0.19876288659793817, 0.23971749065226422, 0.21835307549593264, 0.2109826589595376, 0.22246626921870094, 0.19677734375, 0.25212027756360833, 0.21155638397017706, 0.19130732375085557, 0.24268774703557316, 0.21485260770975056, 0.2627627627627628, 0.2222222222222222, 0.20628834355828218, 0.19261053906723202]\n","prob node -1\n","new errs [0.1918568994889267, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.1717171717171717, 0.23076923076923073, 0.276218611521418, 0.19876288659793817, 0.23971749065226422, 0.21778221778221774, 0.2109826589595376, 0.22246626921870094, 0.197265625, 0.25173477255204313, 0.21155638397017706, 0.19096509240246407, 0.24268774703557316, 0.21485260770975056, 0.2627627627627628, 0.2222222222222222, 0.20398773006134974, 0.1924591156874621]\n","Testing Group: fifties\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2214153014693464\n","Error of h trained on proposed group: 0.2300287113663233\n","Group size in test set: 5921\n","Group weight in test set: 0.20173764906303238\n","Training Error of current model on proposed group: 0.18672214182344427\n","Training Error of h trained on proposed group: 0.1780028943560058\n","Group size in training set: 27640\n","Group weight in training set: 0.20180485383020355\n","Failed improvement check.\n","Testing Group: MIL\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2596153846153846\n","Error of h trained on proposed group: 0.1923076923076923\n","Group size in test set: 104\n","Group weight in test set: 0.0035434412265758094\n","Training Error of current model on proposed group: 0.13003663003663002\n","Training Error of h trained on proposed group: 0.012820512820512775\n","Group size in training set: 546\n","Group weight in training set: 0.003986448993896207\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node -1\n","new errs [0.19161839863713803, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.1717171717171717, 0.23076923076923073, 0.276218611521418, 0.19876288659793817, 0.23971749065226422, 0.2174967889253604, 0.2109826589595376, 0.22026984625039225, 0.197265625, 0.25173477255204313, 0.21155638397017706, 0.19096509240246407, 0.24268774703557316, 0.2142857142857143, 0.2627627627627628, 0.2222222222222222, 0.20322085889570551, 0.19210579446799925, 0.1923076923076923]\n","Testing Group: white\n","building h\n","finished building h\n","Error of current model on proposed group: 0.19656478077155248\n","Error of h trained on proposed group: 0.196455029358503\n","Group size in test set: 18223\n","Group weight in test set: 0.6208858603066439\n","Training Error of current model on proposed group: 0.16223269214873592\n","Training Error of h trained on proposed group: 0.178787771440717\n","Group size in training set: 84687\n","Group weight in training set: 0.618315761806022\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 1\n","new errs [0.19155025553662697, 0.18416712479384278, 0.30470347648261764, 0.25458468176914784, 0.16666666666666663, 0.21153846153846156, 0.2717872968980798, 0.2070103092783505, 0.2434565849605318, 0.22620236905951197, 0.18786127167630062, 0.22372136805773457, 0.201171875, 0.25057825751734775, 0.2199440820130475, 0.19301848049281312, 0.2490118577075099, 0.21315192743764177, 0.2702702702702703, 0.21111111111111114, 0.20322085889570551, 0.1927114879870785, 0.1923076923076923, 0.196455029358503]\n","prob node 2\n","new errs [0.1909028960817717, 0.17372182517866963, 0.30470347648261764, 0.25458468176914784, 0.16666666666666663, 0.21153846153846156, 0.2717872968980798, 0.20329896907216494, 0.24387203988367268, 0.22334808049093768, 0.18786127167630062, 0.2227800439284594, 0.20068359375, 0.24942174248265225, 0.2199440820130475, 0.19301848049281312, 0.2490118577075099, 0.21145124716553287, 0.2702702702702703, 0.21111111111111114, 0.20322085889570551, 0.1919038966283061, 0.1923076923076923, 0.19541239093453333]\n","prob node 3\n","new errs [0.1906984667802385, 0.17372182517866963, 0.29243353783231085, 0.25458468176914784, 0.16666666666666663, 0.21153846153846156, 0.2717872968980798, 0.20412371134020624, 0.24470294972995432, 0.22263450834879406, 0.18786127167630062, 0.2221524945089426, 0.19775390625, 0.25134926754047804, 0.2199440820130475, 0.19301848049281312, 0.2490118577075099, 0.21145124716553287, 0.2702702702702703, 0.21111111111111114, 0.20322085889570551, 0.19180294770845951, 0.1923076923076923, 0.1950831366953849]\n","prob node 7\n","new errs [0.1902896081771721, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.16666666666666663, 0.21153846153846156, 0.2717872968980798, 0.20329896907216494, 0.2451184046530951, 0.22206365063507916, 0.18786127167630062, 0.22152494508942577, 0.1962890625, 0.25096376252891284, 0.2199440820130475, 0.19301848049281312, 0.2490118577075099, 0.2108843537414966, 0.2702702702702703, 0.19999999999999996, 0.20322085889570551, 0.19180294770845951, 0.1923076923076923, 0.19442462821708828]\n","prob node 8\n","new errs [0.1899148211243612, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15319865319865322, 0.21153846153846156, 0.2717872968980798, 0.19876288659793817, 0.24470294972995432, 0.22149279292136437, 0.19075144508670516, 0.22152494508942577, 0.1962890625, 0.25096376252891284, 0.2190121155638397, 0.19370294318959613, 0.2474308300395257, 0.20804988662131518, 0.26876876876876876, 0.19999999999999996, 0.20322085889570551, 0.19170199878861294, 0.1923076923076923, 0.1938209954453164]\n","prob node 9\n","new errs [0.1895059625212947, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.22115384615384615, 0.27326440177252587, 0.19876288659793817, 0.23971749065226422, 0.22149279292136437, 0.19075144508670516, 0.2199560715406338, 0.19482421875, 0.24980724749421745, 0.21714818266542402, 0.19062286105407256, 0.24664031620553362, 0.20975056689342408, 0.2657657657657657, 0.19999999999999996, 0.20322085889570551, 0.1912477286493035, 0.1826923076923077, 0.19316248696701965]\n","prob node 5\n","new errs [0.1885519591141397, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.24038461538461542, 0.27474150664697194, 0.19876288659793817, 0.23971749065226422, 0.2174967889253604, 0.19075144508670516, 0.21587700031377466, 0.1923828125, 0.25019275250578255, 0.20969245107176138, 0.19130732375085557, 0.24584980237154153, 0.2091836734693877, 0.2657657657657657, 0.19999999999999996, 0.20322085889570551, 0.18983444377145164, 0.1826923076923077, 0.19162596718432745]\n","prob node 15\n","new errs [0.1885178875638842, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27474150664697194, 0.19876288659793817, 0.23971749065226422, 0.2174967889253604, 0.19075144508670516, 0.21587700031377466, 0.1923828125, 0.25057825751734775, 0.20969245107176138, 0.19130732375085557, 0.24584980237154153, 0.2091836734693877, 0.2657657657657657, 0.19999999999999996, 0.20322085889570551, 0.18983444377145164, 0.1826923076923077, 0.19157109147780282]\n","prob node 16\n","new errs [0.18848381601362862, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27474150664697194, 0.19876288659793817, 0.23971749065226422, 0.2174967889253604, 0.19075144508670516, 0.21681832444304994, 0.1923828125, 0.25096376252891284, 0.20969245107176138, 0.19096509240246407, 0.24584980237154153, 0.20804988662131518, 0.2657657657657657, 0.21111111111111114, 0.20322085889570551, 0.18973349485160507, 0.1826923076923077, 0.1915162157712781]\n","prob node 18\n","new errs [0.1883475298126065, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27474150664697194, 0.19876288659793817, 0.23971749065226422, 0.21735407449693167, 0.19075144508670516, 0.21619077502353312, 0.19287109375, 0.25057825751734775, 0.20969245107176138, 0.19096509240246407, 0.24268774703557316, 0.20804988662131518, 0.2657657657657657, 0.21111111111111114, 0.20322085889570551, 0.18958207147183526, 0.1826923076923077, 0.1912418372386544]\n","prob node -1\n","new errs [0.18827938671209543, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27474150664697194, 0.19876288659793817, 0.23971749065226422, 0.21735407449693167, 0.19075144508670516, 0.21619077502353312, 0.19287109375, 0.25057825751734775, 0.20969245107176138, 0.19096509240246407, 0.24268774703557316, 0.20861678004535145, 0.2627627627627628, 0.2222222222222222, 0.20322085889570551, 0.18943064809206545, 0.1826923076923077, 0.19113208582560504]\n","Testing Group: other race\n","building h\n","finished building h\n","Error of current model on proposed group: 0.17021276595744683\n","Error of h trained on proposed group: 0.22042553191489367\n","Group size in test set: 1175\n","Group weight in test set: 0.04003407155025554\n","Training Error of current model on proposed group: 0.14256127027959964\n","Training Error of h trained on proposed group: 0.10994131860545386\n","Group size in training set: 5794\n","Group weight in training set: 0.04230308694255425\n","Failed improvement check.\n","Testing Group: male\n","building h\n","finished building h\n","Error of current model on proposed group: 0.19475704156638263\n","Error of h trained on proposed group: 0.20132700188642427\n","Group size in test set: 15373\n","Group weight in test set: 0.5237819420783646\n","Training Error of current model on proposed group: 0.16487678124697658\n","Training Error of h trained on proposed group: 0.18068858757999195\n","Group size in training set: 72351\n","Group weight in training set: 0.5282482988230484\n","Failed improvement check.\n","Testing Group: thirties\n","building h\n","finished building h\n","Error of current model on proposed group: 0.19824421388667202\n","Error of h trained on proposed group: 0.21197126895450913\n","Group size in test set: 6265\n","Group weight in test set: 0.21345826235093696\n","Training Error of current model on proposed group: 0.1767372646733112\n","Training Error of h trained on proposed group: 0.1777408637873754\n","Group size in training set: 28896\n","Group weight in training set: 0.21097514675389153\n","Failed improvement check.\n","Testing Group: forties\n","building h\n","finished building h\n","Error of current model on proposed group: 0.19761529019814128\n","Error of h trained on proposed group: 0.21129230229703666\n","Group size in test set: 5703\n","Group weight in test set: 0.19431005110732538\n","Training Error of current model on proposed group: 0.17126623376623373\n","Training Error of h trained on proposed group: 0.17027007083825263\n","Group size in training set: 27104\n","Group weight in training set: 0.19789141672264243\n","Failed improvement check.\n","Testing Group: high school grad\n","building h\n","finished building h\n","Error of current model on proposed group: 0.18628088426527956\n","Error of h trained on proposed group: 0.19237646293888166\n","Group size in test set: 12304\n","Group weight in test set: 0.41921635434412263\n","Training Error of current model on proposed group: 0.1678695350451076\n","Training Error of h trained on proposed group: 0.1733344899375434\n","Group size in training set: 57640\n","Group weight in training set: 0.4208405128354896\n","Failed improvement check.\n","Testing Group: full time\n","building h\n","finished building h\n","Error of current model on proposed group: 0.20693082727149503\n","Error of h trained on proposed group: 0.21632855916504767\n","Group size in test set: 22133\n","Group weight in test set: 0.7541056218057922\n","Training Error of current model on proposed group: 0.18085978568052707\n","Training Error of h trained on proposed group: 0.19842395131031954\n","Group size in training set: 103677\n","Group weight in training set: 0.7569653339563681\n","Failed improvement check.\n","Testing Group: OFF\n","building h\n","finished building h\n","Error of current model on proposed group: 0.22432762836185816\n","Error of h trained on proposed group: 0.23838630806845962\n","Group size in test set: 3272\n","Group weight in test set: 0.11148211243611585\n","Training Error of current model on proposed group: 0.2134340999413643\n","Training Error of h trained on proposed group: 0.180402632093296\n","Group size in training set: 15349\n","Group weight in training set: 0.11206594433573785\n","Failed improvement check.\n","Testing Group: MGR\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2074992055926279\n","Error of h trained on proposed group: 0.2176676199555132\n","Group size in test set: 3147\n","Group weight in test set: 0.10722316865417376\n","Training Error of current model on proposed group: 0.17575023549993274\n","Training Error of h trained on proposed group: 0.14318395909029735\n","Group size in training set: 14862\n","Group weight in training set: 0.10851026547121871\n","Failed improvement check.\n","Testing Group: female\n","building h\n","finished building h\n","Error of current model on proposed group: 0.1811547542391071\n","Error of h trained on proposed group: 0.18637762037633254\n","Group size in test set: 13977\n","Group weight in test set: 0.4762180579216354\n","Training Error of current model on proposed group: 0.1522294275145869\n","Training Error of h trained on proposed group: 0.1615619147849504\n","Group size in training set: 64613\n","Group weight in training set: 0.4717517011769516\n","Failed improvement check.\n","Testing Group: part time\n","building h\n","finished building h\n","Error of current model on proposed group: 0.10599669906473497\n","Error of h trained on proposed group: 0.10599669906473497\n","Group size in test set: 5453\n","Group weight in test set: 0.18579216354344122\n","Training Error of current model on proposed group: 0.0696800727646617\n","Training Error of h trained on proposed group: 0.06533001146834339\n","Group size in training set: 25287\n","Group weight in training set: 0.18462515697555562\n","Failed improvement check.\n","Testing Group: asian\n","building h\n","finished building h\n","Error of current model on proposed group: 0.20283616105343127\n","Error of h trained on proposed group: 0.21701696632058753\n","Group size in test set: 3949\n","Group weight in test set: 0.13454855195911414\n","Training Error of current model on proposed group: 0.1650716409840497\n","Training Error of h trained on proposed group: 0.1619356582860233\n","Group size in training set: 18495\n","Group weight in training set: 0.13503548377675886\n","Failed improvement check.\n","Testing Group: asian\n","building h\n","finished building h\n","Error of current model on proposed group: 0.20283616105343127\n","Error of h trained on proposed group: 0.21701696632058753\n","Group size in test set: 3949\n","Group weight in test set: 0.13454855195911414\n","Training Error of current model on proposed group: 0.1650716409840497\n","Training Error of h trained on proposed group: 0.1619356582860233\n","Group size in training set: 18495\n","Group weight in training set: 0.13503548377675886\n","Failed improvement check.\n","Testing Group: PRD\n","building h\n","finished building h\n","Error of current model on proposed group: 0.19192688499619193\n","Error of h trained on proposed group: 0.2231530845392231\n","Group size in test set: 1313\n","Group weight in test set: 0.04473594548551959\n","Training Error of current model on proposed group: 0.18567466144558653\n","Training Error of h trained on proposed group: 0.12824278022515911\n","Group size in training set: 6129\n","Group weight in training set: 0.04474898513477994\n","Failed improvement check.\n","Testing Group: native american\n","building h\n","finished building h\n","Error of current model on proposed group: 0.19252873563218387\n","Error of h trained on proposed group: 0.23275862068965514\n","Group size in test set: 348\n","Group weight in test set: 0.011856899488926747\n","Training Error of current model on proposed group: 0.1487702459508098\n","Training Error of h trained on proposed group: 0.05818836232753455\n","Group size in training set: 1667\n","Group weight in training set: 0.012171081452060395\n","Failed improvement check.\n","Testing Group: mar minorities\n","building h\n","finished building h\n","Error of current model on proposed group: 0.20837209302325577\n","Error of h trained on proposed group: 0.2613953488372093\n","Group size in test set: 1075\n","Group weight in test set: 0.036626916524701875\n","Training Error of current model on proposed group: 0.16557511256651658\n","Training Error of h trained on proposed group: 0.12075317232910354\n","Group size in training set: 4886\n","Group weight in training set: 0.03567360766332759\n","Failed improvement check.\n","Testing Group: for profit worker\n","building h\n","finished building h\n","Error of current model on proposed group: 0.17333472978634268\n","Error of h trained on proposed group: 0.17899036447423544\n","Group size in test set: 19096\n","Group weight in test set: 0.6506303236797274\n","Training Error of current model on proposed group: 0.15201990338379834\n","Training Error of h trained on proposed group: 0.1602646346769605\n","Group size in training set: 89633\n","Group weight in training set: 0.6544274407873601\n","Failed improvement check.\n","Testing Group: single\n","building h\n","finished building h\n","Error of current model on proposed group: 0.14410735122520424\n","Error of h trained on proposed group: 0.15256709451575268\n","Group size in test set: 10284\n","Group weight in test set: 0.3503918228279387\n","Training Error of current model on proposed group: 0.1164663662159906\n","Training Error of h trained on proposed group: 0.12746202637289272\n","Group size in training set: 47928\n","Group weight in training set: 0.349931368826845\n","Failed improvement check.\n","Testing Group: MED\n","building h\n","finished building h\n","Error of current model on proposed group: 0.21038790269559504\n","Error of h trained on proposed group: 0.22287968441814598\n","Group size in test set: 1521\n","Group weight in test set: 0.05182282793867121\n","Training Error of current model on proposed group: 0.15573199609973531\n","Training Error of h trained on proposed group: 0.1333054743000418\n","Group size in training set: 7179\n","Group weight in training set: 0.05241523319996495\n","Failed improvement check.\n","Testing Group: advanced degree\n","building h\n","finished building h\n","Error of current model on proposed group: 0.1686831084225221\n","Error of h trained on proposed group: 0.18310842252210335\n","Group size in test set: 4298\n","Group weight in test set: 0.14643952299829643\n","Training Error of current model on proposed group: 0.12864208412908285\n","Training Error of h trained on proposed group: 0.13461632633073795\n","Group size in training set: 20421\n","Group weight in training set: 0.1490975730848982\n","Failed improvement check.\n","Testing Group: twenties\n","building h\n","finished building h\n","Error of current model on proposed group: 0.1065002901915264\n","Error of h trained on proposed group: 0.10998258850841558\n","Group size in test set: 6892\n","Group weight in test set: 0.23482112436115843\n","Training Error of current model on proposed group: 0.0811795584375482\n","Training Error of h trained on proposed group: 0.08368071638104058\n","Group size in training set: 32385\n","Group weight in training set: 0.2364489938962063\n","Failed improvement check.\n","Testing Group: CMM\n","building h\n","finished building h\n","Error of current model on proposed group: 0.14674441205053446\n","Error of h trained on proposed group: 0.1671525753158406\n","Group size in test set: 1029\n","Group weight in test set: 0.03505962521294719\n","Training Error of current model on proposed group: 0.13876524095219667\n","Training Error of h trained on proposed group: 0.08902651441842457\n","Group size in training set: 5167\n","Group weight in training set: 0.03772524166934377\n","Failed improvement check.\n","Testing Group: multiple race\n","building h\n","finished building h\n","Error of current model on proposed group: 0.17119954194102494\n","Error of h trained on proposed group: 0.173203549957057\n","Group size in test set: 3493\n","Group weight in test set: 0.11901192504258944\n","Training Error of current model on proposed group: 0.13336691644103016\n","Training Error of h trained on proposed group: 0.12669227378628545\n","Group size in training set: 15881\n","Group weight in training set: 0.11595017668876494\n","Failed improvement check.\n","Testing Group: american\n","building h\n","finished building h\n","Error of current model on proposed group: 0.16946902654867257\n","Error of h trained on proposed group: 0.1825221238938053\n","Group size in test set: 4520\n","Group weight in test set: 0.15400340715502556\n","Training Error of current model on proposed group: 0.13542702856330824\n","Training Error of h trained on proposed group: 0.13272701435270717\n","Group size in training set: 21111\n","Group weight in training set: 0.1541353932420198\n","Failed improvement check.\n","Testing Group: TRN\n","building h\n","finished building h\n","Error of current model on proposed group: 0.18668528864059586\n","Error of h trained on proposed group: 0.18482309124767227\n","Group size in test set: 2148\n","Group weight in test set: 0.07318568994889267\n","Training Error of current model on proposed group: 0.15188938146506226\n","Training Error of h trained on proposed group: 0.11842938568714378\n","Group size in training set: 9474\n","Group weight in training set: 0.06917146111386933\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 7\n","new errs [0.1881431005110733, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27474150664697194, 0.19999999999999996, 0.23722476111341917, 0.2169259312116455, 0.19075144508670516, 0.2171320991528083, 0.19140625, 0.25057825751734775, 0.20969245107176138, 0.19096509240246407, 0.24268774703557316, 0.21315192743764177, 0.2627627627627628, 0.23333333333333328, 0.20628834355828218, 0.18811831213406016, 0.1826923076923077, 0.19025407452120946, 0.18482309124767227]\n","prob node 11\n","new errs [0.18804088586030665, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27474150664697194, 0.19876288659793817, 0.23764021603656005, 0.21649778792635932, 0.19075144508670516, 0.2171320991528083, 0.19140625, 0.25057825751734775, 0.20969245107176138, 0.19096509240246407, 0.24268774703557316, 0.21315192743764177, 0.2627627627627628, 0.23333333333333328, 0.20398773006134974, 0.187916414294367, 0.1826923076923077, 0.1903089502277342, 0.18342644320297952]\n","prob node 17\n","new errs [0.18793867120954, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27474150664697194, 0.19876288659793817, 0.23805567095970093, 0.21664050235478804, 0.19075144508670516, 0.21619077502353312, 0.19140625, 0.25057825751734775, 0.20969245107176138, 0.19096509240246407, 0.24268774703557316, 0.21315192743764177, 0.2627627627627628, 0.23333333333333328, 0.20475460122699385, 0.18796688875429035, 0.1826923076923077, 0.19025407452120946, 0.18202979515828677]\n","prob node 19\n","new errs [0.18766609880749574, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27474150664697194, 0.19835051546391758, 0.23764021603656005, 0.2163550734979306, 0.19075144508670516, 0.21619077502353312, 0.19140625, 0.24903623747108716, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20861678004535145, 0.2627627627627628, 0.23333333333333328, 0.20322085889570551, 0.18801736321421358, 0.1826923076923077, 0.1904735773473083, 0.17877094972067042]\n","prob node 14\n","new errs [0.18763202725724015, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.19835051546391758, 0.23764021603656005, 0.2163550734979306, 0.19075144508670516, 0.21587700031377466, 0.19140625, 0.24903623747108716, 0.21062441752096928, 0.19096509240246407, 0.24189723320158107, 0.20861678004535145, 0.2627627627627628, 0.2222222222222222, 0.20322085889570551, 0.18801736321421358, 0.1826923076923077, 0.1904735773473083, 0.17830540037243947]\n","prob node -1\n","new errs [0.18759795570698468, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.19835051546391758, 0.23764021603656005, 0.2163550734979306, 0.19075144508670516, 0.21587700031377466, 0.19140625, 0.24903623747108716, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20861678004535145, 0.2627627627627628, 0.21111111111111114, 0.20322085889570551, 0.18801736321421358, 0.1826923076923077, 0.19041870164078367, 0.17830540037243947]\n","Testing Group: ENG\n","building h\n","finished building h\n","Error of current model on proposed group: 0.14204545454545459\n","Error of h trained on proposed group: 0.15767045454545459\n","Group size in test set: 704\n","Group weight in test set: 0.023986371379897786\n","Training Error of current model on proposed group: 0.12488493402884315\n","Training Error of h trained on proposed group: 0.07394906413010127\n","Group size in training set: 3259\n","Group weight in training set: 0.023794573756607575\n","Failed improvement check.\n","Testing Group: no school\n","building h\n","finished building h\n","Error of current model on proposed group: 0.11776061776061775\n","Error of h trained on proposed group: 0.16023166023166024\n","Group size in test set: 518\n","Group weight in test set: 0.017649063032367972\n","Training Error of current model on proposed group: 0.10353430353430348\n","Training Error of h trained on proposed group: 0.0619542619542619\n","Group size in training set: 2405\n","Group weight in training set: 0.017559358663590433\n","Failed improvement check.\n","Testing Group: some school\n","building h\n","finished building h\n","Error of current model on proposed group: 0.12322443181818177\n","Error of h trained on proposed group: 0.12926136363636365\n","Group size in test set: 2816\n","Group weight in test set: 0.09594548551959114\n","Training Error of current model on proposed group: 0.09836699945824623\n","Training Error of h trained on proposed group: 0.0862162371333488\n","Group size in training set: 12921\n","Group weight in training set: 0.0943386583335767\n","Failed improvement check.\n","Testing Group: PRS\n","building h\n","finished building h\n","Error of current model on proposed group: 0.129883843717001\n","Error of h trained on proposed group: 0.12354804646251316\n","Group size in test set: 947\n","Group weight in test set: 0.03226575809199318\n","Training Error of current model on proposed group: 0.11261999531725586\n","Training Error of h trained on proposed group: 0.06064153594006083\n","Group size in training set: 4271\n","Group weight in training set: 0.031183376653719227\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 7\n","new errs [0.18739352640545148, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.19917525773195877, 0.23930203572912334, 0.21492792921364345, 0.19075144508670516, 0.21587700031377466, 0.19189453125, 0.2455666923670008, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20748299319727892, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.18731072077528765, 0.1826923076923077, 0.1898699445755364, 0.17830540037243947, 0.12354804646251316]\n","prob node 8\n","new errs [0.18732538330494042, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.19835051546391758, 0.23888658080598257, 0.21492792921364345, 0.19075144508670516, 0.21587700031377466, 0.19189453125, 0.2455666923670008, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20748299319727892, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.1872097718554412, 0.1826923076923077, 0.18981506886901167, 0.17830540037243947, 0.12143611404435062]\n","prob node 11\n","new errs [0.18722316865417377, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.19835051546391758, 0.23764021603656005, 0.21492792921364345, 0.19075144508670516, 0.21650454973329147, 0.19140625, 0.2447956823438705, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20691609977324266, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.1872097718554412, 0.1826923076923077, 0.1897053174559622, 0.17830540037243947, 0.11826821541710664]\n","prob node -1\n","new errs [0.1871550255536627, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.19835051546391758, 0.23764021603656005, 0.21507064364207218, 0.19075144508670516, 0.21587700031377466, 0.19140625, 0.2447956823438705, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20691609977324266, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.1872097718554412, 0.1826923076923077, 0.18965044174943757, 0.17830540037243947, 0.11615628299894398]\n","Testing Group: CLN\n","building h\n","finished building h\n","Error of current model on proposed group: 0.13979496738117425\n","Error of h trained on proposed group: 0.12395153774464118\n","Group size in test set: 1073\n","Group weight in test set: 0.0365587734241908\n","Training Error of current model on proposed group: 0.10688082490581008\n","Training Error of h trained on proposed group: 0.07158437438032916\n","Group size in training set: 5043\n","Group weight in training set: 0.03681989427878859\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 9\n","new errs [0.1865758091993186, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.1950515463917526, 0.2368093061902784, 0.21564150135578708, 0.19075144508670516, 0.2149356761844995, 0.189453125, 0.2447956823438705, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20804988662131518, 0.2627627627627628, 0.2222222222222222, 0.20168711656441718, 0.18635170603674545, 0.1826923076923077, 0.18899193327114083, 0.17830540037243947, 0.11615628299894398, 0.12395153774464118]\n","prob node 13\n","new errs [0.18643952299829647, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.2368093061902784, 0.21507064364207218, 0.19075144508670516, 0.2143081267649828, 0.18896484375, 0.2455666923670008, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20804988662131518, 0.2627627627627628, 0.21111111111111114, 0.20168711656441718, 0.1863012315768221, 0.1826923076923077, 0.18899193327114083, 0.17830540037243947, 0.11615628299894398, 0.12022367194780992]\n","prob node 20\n","new errs [0.1863713798977853, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.2368093061902784, 0.21507064364207218, 0.19075144508670516, 0.2143081267649828, 0.18896484375, 0.2447956823438705, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.2063492063492064, 0.2627627627627628, 0.21111111111111114, 0.20168711656441718, 0.18640218049666868, 0.1826923076923077, 0.18904680897766557, 0.17830540037243947, 0.11615628299894398, 0.11835973904939423]\n","prob node 25\n","new errs [0.18633730834752982, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.2368093061902784, 0.21507064364207218, 0.19075144508670516, 0.2143081267649828, 0.1884765625, 0.2447956823438705, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.2063492063492064, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.18635170603674545, 0.1826923076923077, 0.18904680897766557, 0.17830540037243947, 0.11615628299894398, 0.11742777260018644]\n","prob node 9\n","new errs [0.1861669505962521, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.23597839634399664, 0.21649778792635932, 0.19075144508670516, 0.21399435205522432, 0.18798828125, 0.2444101773323053, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.2063492063492064, 0.2627627627627628, 0.2222222222222222, 0.2024539877300614, 0.18625075711689887, 0.1826923076923077, 0.1889370575646161, 0.17830540037243947, 0.11087645195353746, 0.11742777260018644]\n","prob node 19\n","new errs [0.18582623509369678, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.23597839634399664, 0.21507064364207218, 0.19075144508670516, 0.2133668026357075, 0.1875, 0.24325366229760992, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20578231292517002, 0.2627627627627628, 0.2222222222222222, 0.20168711656441718, 0.18579648697758933, 0.1826923076923077, 0.18844317620589368, 0.17830540037243947, 0.10031678986272441, 0.11742777260018644]\n","prob node 20\n","new errs [0.1857921635434412, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.23597839634399664, 0.21507064364207218, 0.19075144508670516, 0.21305302792594916, 0.1875, 0.24325366229760992, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20578231292517002, 0.2627627627627628, 0.21111111111111114, 0.20168711656441718, 0.18579648697758933, 0.1826923076923077, 0.18838830049936894, 0.17830540037243947, 0.09926082365364308, 0.11742777260018644]\n","prob node -1\n","new errs [0.18575809199318571, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.23597839634399664, 0.21507064364207218, 0.19075144508670516, 0.2127392532161908, 0.1875, 0.24325366229760992, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20578231292517002, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.1857460125176661, 0.1826923076923077, 0.18838830049936894, 0.17830540037243947, 0.09820485744456176, 0.11742777260018644]\n","Testing Group: HLS\n","building h\n","finished building h\n","Error of current model on proposed group: 0.13844621513944222\n","Error of h trained on proposed group: 0.14143426294820716\n","Group size in test set: 1004\n","Group weight in test set: 0.034207836456558775\n","Training Error of current model on proposed group: 0.09564628919467633\n","Training Error of h trained on proposed group: 0.06090683510038353\n","Group size in training set: 4433\n","Group weight in training set: 0.03236616921234777\n","Failed improvement check.\n","Testing Group: FFF\n","building h\n","finished building h\n","Error of current model on proposed group: 0.07493540051679581\n","Error of h trained on proposed group: 0.06459948320413433\n","Group size in test set: 387\n","Group weight in test set: 0.013185689948892675\n","Training Error of current model on proposed group: 0.08079748163693601\n","Training Error of h trained on proposed group: 0.017313746065057756\n","Group size in training set: 1906\n","Group weight in training set: 0.013916065535469174\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node -1\n","new errs [0.18562180579216359, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.235147486497715, 0.21507064364207218, 0.19075144508670516, 0.2127392532161908, 0.1865234375, 0.24325366229760992, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.2046485260770975, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.18544316575812636, 0.1826923076923077, 0.18822367337979473, 0.17830540037243947, 0.09820485744456176, 0.11742777260018644, 0.06459948320413433]\n","Testing Group: EAT\n","building h\n","finished building h\n","Error of current model on proposed group: 0.06183510638297873\n","Error of h trained on proposed group: 0.061170212765957466\n","Group size in test set: 1504\n","Group weight in test set: 0.05124361158432709\n","Training Error of current model on proposed group: 0.054785020804438256\n","Training Error of h trained on proposed group: 0.02760055478502077\n","Group size in training set: 7210\n","Group weight in training set: 0.05264157004760375\n","Passed checks.\n","Running Update\n","updating errors\n","getting new errors\n","prob node 9\n","new errs [0.185587734241908, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.2318238471125883, 0.21535607249892963, 0.19075144508670516, 0.21085660495764036, 0.18701171875, 0.243639167309175, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20408163265306123, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.18519079345850997, 0.1826923076923077, 0.18772979202107232, 0.17830540037243947, 0.09820485744456176, 0.11742777260018644, 0.06459948320413433, 0.061170212765957466]\n","prob node 12\n","new errs [0.18551959114139693, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.2318238471125883, 0.21507064364207218, 0.19075144508670516, 0.21117037966739882, 0.1875, 0.24325366229760992, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.2029478458049887, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.18519079345850997, 0.1826923076923077, 0.18783954343412168, 0.17830540037243947, 0.09820485744456176, 0.11742777260018644, 0.06459948320413433, 0.05984042553191493]\n","prob node 27\n","new errs [0.18545144804088587, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.23223930203572918, 0.21492792921364345, 0.19075144508670516, 0.21117037966739882, 0.1865234375, 0.24325366229760992, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.2029478458049887, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.18519079345850997, 0.1826923076923077, 0.18772979202107232, 0.17830540037243947, 0.09820485744456176, 0.11742777260018644, 0.06201550387596899, 0.059175531914893664]\n","prob node 9\n","new errs [0.18531516183986374, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.2318238471125883, 0.21549878692735835, 0.19075144508670516, 0.212111703796674, 0.1865234375, 0.24325366229760992, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.20408163265306123, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.18524126791843332, 0.1826923076923077, 0.1875651649014981, 0.17830540037243947, 0.09820485744456176, 0.11742777260018644, 0.05167958656330751, 0.059175531914893664]\n","prob node 12\n","new errs [0.1852129471890971, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.2318238471125883, 0.21507064364207218, 0.19075144508670516, 0.21148415437715717, 0.18701171875, 0.24286815728604472, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.2029478458049887, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.1850898445386634, 0.1826923076923077, 0.1874005377819239, 0.17830540037243947, 0.09820485744456176, 0.11742777260018644, 0.043927648578811374, 0.059175531914893664]\n","prob node -1\n","new errs [0.1851788756388416, 0.17372182517866963, 0.29243353783231085, 0.24163969795037754, 0.15656565656565657, 0.23076923076923073, 0.27326440177252587, 0.194639175257732, 0.2318238471125883, 0.21492792921364345, 0.19075144508670516, 0.21148415437715717, 0.1865234375, 0.24286815728604472, 0.20969245107176138, 0.19096509240246407, 0.24189723320158107, 0.2029478458049887, 0.2627627627627628, 0.21111111111111114, 0.20092024539877296, 0.18503937007874016, 0.1826923076923077, 0.18734566207539927, 0.17830540037243947, 0.09820485744456176, 0.11742777260018644, 0.04134366925064603, 0.059175531914893664]\n","Testing Group: black men\n","building h\n","finished building h\n","Error of current model on proposed group: 0.20341614906832295\n","Error of h trained on proposed group: 0.24689440993788825\n","Group size in test set: 644\n","Group weight in test set: 0.021942078364565586\n","Training Error of current model on proposed group: 0.1585704371963914\n","Training Error of h trained on proposed group: 0.12213740458015265\n","Group size in training set: 2882\n","Group weight in training set: 0.02104202564177448\n","Failed improvement check.\n","Testing Group: black women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.1984848484848485\n","Error of h trained on proposed group: 0.24242424242424243\n","Group size in test set: 660\n","Group weight in test set: 0.022487223168654175\n","Training Error of current model on proposed group: 0.1538711776187378\n","Training Error of h trained on proposed group: 0.12556929082628498\n","Group size in training set: 3074\n","Group weight in training set: 0.02244385385940831\n","Failed improvement check.\n","Testing Group: white men\n","building h\n","finished building h\n","Error of current model on proposed group: 0.18994933305759487\n","Error of h trained on proposed group: 0.20401199462309993\n","Group size in test set: 9671\n","Group weight in test set: 0.3295059625212947\n","Training Error of current model on proposed group: 0.16199960342814335\n","Training Error of h trained on proposed group: 0.1726850117869968\n","Group size in training set: 45389\n","Group weight in training set: 0.3313936508863643\n","Failed improvement check.\n","Testing Group: white women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.1844013096351731\n","Error of h trained on proposed group: 0.19176800748362954\n","Group size in test set: 8552\n","Group weight in test set: 0.29137989778534923\n","Training Error of current model on proposed group: 0.1576925034352893\n","Training Error of h trained on proposed group: 0.15924474527965804\n","Group size in training set: 39298\n","Group weight in training set: 0.28692211091965775\n","Failed improvement check.\n","Testing Group: asian men\n","building h\n","finished building h\n","Error of current model on proposed group: 0.1868962620747585\n","Error of h trained on proposed group: 0.20999580008399832\n","Group size in test set: 2381\n","Group weight in test set: 0.0811243611584327\n","Training Error of current model on proposed group: 0.15625823451910403\n","Training Error of h trained on proposed group: 0.14510320597277115\n","Group size in training set: 11385\n","Group weight in training set: 0.08312403259250606\n","Failed improvement check.\n","Testing Group: asian_women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.19291014014839236\n","Error of h trained on proposed group: 0.21434460016488044\n","Group size in test set: 2426\n","Group weight in test set: 0.08265758091993186\n","Training Error of current model on proposed group: 0.15853027427979993\n","Training Error of h trained on proposed group: 0.1442987752285665\n","Group size in training set: 11594\n","Group weight in training set: 0.08464998101690956\n","Failed improvement check.\n","Testing Group: native american men\n","building h\n","finished building h\n","Error of current model on proposed group: 0.1657142857142857\n","Error of h trained on proposed group: 0.2171428571428572\n","Group size in test set: 175\n","Group weight in test set: 0.00596252129471891\n","Training Error of current model on proposed group: 0.13808975834292292\n","Training Error of h trained on proposed group: 0.0736478711162255\n","Group size in training set: 869\n","Group weight in training set: 0.006344732922519786\n","Failed improvement check.\n","Testing Group: natie american women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.18497109826589597\n","Error of h trained on proposed group: 0.2601156069364162\n","Group size in test set: 173\n","Group weight in test set: 0.005894378194207836\n","Training Error of current model on proposed group: 0.14661654135338342\n","Training Error of h trained on proposed group: 0.02130325814536338\n","Group size in training set: 798\n","Group weight in training set: 0.00582634852954061\n","Failed improvement check.\n","Testing Group: middle aged black\n","building h\n","finished building h\n","Error of current model on proposed group: 0.25113464447806355\n","Error of h trained on proposed group: 0.29198184568835095\n","Group size in test set: 661\n","Group weight in test set: 0.02252129471890971\n","Training Error of current model on proposed group: 0.2038523274478331\n","Training Error of h trained on proposed group: 0.15313001605136434\n","Group size in training set: 3115\n","Group weight in training set: 0.0227432025933822\n","Failed improvement check.\n","Testing Group: middle aged white\n","building h\n","finished building h\n","Error of current model on proposed group: 0.21049933353839845\n","Error of h trained on proposed group: 0.22106018660924842\n","Group size in test set: 9753\n","Group weight in test set: 0.33229982964224875\n","Training Error of current model on proposed group: 0.18341244725738393\n","Training Error of h trained on proposed group: 0.1878955696202531\n","Group size in training set: 45504\n","Group weight in training set: 0.3322332875792179\n","Failed improvement check.\n","Testing Group: middle aged asian\n","building h\n","finished building h\n","Error of current model on proposed group: 0.20376055257099002\n","Error of h trained on proposed group: 0.22102839600920954\n","Group size in test set: 2606\n","Group weight in test set: 0.08879045996592845\n","Training Error of current model on proposed group: 0.1702953052014884\n","Training Error of h trained on proposed group: 0.15936980444937054\n","Group size in training set: 12631\n","Group weight in training set: 0.09222131362985894\n","Failed improvement check.\n","Testing Group: middle aged native american\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2325581395348837\n","Error of h trained on proposed group: 0.313953488372093\n","Group size in test set: 172\n","Group weight in test set: 0.0058603066439522995\n","Training Error of current model on proposed group: 0.1701346389228886\n","Training Error of h trained on proposed group: 0.05630354957160344\n","Group size in training set: 817\n","Group weight in training set: 0.0059650711135772905\n","Failed improvement check.\n","Testing Group: overtime white\n","building h\n","finished building h\n","Error of current model on proposed group: 0.1831470335339639\n","Error of h trained on proposed group: 0.2003439380911436\n","Group size in test set: 1163\n","Group weight in test set: 0.0396252129471891\n","Training Error of current model on proposed group: 0.15157661810805823\n","Training Error of h trained on proposed group: 0.12446985063617921\n","Group size in training set: 5423\n","Group weight in training set: 0.03959434595952221\n","Failed improvement check.\n","Testing Group: overtime black\n","building h\n","finished building h\n","Error of current model on proposed group: 0.20731707317073167\n","Error of h trained on proposed group: 0.24390243902439024\n","Group size in test set: 82\n","Group weight in test set: 0.0027938671209540035\n","Training Error of current model on proposed group: 0.1498559077809798\n","Training Error of h trained on proposed group: 0.01729106628242072\n","Group size in training set: 347\n","Group weight in training set: 0.0025335124558278086\n","Failed improvement check.\n","Testing Group: overtime asian\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2449799196787149\n","Error of h trained on proposed group: 0.2449799196787149\n","Group size in test set: 249\n","Group weight in test set: 0.00848381601362862\n","Training Error of current model on proposed group: 0.1293402777777778\n","Training Error of h trained on proposed group: 0.07986111111111116\n","Group size in training set: 1152\n","Group weight in training set: 0.008410969305802984\n","Failed improvement check.\n","Testing Group: overtime native american\n","building h\n","finished building h\n","Error of current model on proposed group: 0.17142857142857137\n","Error of h trained on proposed group: 0.4571428571428572\n","Group size in test set: 35\n","Group weight in test set: 0.001192504258943782\n","Training Error of current model on proposed group: 0.1839080459770115\n","Training Error of h trained on proposed group: 0.0\n","Group size in training set: 87\n","Group weight in training set: 0.0006352034111153296\n","Failed improvement check.\n","Testing Group: married men\n","building h\n","finished building h\n","Error of current model on proposed group: 0.21121562090725088\n","Error of h trained on proposed group: 0.22931301345398258\n","Group size in test set: 8399\n","Group weight in test set: 0.28616695059625213\n","Training Error of current model on proposed group: 0.17857500500700985\n","Training Error of h trained on proposed group: 0.1908171440016022\n","Group size in training set: 39944\n","Group weight in training set: 0.29163867877690486\n","Failed improvement check.\n","Testing Group: married women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.1993067590987868\n","Error of h trained on proposed group: 0.2097053726169844\n","Group size in test set: 6924\n","Group weight in test set: 0.2359114139693356\n","Training Error of current model on proposed group: 0.16965578000125858\n","Training Error of h trained on proposed group: 0.16666666666666663\n","Group size in training set: 31782\n","Group weight in training set: 0.23204637715020004\n","Failed improvement check.\n","Testing Group: divorced women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.23463687150837986\n","Error of h trained on proposed group: 0.2811918063314711\n","Group size in test set: 1074\n","Group weight in test set: 0.036592844974446335\n","Training Error of current model on proposed group: 0.20487019521030392\n","Training Error of h trained on proposed group: 0.1551620044274502\n","Group size in training set: 4969\n","Group weight in training set: 0.03627960631990888\n","Failed improvement check.\n","Testing Group: single men\n","building h\n","finished building h\n","Error of current model on proposed group: 0.14793433158939207\n","Error of h trained on proposed group: 0.15695471766191593\n","Group size in test set: 5543\n","Group weight in test set: 0.18885860306643953\n","Training Error of current model on proposed group: 0.1179437439379244\n","Training Error of h trained on proposed group: 0.12426770126091169\n","Group size in training set: 25775\n","Group weight in training set: 0.18818813702870826\n","Failed improvement check.\n","Testing Group: single women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.13267243197637624\n","Error of h trained on proposed group: 0.14849187935034802\n","Group size in test set: 4741\n","Group weight in test set: 0.16153321976149915\n","Training Error of current model on proposed group: 0.10784092447975446\n","Training Error of h trained on proposed group: 0.10328172256579249\n","Group size in training set: 22153\n","Group weight in training set: 0.16174323179813674\n","Failed improvement check.\n","Testing Group: divorced black women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.32098765432098764\n","Error of h trained on proposed group: 0.3827160493827161\n","Group size in test set: 81\n","Group weight in test set: 0.0027597955706984666\n","Training Error of current model on proposed group: 0.2351598173515982\n","Training Error of h trained on proposed group: 0.1004566210045662\n","Group size in training set: 438\n","Group weight in training set: 0.0031979206214771765\n","Failed improvement check.\n","Testing Group: single black women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.14984709480122327\n","Error of h trained on proposed group: 0.17737003058103973\n","Group size in test set: 327\n","Group weight in test set: 0.011141396933560477\n","Training Error of current model on proposed group: 0.1062124248496994\n","Training Error of h trained on proposed group: 0.05611222444889774\n","Group size in training set: 1497\n","Group weight in training set: 0.010929879384363775\n","Failed improvement check.\n","Testing Group: divorced white women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2198198198198198\n","Error of h trained on proposed group: 0.28918918918918923\n","Group size in test set: 1110\n","Group weight in test set: 0.037819420783645655\n","Training Error of current model on proposed group: 0.19161327897495628\n","Training Error of h trained on proposed group: 0.15375655212580086\n","Group size in training set: 5151\n","Group weight in training set: 0.03760842265120762\n","Failed improvement check.\n","Testing Group: single white women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.13270676691729322\n","Error of h trained on proposed group: 0.15864661654135337\n","Group size in test set: 2660\n","Group weight in test set: 0.09063032367972743\n","Training Error of current model on proposed group: 0.1104080987029421\n","Training Error of h trained on proposed group: 0.0972793419803859\n","Group size in training set: 12644\n","Group weight in training set: 0.09231622908209457\n","Failed improvement check.\n","Testing Group: divorced asian women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.23469387755102045\n","Error of h trained on proposed group: 0.2755102040816326\n","Group size in test set: 196\n","Group weight in test set: 0.006678023850085179\n","Training Error of current model on proposed group: 0.18008948545861303\n","Training Error of h trained on proposed group: 0.0738255033557047\n","Group size in training set: 894\n","Group weight in training set: 0.006527262638357525\n","Failed improvement check.\n","Testing Group: single asian women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.17861975642760486\n","Error of h trained on proposed group: 0.21650879566982406\n","Group size in test set: 739\n","Group weight in test set: 0.025178875638841566\n","Training Error of current model on proposed group: 0.13140643623361148\n","Training Error of h trained on proposed group: 0.08611442193087004\n","Group size in training set: 3356\n","Group weight in training set: 0.024502789054058\n","Failed improvement check.\n","Testing Group: divorced_native_american_women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.2777777777777778\n","Error of h trained on proposed group: 0.2777777777777778\n","Group size in test set: 18\n","Group weight in test set: 0.0006132879045996593\n","Training Error of current model on proposed group: 0.12195121951219512\n","Training Error of h trained on proposed group: 0.0\n","Group size in training set: 82\n","Group weight in training set: 0.0005986974679477819\n","Failed improvement check.\n","Testing Group: single native american women\n","building h\n","finished building h\n","Error of current model on proposed group: 0.09523809523809523\n","Error of h trained on proposed group: 0.19047619047619047\n","Group size in test set: 63\n","Group weight in test set: 0.0021465076660988074\n","Training Error of current model on proposed group: 0.10738255033557043\n","Training Error of h trained on proposed group: 0.020134228187919434\n","Group size in training set: 298\n","Group weight in training set: 0.0021757542127858417\n","Failed improvement check.\n"]}]},{"cell_type":"markdown","metadata":{"id":"zlnnHF4Iyo2a"},"source":["# Saving Your Model"]},{"cell_type":"markdown","metadata":{"id":"Z08dwAENyo2b"},"source":["We'd like to output the PDL to some permanent location for grading purposes. The lines below do this."]},{"cell_type":"code","execution_count":26,"metadata":{"pycharm":{"name":"#%%\n"},"id":"32vNl5BVyo2b","executionInfo":{"status":"ok","timestamp":1648666932244,"user_tz":240,"elapsed":227,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"outputs":[],"source":["# you will probably need to install dill, which you do w pip install dill in your command line\n","with open('pdl.pkl', 'wb') as pickle_file:\n","    pickle.dump(f, pickle_file)"]},{"cell_type":"markdown","metadata":{"id":"DeuYYAvcyo2b"},"source":["If you saved your PDL to pdl.pkl and you want to reload it, you can do so as follows (instead of re-building it from scratch every time you shut down your kernel). Just be sure that your final PDL is fully replicable in the final version of your code, so that we can re-build it just given your gs and hs."]},{"cell_type":"code","execution_count":27,"metadata":{"pycharm":{"name":"#%%\n"},"id":"VdGJ12_9yo2b","executionInfo":{"status":"ok","timestamp":1648673147107,"user_tz":240,"elapsed":119,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"outputs":[],"source":["with open('pdl.pkl', 'rb') as pickle_file:\n","    content = pickle.load(pickle_file)"]},{"cell_type":"code","source":["pprint(vars(f))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-Ht17-Hmf--","executionInfo":{"status":"ok","timestamp":1648673594850,"user_tz":240,"elapsed":544,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}},"outputId":"b390def1-ccee-4e70-87ba-3193c7afb13b"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["{'all_groups': None,\n"," 'curr_node': <__main__.PointerDecisionListNode object at 0x7f4c97d98f90>,\n"," 'head': <__main__.PointerDecisionListNode object at 0x7f4c97d98f90>,\n"," 'leaves': [<bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=1, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=10, random_state=0)>],\n"," 'num_rounds': 28,\n"," 'pred_names': ['Total',\n","                'EDU',\n","                'CMS',\n","                'ENT',\n","                'PRT',\n","                'LGL',\n","                'RPR',\n","                'local govt worker',\n","                'associates degree',\n","                'bachelors degree',\n","                'SCI',\n","                'cow minorities',\n","                'non profit worker',\n","                'self employed worker',\n","                'BUS',\n","                'SAL',\n","                'CON',\n","                'over time',\n","                'FIN',\n","                'oceanian',\n","                'black',\n","                'US born',\n","                'MIL',\n","                'white',\n","                'TRN',\n","                'PRS',\n","                'CLN',\n","                'FFF',\n","                'EAT'],\n"," 'predicates': [<function DecisionlistNode.__init__.<locals>.<lambda> at 0x7f4c9822f050>,\n","                <function EDU at 0x7f4c981ae5f0>,\n","                <function CMS at 0x7f4c981ae3b0>,\n","                <function ENT at 0x7f4c981ae710>,\n","                <function PRT at 0x7f4c981aea70>,\n","                <function LGL at 0x7f4c981ae4d0>,\n","                <function RPR at 0x7f4c981b04d0>,\n","                <function local_govt_worker at 0x7f4c9822ccb0>,\n","                <function assoc_degree at 0x7f4c981c74d0>,\n","                <function bachelor_degree at 0x7f4c981c7440>,\n","                <function SCI at 0x7f4c981ae290>,\n","                <function cow_minorities at 0x7f4c9822cf80>,\n","                <function non_profit_worker at 0x7f4c9820f3b0>,\n","                <function self_employed_worker at 0x7f4c9822c3b0>,\n","                <function BUS at 0x7f4c9822fa70>,\n","                <function SAL at 0x7f4c981aeef0>,\n","                <function CON at 0x7f4c981b0290>,\n","                <function over_time at 0x7f4c981c7560>,\n","                <function FIN at 0x7f4c9822fdd0>,\n","                <function oceanian at 0x7f4c981b0dd0>,\n","                <function black at 0x7f4c981cbe60>,\n","                <function us_born at 0x7f4c981b0950>,\n","                <function MIL at 0x7f4c981b0830>,\n","                <function white at 0x7f4c981cb950>,\n","                <function TRN at 0x7f4c981b0710>,\n","                <function PRS at 0x7f4c981aedd0>,\n","                <function CLN at 0x7f4c981aecb0>,\n","                <function FFF at 0x7f4c981b0170>,\n","                <function EAT at 0x7f4c981aeb90>],\n"," 'test_errors': [[0.2796252129471891,\n","                  0.5519516217702034,\n","                  0.5603271983640081,\n","                  0.5544768069039914,\n","                  0.48484848484848486,\n","                  0.40384615384615385,\n","                  0.431314623338257,\n","                  0.385979381443299,\n","                  0.36352305774823435,\n","                  0.3462252033680605,\n","                  0.32369942196531787,\n","                  0.342955757765924,\n","                  0.33642578125,\n","                  0.35389360061680797,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.3378684807256236,\n","                  0.3558558558558559,\n","                  0.3555555555555555,\n","                  0.30828220858895705,\n","                  0.2994144962648899,\n","                  0.3173076923076923,\n","                  0.29457279262470504,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.2571039182282794,\n","                  0.18856514568444205,\n","                  0.5603271983640081,\n","                  0.5544768069039914,\n","                  0.48484848484848486,\n","                  0.40384615384615385,\n","                  0.431314623338257,\n","                  0.2964948453608247,\n","                  0.32405484004985463,\n","                  0.31268731268731265,\n","                  0.32369942196531787,\n","                  0.31848133040476934,\n","                  0.28173828125,\n","                  0.33770239013107173,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.3378684807256236,\n","                  0.3558558558558559,\n","                  0.34444444444444444,\n","                  0.2845092024539877,\n","                  0.27493438320209973,\n","                  0.3173076923076923,\n","                  0.2726225100148164,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.2531856899488927,\n","                  0.18856514568444205,\n","                  0.32515337423312884,\n","                  0.5544768069039914,\n","                  0.48484848484848486,\n","                  0.40384615384615385,\n","                  0.431314623338257,\n","                  0.29072164948453605,\n","                  0.3232239302035729,\n","                  0.30569430569430567,\n","                  0.32369942196531787,\n","                  0.3153435833071855,\n","                  0.2529296875,\n","                  0.33616037008481114,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.3361678004535147,\n","                  0.3558558558558559,\n","                  0.34444444444444444,\n","                  0.27377300613496935,\n","                  0.27079547748839083,\n","                  0.3173076923076923,\n","                  0.26927509191680843,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.24398637137989776,\n","                  0.18856514568444205,\n","                  0.32515337423312884,\n","                  0.2632146709816613,\n","                  0.48484848484848486,\n","                  0.40384615384615385,\n","                  0.431314623338257,\n","                  0.28907216494845356,\n","                  0.3144993768176153,\n","                  0.29499072356215217,\n","                  0.32369942196531787,\n","                  0.3103231879510512,\n","                  0.2412109375,\n","                  0.28257517347725525,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.33390022675736963,\n","                  0.3558558558558559,\n","                  0.3222222222222222,\n","                  0.26457055214723924,\n","                  0.26065011104381186,\n","                  0.3173076923076923,\n","                  0.2581901991988147,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.2379897785349233,\n","                  0.18856514568444205,\n","                  0.32515337423312884,\n","                  0.2632146709816613,\n","                  0.18855218855218858,\n","                  0.40384615384615385,\n","                  0.431314623338257,\n","                  0.23587628865979382,\n","                  0.30078936435396764,\n","                  0.28728414442700156,\n","                  0.32369942196531787,\n","                  0.29118293065578915,\n","                  0.2412109375,\n","                  0.28257517347725525,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.3163265306122449,\n","                  0.3558558558558559,\n","                  0.33333333333333337,\n","                  0.25996932515337423,\n","                  0.25272562083585703,\n","                  0.3173076923076923,\n","                  0.25078197881797726,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.23768313458262347,\n","                  0.18856514568444205,\n","                  0.32515337423312884,\n","                  0.2632146709816613,\n","                  0.18855218855218858,\n","                  0.3173076923076923,\n","                  0.431314623338257,\n","                  0.23711340206185572,\n","                  0.2995429995845451,\n","                  0.2874268588554303,\n","                  0.32369942196531787,\n","                  0.29055538123627234,\n","                  0.24072265625,\n","                  0.28180416345412496,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.3168934240362812,\n","                  0.3558558558558559,\n","                  0.33333333333333337,\n","                  0.25920245398773,\n","                  0.252523722996164,\n","                  0.3173076923076923,\n","                  0.25061735169840316,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.23478705281090284,\n","                  0.18856514568444205,\n","                  0.32515337423312884,\n","                  0.2632146709816613,\n","                  0.18855218855218858,\n","                  0.3173076923076923,\n","                  0.3057607090103397,\n","                  0.23546391752577323,\n","                  0.28915662650602414,\n","                  0.28599971457114315,\n","                  0.32369942196531787,\n","                  0.2858487605898965,\n","                  0.240234375,\n","                  0.28218966846569005,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.31065759637188206,\n","                  0.3558558558558559,\n","                  0.33333333333333337,\n","                  0.25230061349693256,\n","                  0.2487886129618413,\n","                  0.3173076923076923,\n","                  0.24694067936124675,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.23356047700170357,\n","                  0.18856514568444205,\n","                  0.3210633946830266,\n","                  0.2632146709816613,\n","                  0.18855218855218858,\n","                  0.28846153846153844,\n","                  0.3057607090103397,\n","                  0.22061855670103092,\n","                  0.28749480681346073,\n","                  0.2862851434280006,\n","                  0.32369942196531787,\n","                  0.2858487605898965,\n","                  0.240234375,\n","                  0.28218966846569005,\n","                  0.3280521901211556,\n","                  0.3199863107460643,\n","                  0.31067193675889326,\n","                  0.31065759637188206,\n","                  0.35285285285285284,\n","                  0.34444444444444444,\n","                  0.2538343558282209,\n","                  0.2467696345649102,\n","                  0.3173076923076923,\n","                  0.24551391099160402,\n","                  0.20623836126629425,\n","                  0.11193241816261879,\n","                  0.12395153774464118,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.23039182282793869,\n","                  0.1869158878504673,\n","                  0.3210633946830266,\n","                  0.2632146709816613,\n","                  0.18855218855218858,\n","                  0.28846153846153844,\n","                  0.30132939438700146,\n","                  0.21896907216494843,\n","                  0.24885749896136267,\n","                  0.2862851434280006,\n","                  0.3063583815028902,\n","                  0.27957326639472857,\n","                  0.236328125,\n","                  0.28411719352351583,\n","                  0.3140726933830382,\n","                  0.31040383299110197,\n","                  0.31225296442687744,\n","                  0.3032879818594104,\n","                  0.3198198198198198,\n","                  0.34444444444444444,\n","                  0.2507668711656442,\n","                  0.24232788209166167,\n","                  0.3173076923076923,\n","                  0.24095922735005215,\n","                  0.2085661080074488,\n","                  0.11298838437170011,\n","                  0.1230195712954334,\n","                  0.06201550387596899,\n","                  0.059175531914893664],\n","                 [0.21655877342419083,\n","                  0.18031885651456847,\n","                  0.31492842535787324,\n","                  0.2632146709816613,\n","                  0.17845117845117842,\n","                  0.28846153846153844,\n","                  0.30132939438700146,\n","                  0.20412371134020624,\n","                  0.24885749896136267,\n","                  0.22834308548594262,\n","                  0.2774566473988439,\n","                  0.2629432067775337,\n","                  0.22607421875,\n","                  0.28257517347725525,\n","                  0.28704566635601114,\n","                  0.24435318275154005,\n","                  0.3003952569169961,\n","                  0.27494331065759636,\n","                  0.29579579579579585,\n","                  0.33333333333333337,\n","                  0.24003067484662577,\n","                  0.2241570765192813,\n","                  0.25,\n","                  0.22257586566427046,\n","                  0.2039106145251397,\n","                  0.11510031678986277,\n","                  0.12488350419384897,\n","                  0.056847545219638196,\n","                  0.05984042553191493],\n","                 [0.2162180579216354,\n","                  0.18031885651456847,\n","                  0.31492842535787324,\n","                  0.2632146709816613,\n","                  0.17845117845117842,\n","                  0.28846153846153844,\n","                  0.30132939438700146,\n","                  0.20412371134020624,\n","                  0.2484420440382219,\n","                  0.22834308548594262,\n","                  0.24855491329479773,\n","                  0.26388453090680886,\n","                  0.2236328125,\n","                  0.28296067848882034,\n","                  0.28704566635601114,\n","                  0.24435318275154005,\n","                  0.3003952569169961,\n","                  0.27380952380952384,\n","                  0.29579579579579585,\n","                  0.33333333333333337,\n","                  0.23849693251533743,\n","                  0.22360185746012518,\n","                  0.25,\n","                  0.2223563628381715,\n","                  0.2039106145251397,\n","                  0.11510031678986277,\n","                  0.12488350419384897,\n","                  0.056847545219638196,\n","                  0.05984042553191493],\n","                 [0.21284497444633732,\n","                  0.18031885651456847,\n","                  0.31492842535787324,\n","                  0.2632146709816613,\n","                  0.17845117845117842,\n","                  0.28846153846153844,\n","                  0.2983751846381093,\n","                  0.20412371134020624,\n","                  0.24719567926879937,\n","                  0.2276295133437991,\n","                  0.2225433526011561,\n","                  0.232820834640728,\n","                  0.2236328125,\n","                  0.28296067848882034,\n","                  0.2786579683131407,\n","                  0.231006160164271,\n","                  0.28774703557312253,\n","                  0.2647392290249433,\n","                  0.30930930930930933,\n","                  0.33333333333333337,\n","                  0.23619631901840488,\n","                  0.2200181708055724,\n","                  0.23076923076923073,\n","                  0.219118696153213,\n","                  0.19692737430167595,\n","                  0.11510031678986277,\n","                  0.1342031686859273,\n","                  0.05167958656330751,\n","                  0.06382978723404253],\n","                 [0.2114821124361158,\n","                  0.18031885651456847,\n","                  0.30265848670756645,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.28846153846153844,\n","                  0.2983751846381093,\n","                  0.20412371134020624,\n","                  0.24719567926879937,\n","                  0.2276295133437991,\n","                  0.2167630057803468,\n","                  0.232820834640728,\n","                  0.2041015625,\n","                  0.28296067848882034,\n","                  0.2712022367194781,\n","                  0.2306639288158795,\n","                  0.28774703557312253,\n","                  0.26077097505668934,\n","                  0.3078078078078078,\n","                  0.3222222222222222,\n","                  0.23389570552147243,\n","                  0.21835251362810415,\n","                  0.23076923076923073,\n","                  0.2175821763705208,\n","                  0.19878957169459965,\n","                  0.11615628299894398,\n","                  0.1351351351351351,\n","                  0.054263565891472854,\n","                  0.06382978723404253],\n","                 [0.20940374787052807,\n","                  0.17757009345794394,\n","                  0.30265848670756645,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.28846153846153844,\n","                  0.29542097488921715,\n","                  0.20412371134020624,\n","                  0.24054840049854587,\n","                  0.22605965463108324,\n","                  0.2109826589595376,\n","                  0.232820834640728,\n","                  0.2041015625,\n","                  0.25944487278334616,\n","                  0.26374650512581543,\n","                  0.2265571526351814,\n","                  0.28537549407114626,\n","                  0.26077097505668934,\n","                  0.3003003003003003,\n","                  0.3222222222222222,\n","                  0.23006134969325154,\n","                  0.2154754694124773,\n","                  0.23076923076923073,\n","                  0.21571640234868028,\n","                  0.2011173184357542,\n","                  0.12249208025343195,\n","                  0.13699906803355077,\n","                  0.056847545219638196,\n","                  0.0644946808510638],\n","                 [0.20824531516183986,\n","                  0.17757009345794394,\n","                  0.30265848670756645,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.28846153846153844,\n","                  0.29542097488921715,\n","                  0.20412371134020624,\n","                  0.24054840049854587,\n","                  0.22605965463108324,\n","                  0.2109826589595376,\n","                  0.232820834640728,\n","                  0.2041015625,\n","                  0.25944487278334616,\n","                  0.2320596458527493,\n","                  0.2265571526351814,\n","                  0.28537549407114626,\n","                  0.26190476190476186,\n","                  0.3003003003003003,\n","                  0.3111111111111111,\n","                  0.22929447852760731,\n","                  0.21436503129416518,\n","                  0.23076923076923073,\n","                  0.21478351533775997,\n","                  0.2011173184357542,\n","                  0.12249208025343195,\n","                  0.13699906803355077,\n","                  0.056847545219638196,\n","                  0.0644946808510638],\n","                 [0.20514480408858604,\n","                  0.17757009345794394,\n","                  0.30265848670756645,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.28846153846153844,\n","                  0.29542097488921715,\n","                  0.20412371134020624,\n","                  0.24054840049854587,\n","                  0.22605965463108324,\n","                  0.2109826589595376,\n","                  0.232820834640728,\n","                  0.203125,\n","                  0.25944487278334616,\n","                  0.2320596458527493,\n","                  0.19541409993155368,\n","                  0.28537549407114626,\n","                  0.24773242630385484,\n","                  0.3003003003003003,\n","                  0.30000000000000004,\n","                  0.23006134969325154,\n","                  0.21032707450030286,\n","                  0.23076923076923073,\n","                  0.2103934588157823,\n","                  0.2011173184357542,\n","                  0.12249208025343195,\n","                  0.13699906803355077,\n","                  0.056847545219638196,\n","                  0.0644946808510638],\n","                 [0.20330494037478708,\n","                  0.17757009345794394,\n","                  0.30265848670756645,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.28846153846153844,\n","                  0.29542097488921715,\n","                  0.20412371134020624,\n","                  0.2401329455754051,\n","                  0.22605965463108324,\n","                  0.2109826589595376,\n","                  0.232820834640728,\n","                  0.2021484375,\n","                  0.25944487278334616,\n","                  0.2320596458527493,\n","                  0.19541409993155368,\n","                  0.24268774703557316,\n","                  0.24319727891156462,\n","                  0.3003003003003003,\n","                  0.3111111111111111,\n","                  0.23006134969325154,\n","                  0.20765192812436906,\n","                  0.23076923076923073,\n","                  0.20721066783734843,\n","                  0.2011173184357542,\n","                  0.12249208025343195,\n","                  0.13699906803355077,\n","                  0.056847545219638196,\n","                  0.0644946808510638],\n","                 [0.20183986371379903,\n","                  0.1759208356239692,\n","                  0.2985685071574642,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.2692307692307693,\n","                  0.29394387001477107,\n","                  0.20247422680412375,\n","                  0.2401329455754051,\n","                  0.22477522477522482,\n","                  0.2109826589595376,\n","                  0.22936931283338557,\n","                  0.20068359375,\n","                  0.25944487278334616,\n","                  0.22926374650512582,\n","                  0.1926762491444216,\n","                  0.24268774703557316,\n","                  0.21882086167800452,\n","                  0.2987987987987988,\n","                  0.3222222222222222,\n","                  0.23159509202453987,\n","                  0.20613769432667073,\n","                  0.23076923076923073,\n","                  0.2053448938155079,\n","                  0.2001862197392924,\n","                  0.12143611404435062,\n","                  0.136067101584343,\n","                  0.06459948320413433,\n","                  0.06781914893617025],\n","                 [0.20115843270868827,\n","                  0.1759208356239692,\n","                  0.2985685071574642,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.2692307692307693,\n","                  0.29394387001477107,\n","                  0.20247422680412375,\n","                  0.2401329455754051,\n","                  0.22477522477522482,\n","                  0.2109826589595376,\n","                  0.22936931283338557,\n","                  0.20068359375,\n","                  0.25944487278334616,\n","                  0.22926374650512582,\n","                  0.1926762491444216,\n","                  0.24268774703557316,\n","                  0.217687074829932,\n","                  0.26876876876876876,\n","                  0.3222222222222222,\n","                  0.23082822085889576,\n","                  0.2055824752675146,\n","                  0.23076923076923073,\n","                  0.2047412610437359,\n","                  0.2001862197392924,\n","                  0.12143611404435062,\n","                  0.136067101584343,\n","                  0.06459948320413433,\n","                  0.06781914893617025],\n","                 [0.20085178875638843,\n","                  0.1759208356239692,\n","                  0.2965235173824131,\n","                  0.25997842502696866,\n","                  0.17508417508417506,\n","                  0.2692307692307693,\n","                  0.292466765140325,\n","                  0.20164948453608245,\n","                  0.23971749065226422,\n","                  0.22448979591836737,\n","                  0.2109826589595376,\n","                  0.22936931283338557,\n","                  0.20068359375,\n","                  0.25905936777178107,\n","                  0.22926374650512582,\n","                  0.1923340177960301,\n","                  0.24268774703557316,\n","                  0.2159863945578231,\n","                  0.26876876876876876,\n","                  0.2222222222222222,\n","                  0.23082822085889576,\n","                  0.2055824752675146,\n","                  0.23076923076923073,\n","                  0.2047412610437359,\n","                  0.19972067039106145,\n","                  0.12143611404435062,\n","                  0.1351351351351351,\n","                  0.06459948320413433,\n","                  0.06781914893617025],\n","                 [0.19965928449744463,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.25997842502696866,\n","                  0.1717171717171717,\n","                  0.2692307692307693,\n","                  0.292466765140325,\n","                  0.19876288659793817,\n","                  0.23971749065226422,\n","                  0.22448979591836737,\n","                  0.2109826589595376,\n","                  0.22936931283338557,\n","                  0.19970703125,\n","                  0.2579028527370856,\n","                  0.22926374650512582,\n","                  0.1923340177960301,\n","                  0.24268774703557316,\n","                  0.21485260770975056,\n","                  0.26876876876876876,\n","                  0.2222222222222222,\n","                  0.20398773006134974,\n","                  0.20401776700989305,\n","                  0.23076923076923073,\n","                  0.2047412610437359,\n","                  0.1936685288640596,\n","                  0.12143611404435062,\n","                  0.1332712022367195,\n","                  0.06459948320413433,\n","                  0.06648936170212771],\n","                 [0.1918568994889267,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.1717171717171717,\n","                  0.23076923076923073,\n","                  0.276218611521418,\n","                  0.19876288659793817,\n","                  0.23971749065226422,\n","                  0.21778221778221774,\n","                  0.2109826589595376,\n","                  0.22246626921870094,\n","                  0.197265625,\n","                  0.25173477255204313,\n","                  0.21155638397017706,\n","                  0.19096509240246407,\n","                  0.24268774703557316,\n","                  0.21485260770975056,\n","                  0.2627627627627628,\n","                  0.2222222222222222,\n","                  0.20398773006134974,\n","                  0.1924591156874621,\n","                  0.2596153846153846,\n","                  0.1968940350107008,\n","                  0.19320297951582865,\n","                  0.12038014783526929,\n","                  0.13886300093196646,\n","                  0.06718346253229979,\n","                  0.06183510638297873],\n","                 [0.19161839863713803,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.1717171717171717,\n","                  0.23076923076923073,\n","                  0.276218611521418,\n","                  0.19876288659793817,\n","                  0.23971749065226422,\n","                  0.2174967889253604,\n","                  0.2109826589595376,\n","                  0.22026984625039225,\n","                  0.197265625,\n","                  0.25173477255204313,\n","                  0.21155638397017706,\n","                  0.19096509240246407,\n","                  0.24268774703557316,\n","                  0.2142857142857143,\n","                  0.2627627627627628,\n","                  0.2222222222222222,\n","                  0.20322085889570551,\n","                  0.19210579446799925,\n","                  0.1923076923076923,\n","                  0.19656478077155248,\n","                  0.19320297951582865,\n","                  0.12038014783526929,\n","                  0.13886300093196646,\n","                  0.06718346253229979,\n","                  0.06183510638297873],\n","                 [0.18827938671209543,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.15656565656565657,\n","                  0.23076923076923073,\n","                  0.27474150664697194,\n","                  0.19876288659793817,\n","                  0.23971749065226422,\n","                  0.21735407449693167,\n","                  0.19075144508670516,\n","                  0.21619077502353312,\n","                  0.19287109375,\n","                  0.25057825751734775,\n","                  0.20969245107176138,\n","                  0.19096509240246407,\n","                  0.24268774703557316,\n","                  0.20861678004535145,\n","                  0.2627627627627628,\n","                  0.2222222222222222,\n","                  0.20322085889570551,\n","                  0.18943064809206545,\n","                  0.1826923076923077,\n","                  0.19113208582560504,\n","                  0.18668528864059586,\n","                  0.13093980992608234,\n","                  0.13979496738117425,\n","                  0.07493540051679581,\n","                  0.06183510638297873],\n","                 [0.18759795570698468,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.15656565656565657,\n","                  0.23076923076923073,\n","                  0.27326440177252587,\n","                  0.19835051546391758,\n","                  0.23764021603656005,\n","                  0.2163550734979306,\n","                  0.19075144508670516,\n","                  0.21587700031377466,\n","                  0.19140625,\n","                  0.24903623747108716,\n","                  0.20969245107176138,\n","                  0.19096509240246407,\n","                  0.24189723320158107,\n","                  0.20861678004535145,\n","                  0.2627627627627628,\n","                  0.21111111111111114,\n","                  0.20322085889570551,\n","                  0.18801736321421358,\n","                  0.1826923076923077,\n","                  0.19041870164078367,\n","                  0.17830540037243947,\n","                  0.129883843717001,\n","                  0.13979496738117425,\n","                  0.07493540051679581,\n","                  0.06183510638297873],\n","                 [0.1871550255536627,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.15656565656565657,\n","                  0.23076923076923073,\n","                  0.27326440177252587,\n","                  0.19835051546391758,\n","                  0.23764021603656005,\n","                  0.21507064364207218,\n","                  0.19075144508670516,\n","                  0.21587700031377466,\n","                  0.19140625,\n","                  0.2447956823438705,\n","                  0.20969245107176138,\n","                  0.19096509240246407,\n","                  0.24189723320158107,\n","                  0.20691609977324266,\n","                  0.2627627627627628,\n","                  0.21111111111111114,\n","                  0.20092024539877296,\n","                  0.1872097718554412,\n","                  0.1826923076923077,\n","                  0.18965044174943757,\n","                  0.17830540037243947,\n","                  0.11615628299894398,\n","                  0.13979496738117425,\n","                  0.07493540051679581,\n","                  0.06183510638297873],\n","                 [0.18575809199318571,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.15656565656565657,\n","                  0.23076923076923073,\n","                  0.27326440177252587,\n","                  0.194639175257732,\n","                  0.23597839634399664,\n","                  0.21507064364207218,\n","                  0.19075144508670516,\n","                  0.2127392532161908,\n","                  0.1875,\n","                  0.24325366229760992,\n","                  0.20969245107176138,\n","                  0.19096509240246407,\n","                  0.24189723320158107,\n","                  0.20578231292517002,\n","                  0.2627627627627628,\n","                  0.21111111111111114,\n","                  0.20092024539877296,\n","                  0.1857460125176661,\n","                  0.1826923076923077,\n","                  0.18838830049936894,\n","                  0.17830540037243947,\n","                  0.09820485744456176,\n","                  0.11742777260018644,\n","                  0.07493540051679581,\n","                  0.06183510638297873],\n","                 [0.18562180579216359,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.15656565656565657,\n","                  0.23076923076923073,\n","                  0.27326440177252587,\n","                  0.194639175257732,\n","                  0.235147486497715,\n","                  0.21507064364207218,\n","                  0.19075144508670516,\n","                  0.2127392532161908,\n","                  0.1865234375,\n","                  0.24325366229760992,\n","                  0.20969245107176138,\n","                  0.19096509240246407,\n","                  0.24189723320158107,\n","                  0.2046485260770975,\n","                  0.2627627627627628,\n","                  0.21111111111111114,\n","                  0.20092024539877296,\n","                  0.18544316575812636,\n","                  0.1826923076923077,\n","                  0.18822367337979473,\n","                  0.17830540037243947,\n","                  0.09820485744456176,\n","                  0.11742777260018644,\n","                  0.06459948320413433,\n","                  0.06183510638297873],\n","                 [0.1851788756388416,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.15656565656565657,\n","                  0.23076923076923073,\n","                  0.27326440177252587,\n","                  0.194639175257732,\n","                  0.2318238471125883,\n","                  0.21492792921364345,\n","                  0.19075144508670516,\n","                  0.21148415437715717,\n","                  0.1865234375,\n","                  0.24286815728604472,\n","                  0.20969245107176138,\n","                  0.19096509240246407,\n","                  0.24189723320158107,\n","                  0.2029478458049887,\n","                  0.2627627627627628,\n","                  0.21111111111111114,\n","                  0.20092024539877296,\n","                  0.18503937007874016,\n","                  0.1826923076923077,\n","                  0.18734566207539927,\n","                  0.17830540037243947,\n","                  0.09820485744456176,\n","                  0.11742777260018644,\n","                  0.04134366925064603,\n","                  0.059175531914893664]],\n"," 'track_rejects': [1],\n"," 'train_errors': [[0.28344674513010715,\n","                   0.5546695662679715,\n","                   0.5438673068529027,\n","                   0.5312569521690768,\n","                   0.5147113594040968,\n","                   0.437984496124031,\n","                   0.42611894543225015,\n","                   0.38288740754269013,\n","                   0.3556405353728489,\n","                   0.3521507025832975,\n","                   0.34404761904761905,\n","                   0.34098577571948396,\n","                   0.33964226289517474,\n","                   0.33831230148510605,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.328375,\n","                   0.3203002815139193,\n","                   0.3174224343675418,\n","                   0.3038952316991269,\n","                   0.3037614639210936,\n","                   0.2985347985347986,\n","                   0.2980504681946462,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.25689962325866655,\n","                   0.11537996858765254,\n","                   0.5438673068529027,\n","                   0.5312569521690768,\n","                   0.5147113594040968,\n","                   0.437984496124031,\n","                   0.42611894543225015,\n","                   0.28380969774449827,\n","                   0.31903851406719475,\n","                   0.3126955881450574,\n","                   0.34404761904761905,\n","                   0.3062520674826331,\n","                   0.276518302828619,\n","                   0.31899733882736714,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.322125,\n","                   0.3203002815139193,\n","                   0.2935560859188544,\n","                   0.2718267293485561,\n","                   0.2742148295552864,\n","                   0.2985347985347986,\n","                   0.2712694982701005,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.24992698811366487,\n","                   0.11537996858765254,\n","                   0.12701876909646448,\n","                   0.5312569521690768,\n","                   0.5147113594040968,\n","                   0.437984496124031,\n","                   0.42611894543225015,\n","                   0.2703862660944206,\n","                   0.31266502777019034,\n","                   0.3021108179419525,\n","                   0.34404761904761905,\n","                   0.30069467416473705,\n","                   0.2268094841930116,\n","                   0.3154777234097348,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.3175,\n","                   0.3203002815139193,\n","                   0.2816229116945107,\n","                   0.25520483546004025,\n","                   0.26653616542654435,\n","                   0.2985347985347986,\n","                   0.2648812686716969,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.23750036505943173,\n","                   0.11537996858765254,\n","                   0.12701876909646448,\n","                   0.1526140155728587,\n","                   0.5147113594040968,\n","                   0.437984496124031,\n","                   0.42611894543225015,\n","                   0.26719021093964024,\n","                   0.29882545752526635,\n","                   0.28483770019021903,\n","                   0.34404761904761905,\n","                   0.2903076414158121,\n","                   0.21006655574043265,\n","                   0.2592497210060949,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.31037499999999996,\n","                   0.3203002815139193,\n","                   0.26730310262529833,\n","                   0.2432840832773674,\n","                   0.2522278940993251,\n","                   0.2985347985347986,\n","                   0.25123100357788086,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.22879734820828834,\n","                   0.11537996858765254,\n","                   0.12701876909646448,\n","                   0.1526140155728587,\n","                   0.07076350093109873,\n","                   0.437984496124031,\n","                   0.42611894543225015,\n","                   0.20345173956716278,\n","                   0.28025129745971045,\n","                   0.2733325151868442,\n","                   0.34404761904761905,\n","                   0.26635792259345026,\n","                   0.20902662229617308,\n","                   0.2583054339428277,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.28937500000000005,\n","                   0.3203002815139193,\n","                   0.26730310262529833,\n","                   0.23052384150436533,\n","                   0.24114249870219762,\n","                   0.2985347985347986,\n","                   0.24120585213787238,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.2274612306883561,\n","                   0.11537996858765254,\n","                   0.12701876909646448,\n","                   0.1526140155728587,\n","                   0.07076350093109873,\n","                   0.08333333333333337,\n","                   0.42611894543225015,\n","                   0.2021733175052507,\n","                   0.2775198033324229,\n","                   0.27109283917285387,\n","                   0.34404761904761905,\n","                   0.2659609659278862,\n","                   0.20746672212978368,\n","                   0.25718945832260276,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.2885,\n","                   0.3203002815139193,\n","                   0.26491646778042954,\n","                   0.2293485560779046,\n","                   0.2397473611351445,\n","                   0.2985347985347986,\n","                   0.23991875966795373,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.2211749072749044,\n","                   0.11537996858765254,\n","                   0.12701876909646448,\n","                   0.1526140155728587,\n","                   0.07076350093109873,\n","                   0.08333333333333337,\n","                   0.1621704475781729,\n","                   0.19258515204090954,\n","                   0.2651370299553856,\n","                   0.2681475118119899,\n","                   0.34404761904761905,\n","                   0.25881574594773404,\n","                   0.2063227953410982,\n","                   0.2539273757404069,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.277,\n","                   0.3203002815139193,\n","                   0.2601431980906921,\n","                   0.22246474143720618,\n","                   0.23303123377747015,\n","                   0.2985347985347986,\n","                   0.2327511896749206,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.21621009900411792,\n","                   0.11634650235592603,\n","                   0.14185945002182454,\n","                   0.1526140155728587,\n","                   0.07076350093109873,\n","                   0.1027131782945736,\n","                   0.1621704475781729,\n","                   0.13049036617660492,\n","                   0.25694254757352275,\n","                   0.26366815978400937,\n","                   0.3291666666666667,\n","                   0.25881574594773404,\n","                   0.2063227953410982,\n","                   0.2539273757404069,\n","                   0.3293279022403258,\n","                   0.3351367158941365,\n","                   0.3175582990397805,\n","                   0.271875,\n","                   0.3137316233969346,\n","                   0.2577565632458234,\n","                   0.21642041638683684,\n","                   0.22750475860875585,\n","                   0.2985347985347986,\n","                   0.22798068180476339,\n","                   0.19263246780662868,\n","                   0.11308826972605945,\n","                   0.10945865556216539,\n","                   0.06400839454354668,\n","                   0.05339805825242716],\n","                  [0.20871177827750353,\n","                   0.11707140268213123,\n","                   0.14185945002182454,\n","                   0.1526140155728587,\n","                   0.07076350093109873,\n","                   0.1027131782945736,\n","                   0.171367259350092,\n","                   0.13021641859190947,\n","                   0.16343439861604303,\n","                   0.26366815978400937,\n","                   0.31607142857142856,\n","                   0.24809791597750575,\n","                   0.20081114808652245,\n","                   0.24834749763928232,\n","                   0.3177189409368636,\n","                   0.323439099283521,\n","                   0.30401234567901236,\n","                   0.26,\n","                   0.29371285580231465,\n","                   0.24105011933174225,\n","                   0.21087978509066485,\n","                   0.21916637826613605,\n","                   0.2802197802197802,\n","                   0.2200691959804929,\n","                   0.18672155372598687,\n","                   0.11308826972605945,\n","                   0.10866547689867145,\n","                   0.0645330535152151,\n","                   0.05256588072122048],\n","                  [0.1898382056598814,\n","                   0.13350247674278115,\n","                   0.1671759057180271,\n","                   0.1526140155728587,\n","                   0.10689013035381756,\n","                   0.1027131782945736,\n","                   0.1808706315144083,\n","                   0.14080905853346726,\n","                   0.16343439861604303,\n","                   0.18435908449407867,\n","                   0.24464285714285716,\n","                   0.22123718160767447,\n","                   0.19165973377703827,\n","                   0.2270581165765302,\n","                   0.274949083503055,\n","                   0.24119023249013016,\n","                   0.29441015089163236,\n","                   0.229375,\n","                   0.23709727869878006,\n","                   0.21718377088305485,\n","                   0.20449966420416388,\n","                   0.19680091711368752,\n","                   0.20146520146520142,\n","                   0.19772810466777668,\n","                   0.1796495672366477,\n","                   0.11519550456567551,\n","                   0.10688082490581008,\n","                   0.06400839454354668,\n","                   0.05423023578363384],\n","                  [0.1878376799742998,\n","                   0.13350247674278115,\n","                   0.1671759057180271,\n","                   0.1526140155728587,\n","                   0.10689013035381756,\n","                   0.1027131782945736,\n","                   0.1808706315144083,\n","                   0.14080905853346726,\n","                   0.1628880997905855,\n","                   0.1834079892004663,\n","                   0.08154761904761909,\n","                   0.21812768772742308,\n","                   0.18479617304492513,\n","                   0.22482616533608035,\n","                   0.274949083503055,\n","                   0.24119023249013016,\n","                   0.29441015089163236,\n","                   0.228375,\n","                   0.23709727869878006,\n","                   0.21002386634844872,\n","                   0.20349227669576897,\n","                   0.19474606333275657,\n","                   0.20146520146520142,\n","                   0.1957561373056077,\n","                   0.1796495672366477,\n","                   0.11519550456567551,\n","                   0.10688082490581008,\n","                   0.06400839454354668,\n","                   0.05423023578363384],\n","                  [0.18216465640606294,\n","                   0.13350247674278115,\n","                   0.1671759057180271,\n","                   0.1526140155728587,\n","                   0.10689013035381756,\n","                   0.1027131782945736,\n","                   0.190680564071122,\n","                   0.14080905853346726,\n","                   0.1632522990075571,\n","                   0.18334662821378167,\n","                   0.09166666666666667,\n","                   0.16672179953688393,\n","                   0.18479617304492513,\n","                   0.22482616533608035,\n","                   0.2625254582484725,\n","                   0.23365989179704638,\n","                   0.28532235939643347,\n","                   0.21875,\n","                   0.2349077259931186,\n","                   0.2004773269689738,\n","                   0.19459368703828073,\n","                   0.18815971621387784,\n","                   0.11721611721611724,\n","                   0.18968672877773451,\n","                   0.17584969389909222,\n","                   0.11496136736127371,\n","                   0.10430299424945466,\n","                   0.0645330535152151,\n","                   0.05395284327323158],\n","                  [0.17958003562980052,\n","                   0.13350247674278115,\n","                   0.1907463989524225,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.1027131782945736,\n","                   0.190680564071122,\n","                   0.14080905853346726,\n","                   0.1632522990075571,\n","                   0.18334662821378167,\n","                   0.10297619047619044,\n","                   0.16672179953688393,\n","                   0.14798252911813647,\n","                   0.22482616533608035,\n","                   0.24521384928716905,\n","                   0.2318321392016377,\n","                   0.28292181069958844,\n","                   0.21437499999999998,\n","                   0.23334375977478883,\n","                   0.19570405727923623,\n","                   0.18922095366017466,\n","                   0.18504499048278245,\n","                   0.11721611721611724,\n","                   0.187136160213492,\n","                   0.17394975723031458,\n","                   0.11355654413486305,\n","                   0.10251834225659329,\n","                   0.06348373557187825,\n","                   0.05395284327323158],\n","                  [0.17544026167460058,\n","                   0.13495227739519144,\n","                   0.1907463989524225,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.10658914728682167,\n","                   0.19681177191906807,\n","                   0.14080905853346726,\n","                   0.1644359464627151,\n","                   0.18236485242682698,\n","                   0.10833333333333328,\n","                   0.16672179953688393,\n","                   0.14798252911813647,\n","                   0.17615245943857838,\n","                   0.23340122199592672,\n","                   0.22408246819710487,\n","                   0.2772633744855967,\n","                   0.20799999999999996,\n","                   0.22583672192680637,\n","                   0.16229116945107402,\n","                   0.1905641370047011,\n","                   0.18104343311991689,\n","                   0.11721611721611724,\n","                   0.18169258563888202,\n","                   0.1712054042643023,\n","                   0.11004448606883632,\n","                   0.10113027959547893,\n","                   0.06400839454354668,\n","                   0.05298196948682388],\n","                  [0.17303086942554247,\n","                   0.13495227739519144,\n","                   0.1907463989524225,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.10658914728682167,\n","                   0.19681177191906807,\n","                   0.14080905853346726,\n","                   0.1644359464627151,\n","                   0.18236485242682698,\n","                   0.10833333333333328,\n","                   0.16672179953688393,\n","                   0.14735856905158073,\n","                   0.17615245943857838,\n","                   0.1661914460285132,\n","                   0.22408246819710487,\n","                   0.2772633744855967,\n","                   0.20737499999999998,\n","                   0.22583672192680637,\n","                   0.15990453460620524,\n","                   0.18821356615177975,\n","                   0.17818826786641284,\n","                   0.11721611721611724,\n","                   0.17922467438922152,\n","                   0.1712054042643023,\n","                   0.11004448606883632,\n","                   0.10113027959547893,\n","                   0.06400839454354668,\n","                   0.05298196948682388],\n","                  [0.1651820916445197,\n","                   0.13495227739519144,\n","                   0.1907463989524225,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.10658914728682167,\n","                   0.19681177191906807,\n","                   0.14080905853346726,\n","                   0.1644359464627151,\n","                   0.18236485242682698,\n","                   0.10833333333333328,\n","                   0.1639431028779358,\n","                   0.14642262895174707,\n","                   0.17615245943857838,\n","                   0.1661914460285132,\n","                   0.1454891065945314,\n","                   0.2772633744855967,\n","                   0.186875,\n","                   0.22583672192680637,\n","                   0.14558472553699287,\n","                   0.1821692411014103,\n","                   0.1693415815885101,\n","                   0.11721611721611724,\n","                   0.169707275024502,\n","                   0.1712054042643023,\n","                   0.11004448606883632,\n","                   0.10113027959547893,\n","                   0.06400839454354668,\n","                   0.05298196948682388],\n","                  [0.16197686983440907,\n","                   0.13495227739519144,\n","                   0.1907463989524225,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.10658914728682167,\n","                   0.19681177191906807,\n","                   0.14080905853346726,\n","                   0.16334334881180002,\n","                   0.18236485242682698,\n","                   0.10833333333333328,\n","                   0.16672179953688393,\n","                   0.14642262895174707,\n","                   0.17615245943857838,\n","                   0.1661914460285132,\n","                   0.148559730954818,\n","                   0.19478737997256512,\n","                   0.17862500000000003,\n","                   0.22583672192680637,\n","                   0.14797136038186154,\n","                   0.18132975151108122,\n","                   0.16570773490223223,\n","                   0.11721611721611724,\n","                   0.16552717654421578,\n","                   0.1712054042643023,\n","                   0.11004448606883632,\n","                   0.10113027959547893,\n","                   0.06400839454354668,\n","                   0.05298196948682388],\n","                  [0.15999094652609447,\n","                   0.13567717772139665,\n","                   0.19161938018332603,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.11046511627906974,\n","                   0.20202329858982215,\n","                   0.14099169025659752,\n","                   0.16334334881180002,\n","                   0.18046266183960236,\n","                   0.10833333333333328,\n","                   0.1630830301025471,\n","                   0.14756655574043265,\n","                   0.17615245943857838,\n","                   0.16517311608961305,\n","                   0.14870595116245067,\n","                   0.19650205761316875,\n","                   0.144625,\n","                   0.22489834219580862,\n","                   0.14319809069212408,\n","                   0.17897918065815988,\n","                   0.1640314068177885,\n","                   0.10439560439560436,\n","                   0.16360244193323648,\n","                   0.15948912814017313,\n","                   0.10910793725122925,\n","                   0.10013880626611149,\n","                   0.07082896117523607,\n","                   0.05228848821081833],\n","                  [0.15886656347653394,\n","                   0.13567717772139665,\n","                   0.19161938018332603,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.11046511627906974,\n","                   0.20202329858982215,\n","                   0.14099169025659752,\n","                   0.16334334881180002,\n","                   0.18046266183960236,\n","                   0.10833333333333328,\n","                   0.1630830301025471,\n","                   0.14756655574043265,\n","                   0.17615245943857838,\n","                   0.16517311608961305,\n","                   0.14870595116245067,\n","                   0.19650205761316875,\n","                   0.14437500000000003,\n","                   0.17672818267125434,\n","                   0.13365155131264916,\n","                   0.1774680993955675,\n","                   0.162917459768126,\n","                   0.10439560439560436,\n","                   0.16277586878741723,\n","                   0.15948912814017313,\n","                   0.10910793725122925,\n","                   0.10013880626611149,\n","                   0.07082896117523607,\n","                   0.05228848821081833],\n","                  [0.15861102187436116,\n","                   0.13567717772139665,\n","                   0.19249236141422954,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.11046511627906974,\n","                   0.20110361741263028,\n","                   0.14080905853346726,\n","                   0.1629791495948284,\n","                   0.18030925937289066,\n","                   0.10833333333333328,\n","                   0.1630830301025471,\n","                   0.14756655574043265,\n","                   0.17589492660314188,\n","                   0.16476578411405296,\n","                   0.14870595116245067,\n","                   0.19615912208504804,\n","                   0.14449999999999996,\n","                   0.17672818267125434,\n","                   0.05011933174224348,\n","                   0.1774680993955675,\n","                   0.162917459768126,\n","                   0.10439560439560436,\n","                   0.1626341705338482,\n","                   0.1593835761030188,\n","                   0.10910793725122925,\n","                   0.09994051160023798,\n","                   0.07082896117523607,\n","                   0.052011095700416066],\n","                  [0.15777138518150757,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.1523915461624027,\n","                   0.11210428305400377,\n","                   0.11046511627906974,\n","                   0.20110361741263028,\n","                   0.1411743219797279,\n","                   0.1629791495948284,\n","                   0.18030925937289066,\n","                   0.10833333333333328,\n","                   0.1630830301025471,\n","                   0.1468386023294509,\n","                   0.17400635247660745,\n","                   0.16435845213849287,\n","                   0.14914461178534877,\n","                   0.19598765432098764,\n","                   0.14400000000000002,\n","                   0.178292148889584,\n","                   0.05011933174224348,\n","                   0.15815983881799867,\n","                   0.16177106765876448,\n","                   0.10439560439560436,\n","                   0.16264597872164555,\n","                   0.1573780873970868,\n","                   0.10863966284242565,\n","                   0.10132857426135233,\n","                   0.0713536201469045,\n","                   0.052011095700416066],\n","                  [0.15777138518150757,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.11210428305400377,\n","                   0.1647286821705426,\n","                   0.22532188841201717,\n","                   0.1411743219797279,\n","                   0.1629791495948284,\n","                   0.18767257777505064,\n","                   0.10833333333333328,\n","                   0.17466093284816409,\n","                   0.15848585690515804,\n","                   0.19787106189372483,\n","                   0.18879837067209781,\n","                   0.16530194472876147,\n","                   0.19633058984910834,\n","                   0.14400000000000002,\n","                   0.19705974350954014,\n","                   0.054892601431980936,\n","                   0.15815983881799867,\n","                   0.1617494376189652,\n","                   0.13003663003663002,\n","                   0.16277586878741723,\n","                   0.15104496516782773,\n","                   0.10536174198080073,\n","                   0.10093198492960542,\n","                   0.06873032528856249,\n","                   0.050901525658807234],\n","                  [0.15730410910896298,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.11210428305400377,\n","                   0.1647286821705426,\n","                   0.22532188841201717,\n","                   0.1411743219797279,\n","                   0.16188655194391333,\n","                   0.187243050868258,\n","                   0.10833333333333328,\n","                   0.17042672841548134,\n","                   0.15848585690515804,\n","                   0.19787106189372483,\n","                   0.18879837067209781,\n","                   0.16530194472876147,\n","                   0.19633058984910834,\n","                   0.14312499999999995,\n","                   0.19705974350954014,\n","                   0.054892601431980936,\n","                   0.1576561450638012,\n","                   0.16111135144488664,\n","                   0.012820512820512775,\n","                   0.16223269214873592,\n","                   0.15104496516782773,\n","                   0.10536174198080073,\n","                   0.10093198492960542,\n","                   0.06873032528856249,\n","                   0.050901525658807234],\n","                  [0.15891037060833502,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.13929236499068898,\n","                   0.1647286821705426,\n","                   0.2375843041079092,\n","                   0.1411743219797279,\n","                   0.1629791495948284,\n","                   0.187243050868258,\n","                   0.125,\n","                   0.17565332451207405,\n","                   0.1639975041597338,\n","                   0.2026783414885398,\n","                   0.19307535641547857,\n","                   0.16530194472876147,\n","                   0.19478737997256512,\n","                   0.15649999999999997,\n","                   0.19705974350954014,\n","                   0.06921241050119331,\n","                   0.15815983881799867,\n","                   0.16324191036511504,\n","                   0.08424908424908428,\n","                   0.16479506890077578,\n","                   0.15188938146506226,\n","                   0.11285413252165766,\n","                   0.10688082490581008,\n","                   0.08079748163693601,\n","                   0.054785020804438256],\n","                  [0.15709237463859116,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.1389199255121043,\n","                   0.1647286821705426,\n","                   0.2375843041079092,\n","                   0.14126563784129298,\n","                   0.1630701993990713,\n","                   0.18583174817451065,\n","                   0.1255952380952381,\n","                   0.17565332451207405,\n","                   0.16337354409317806,\n","                   0.20113314447592068,\n","                   0.19307535641547857,\n","                   0.16530194472876147,\n","                   0.19461591220850483,\n","                   0.15637500000000004,\n","                   0.19705974350954014,\n","                   0.05011933174224348,\n","                   0.15799194089993285,\n","                   0.16169536251946703,\n","                   0.08424908424908428,\n","                   0.16283490972640424,\n","                   0.12634578847371758,\n","                   0.11261999531725586,\n","                   0.10688082490581008,\n","                   0.08079748163693601,\n","                   0.054785020804438256],\n","                  [0.15590958207996264,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.1389199255121043,\n","                   0.1647286821705426,\n","                   0.2375843041079092,\n","                   0.14126563784129298,\n","                   0.1630701993990713,\n","                   0.18454316745413268,\n","                   0.1255952380952381,\n","                   0.17565332451207405,\n","                   0.16160565723793674,\n","                   0.1958107992102326,\n","                   0.19307535641547857,\n","                   0.16530194472876147,\n","                   0.19461591220850483,\n","                   0.153625,\n","                   0.19705974350954014,\n","                   0.05011933174224348,\n","                   0.15681665547347212,\n","                   0.16059223048970406,\n","                   0.08424908424908428,\n","                   0.16151239269309337,\n","                   0.12634578847371758,\n","                   0.07468976820416762,\n","                   0.10688082490581008,\n","                   0.08079748163693601,\n","                   0.054785020804438256],\n","                  [0.1558803773254286,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.1389199255121043,\n","                   0.1647286821705426,\n","                   0.2375843041079092,\n","                   0.14090037439503245,\n","                   0.1628880997905855,\n","                   0.18454316745413268,\n","                   0.1255952380952381,\n","                   0.1738008600727754,\n","                   0.16056572379367717,\n","                   0.19907288179242855,\n","                   0.19307535641547857,\n","                   0.16530194472876147,\n","                   0.19461591220850483,\n","                   0.15425,\n","                   0.19705974350954014,\n","                   0.05011933174224348,\n","                   0.15681665547347212,\n","                   0.16050571033050698,\n","                   0.08424908424908428,\n","                   0.16150058450529592,\n","                   0.12634578847371758,\n","                   0.10044486068836334,\n","                   0.08427523299623241,\n","                   0.08079748163693601,\n","                   0.054785020804438256],\n","                  [0.15499693350077393,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.1389199255121043,\n","                   0.1647286821705426,\n","                   0.2375843041079092,\n","                   0.14090037439503245,\n","                   0.16234180096512796,\n","                   0.18417500153402466,\n","                   0.1255952380952381,\n","                   0.17313926563016868,\n","                   0.16035773710482526,\n","                   0.19812859472916133,\n","                   0.19307535641547857,\n","                   0.16530194472876147,\n","                   0.19461591220850483,\n","                   0.14887499999999998,\n","                   0.19705974350954014,\n","                   0.05011933174224348,\n","                   0.1566487575554063,\n","                   0.1597053988579339,\n","                   0.08424908424908428,\n","                   0.16036699847674374,\n","                   0.12634578847371758,\n","                   0.10044486068836334,\n","                   0.08427523299623241,\n","                   0.017313746065057756,\n","                   0.054785020804438256],\n","                  [0.1545734645600304,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.1389199255121043,\n","                   0.1647286821705426,\n","                   0.2375843041079092,\n","                   0.14080905853346726,\n","                   0.1615223527269416,\n","                   0.18454316745413268,\n","                   0.1255952380952381,\n","                   0.17174991730069467,\n","                   0.16035773710482526,\n","                   0.19864366040003434,\n","                   0.19307535641547857,\n","                   0.16530194472876147,\n","                   0.19461591220850483,\n","                   0.14849999999999997,\n","                   0.19705974350954014,\n","                   0.05250596658711215,\n","                   0.15614506380120885,\n","                   0.15926198304204875,\n","                   0.08424908424908428,\n","                   0.1600009446550238,\n","                   0.12634578847371758,\n","                   0.10044486068836334,\n","                   0.08427523299623241,\n","                   0.0598111227701994,\n","                   0.035506241331484056]],\n"," 'update_node_indices_tracking_rejects': [0],\n"," 'update_nodes': [<__main__.PointerDecisionListNode object at 0x7f4c97fe7710>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97e42590>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c98182b10>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c981f9650>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c989cc5d0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c9819bed0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c9929ed10>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c981b5650>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c9ee27310>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c9fed0790>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c982098d0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97ffc450>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c9821d8d0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c98209d50>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97e8e390>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97e8e710>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97e43610>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c9821da90>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c9819bfd0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c98209f50>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c981b5e50>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c981b5590>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97e42210>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97d98fd0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97dae650>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97d4d6d0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97e43490>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97dea910>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97d98f90>]}\n"]}]},{"cell_type":"code","source":["pprint(vars(content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QEue99j_k9H6","executionInfo":{"status":"ok","timestamp":1648673337525,"user_tz":240,"elapsed":422,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}},"outputId":"164e470a-261e-41ea-9600-5c9cd34669d4"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["{'all_groups': None,\n"," 'curr_node': <__main__.PointerDecisionListNode object at 0x7f4c97f6f510>,\n"," 'head': <__main__.PointerDecisionListNode object at 0x7f4c97f6f510>,\n"," 'leaves': [<bound method predict of DecisionTreeClassifier(max_depth=1, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>,\n","            <bound method predict of DecisionTreeClassifier(max_depth=10, random_state=0)>],\n"," 'num_rounds': 28,\n"," 'pred_names': ['Total',\n","                'EDU',\n","                'CMS',\n","                'ENT',\n","                'PRT',\n","                'LGL',\n","                'RPR',\n","                'local govt worker',\n","                'associates degree',\n","                'bachelors degree',\n","                'SCI',\n","                'cow minorities',\n","                'non profit worker',\n","                'self employed worker',\n","                'BUS',\n","                'SAL',\n","                'CON',\n","                'over time',\n","                'FIN',\n","                'oceanian',\n","                'black',\n","                'US born',\n","                'MIL',\n","                'white',\n","                'TRN',\n","                'PRS',\n","                'CLN',\n","                'FFF',\n","                'EAT'],\n"," 'predicates': [<function <lambda> at 0x7f4c97786e60>,\n","                <function EDU at 0x7f4c97786710>,\n","                <function CMS at 0x7f4c977865f0>,\n","                <function ENT at 0x7f4c977864d0>,\n","                <function PRT at 0x7f4c97786b90>,\n","                <function LGL at 0x7f4c977863b0>,\n","                <function RPR at 0x7f4c97786dd0>,\n","                <function local_govt_worker at 0x7f4c97dddef0>,\n","                <function assoc_degree at 0x7f4c97ddd8c0>,\n","                <function bachelor_degree at 0x7f4c97dbee60>,\n","                <function SCI at 0x7f4c97786a70>,\n","                <function cow_minorities at 0x7f4c97dddcb0>,\n","                <function non_profit_worker at 0x7f4c97dbe560>,\n","                <function self_employed_worker at 0x7f4c97ddd680>,\n","                <function BUS at 0x7f4c97dddf80>,\n","                <function SAL at 0x7f4c97786290>,\n","                <function CON at 0x7f4c97786170>,\n","                <function over_time at 0x7f4c97ddd050>,\n","                <function FIN at 0x7f4c97786050>,\n","                <function oceanian at 0x7f4c97ddd7a0>,\n","                <function black at 0x7f4c97ddd320>,\n","                <function us_born at 0x7f4c97786cb0>,\n","                <function MIL at 0x7f4c97786950>,\n","                <function white at 0x7f4c97786830>,\n","                <function TRN at 0x7f4c97dddb90>,\n","                <function PRS at 0x7f4c97ddd170>,\n","                <function CLN at 0x7f4c97ddd440>,\n","                <function FFF at 0x7f4c97e57dd0>,\n","                <function EAT at 0x7f4c97e577a0>],\n"," 'test_errors': [[0.2796252129471891,\n","                  0.5519516217702034,\n","                  0.5603271983640081,\n","                  0.5544768069039914,\n","                  0.48484848484848486,\n","                  0.40384615384615385,\n","                  0.431314623338257,\n","                  0.385979381443299,\n","                  0.36352305774823435,\n","                  0.3462252033680605,\n","                  0.32369942196531787,\n","                  0.342955757765924,\n","                  0.33642578125,\n","                  0.35389360061680797,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.3378684807256236,\n","                  0.3558558558558559,\n","                  0.3555555555555555,\n","                  0.30828220858895705,\n","                  0.2994144962648899,\n","                  0.3173076923076923,\n","                  0.29457279262470504,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.2571039182282794,\n","                  0.18856514568444205,\n","                  0.5603271983640081,\n","                  0.5544768069039914,\n","                  0.48484848484848486,\n","                  0.40384615384615385,\n","                  0.431314623338257,\n","                  0.2964948453608247,\n","                  0.32405484004985463,\n","                  0.31268731268731265,\n","                  0.32369942196531787,\n","                  0.31848133040476934,\n","                  0.28173828125,\n","                  0.33770239013107173,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.3378684807256236,\n","                  0.3558558558558559,\n","                  0.34444444444444444,\n","                  0.2845092024539877,\n","                  0.27493438320209973,\n","                  0.3173076923076923,\n","                  0.2726225100148164,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.2531856899488927,\n","                  0.18856514568444205,\n","                  0.32515337423312884,\n","                  0.5544768069039914,\n","                  0.48484848484848486,\n","                  0.40384615384615385,\n","                  0.431314623338257,\n","                  0.29072164948453605,\n","                  0.3232239302035729,\n","                  0.30569430569430567,\n","                  0.32369942196531787,\n","                  0.3153435833071855,\n","                  0.2529296875,\n","                  0.33616037008481114,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.3361678004535147,\n","                  0.3558558558558559,\n","                  0.34444444444444444,\n","                  0.27377300613496935,\n","                  0.27079547748839083,\n","                  0.3173076923076923,\n","                  0.26927509191680843,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.24398637137989776,\n","                  0.18856514568444205,\n","                  0.32515337423312884,\n","                  0.2632146709816613,\n","                  0.48484848484848486,\n","                  0.40384615384615385,\n","                  0.431314623338257,\n","                  0.28907216494845356,\n","                  0.3144993768176153,\n","                  0.29499072356215217,\n","                  0.32369942196531787,\n","                  0.3103231879510512,\n","                  0.2412109375,\n","                  0.28257517347725525,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.33390022675736963,\n","                  0.3558558558558559,\n","                  0.3222222222222222,\n","                  0.26457055214723924,\n","                  0.26065011104381186,\n","                  0.3173076923076923,\n","                  0.2581901991988147,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.2379897785349233,\n","                  0.18856514568444205,\n","                  0.32515337423312884,\n","                  0.2632146709816613,\n","                  0.18855218855218858,\n","                  0.40384615384615385,\n","                  0.431314623338257,\n","                  0.23587628865979382,\n","                  0.30078936435396764,\n","                  0.28728414442700156,\n","                  0.32369942196531787,\n","                  0.29118293065578915,\n","                  0.2412109375,\n","                  0.28257517347725525,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.3163265306122449,\n","                  0.3558558558558559,\n","                  0.33333333333333337,\n","                  0.25996932515337423,\n","                  0.25272562083585703,\n","                  0.3173076923076923,\n","                  0.25078197881797726,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.23768313458262347,\n","                  0.18856514568444205,\n","                  0.32515337423312884,\n","                  0.2632146709816613,\n","                  0.18855218855218858,\n","                  0.3173076923076923,\n","                  0.431314623338257,\n","                  0.23711340206185572,\n","                  0.2995429995845451,\n","                  0.2874268588554303,\n","                  0.32369942196531787,\n","                  0.29055538123627234,\n","                  0.24072265625,\n","                  0.28180416345412496,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.3168934240362812,\n","                  0.3558558558558559,\n","                  0.33333333333333337,\n","                  0.25920245398773,\n","                  0.252523722996164,\n","                  0.3173076923076923,\n","                  0.25061735169840316,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.23478705281090284,\n","                  0.18856514568444205,\n","                  0.32515337423312884,\n","                  0.2632146709816613,\n","                  0.18855218855218858,\n","                  0.3173076923076923,\n","                  0.3057607090103397,\n","                  0.23546391752577323,\n","                  0.28915662650602414,\n","                  0.28599971457114315,\n","                  0.32369942196531787,\n","                  0.2858487605898965,\n","                  0.240234375,\n","                  0.28218966846569005,\n","                  0.32339235787511644,\n","                  0.3189596167008898,\n","                  0.32252964426877473,\n","                  0.31065759637188206,\n","                  0.3558558558558559,\n","                  0.33333333333333337,\n","                  0.25230061349693256,\n","                  0.2487886129618413,\n","                  0.3173076923076923,\n","                  0.24694067936124675,\n","                  0.2094972067039106,\n","                  0.11087645195353746,\n","                  0.11742777260018644,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.23356047700170357,\n","                  0.18856514568444205,\n","                  0.3210633946830266,\n","                  0.2632146709816613,\n","                  0.18855218855218858,\n","                  0.28846153846153844,\n","                  0.3057607090103397,\n","                  0.22061855670103092,\n","                  0.28749480681346073,\n","                  0.2862851434280006,\n","                  0.32369942196531787,\n","                  0.2858487605898965,\n","                  0.240234375,\n","                  0.28218966846569005,\n","                  0.3280521901211556,\n","                  0.3199863107460643,\n","                  0.31067193675889326,\n","                  0.31065759637188206,\n","                  0.35285285285285284,\n","                  0.34444444444444444,\n","                  0.2538343558282209,\n","                  0.2467696345649102,\n","                  0.3173076923076923,\n","                  0.24551391099160402,\n","                  0.20623836126629425,\n","                  0.11193241816261879,\n","                  0.12395153774464118,\n","                  0.05167958656330751,\n","                  0.05452127659574468],\n","                 [0.23039182282793869,\n","                  0.1869158878504673,\n","                  0.3210633946830266,\n","                  0.2632146709816613,\n","                  0.18855218855218858,\n","                  0.28846153846153844,\n","                  0.30132939438700146,\n","                  0.21896907216494843,\n","                  0.24885749896136267,\n","                  0.2862851434280006,\n","                  0.3063583815028902,\n","                  0.27957326639472857,\n","                  0.236328125,\n","                  0.28411719352351583,\n","                  0.3140726933830382,\n","                  0.31040383299110197,\n","                  0.31225296442687744,\n","                  0.3032879818594104,\n","                  0.3198198198198198,\n","                  0.34444444444444444,\n","                  0.2507668711656442,\n","                  0.24232788209166167,\n","                  0.3173076923076923,\n","                  0.24095922735005215,\n","                  0.2085661080074488,\n","                  0.11298838437170011,\n","                  0.1230195712954334,\n","                  0.06201550387596899,\n","                  0.059175531914893664],\n","                 [0.21655877342419083,\n","                  0.18031885651456847,\n","                  0.31492842535787324,\n","                  0.2632146709816613,\n","                  0.17845117845117842,\n","                  0.28846153846153844,\n","                  0.30132939438700146,\n","                  0.20412371134020624,\n","                  0.24885749896136267,\n","                  0.22834308548594262,\n","                  0.2774566473988439,\n","                  0.2629432067775337,\n","                  0.22607421875,\n","                  0.28257517347725525,\n","                  0.28704566635601114,\n","                  0.24435318275154005,\n","                  0.3003952569169961,\n","                  0.27494331065759636,\n","                  0.29579579579579585,\n","                  0.33333333333333337,\n","                  0.24003067484662577,\n","                  0.2241570765192813,\n","                  0.25,\n","                  0.22257586566427046,\n","                  0.2039106145251397,\n","                  0.11510031678986277,\n","                  0.12488350419384897,\n","                  0.056847545219638196,\n","                  0.05984042553191493],\n","                 [0.2162180579216354,\n","                  0.18031885651456847,\n","                  0.31492842535787324,\n","                  0.2632146709816613,\n","                  0.17845117845117842,\n","                  0.28846153846153844,\n","                  0.30132939438700146,\n","                  0.20412371134020624,\n","                  0.2484420440382219,\n","                  0.22834308548594262,\n","                  0.24855491329479773,\n","                  0.26388453090680886,\n","                  0.2236328125,\n","                  0.28296067848882034,\n","                  0.28704566635601114,\n","                  0.24435318275154005,\n","                  0.3003952569169961,\n","                  0.27380952380952384,\n","                  0.29579579579579585,\n","                  0.33333333333333337,\n","                  0.23849693251533743,\n","                  0.22360185746012518,\n","                  0.25,\n","                  0.2223563628381715,\n","                  0.2039106145251397,\n","                  0.11510031678986277,\n","                  0.12488350419384897,\n","                  0.056847545219638196,\n","                  0.05984042553191493],\n","                 [0.21284497444633732,\n","                  0.18031885651456847,\n","                  0.31492842535787324,\n","                  0.2632146709816613,\n","                  0.17845117845117842,\n","                  0.28846153846153844,\n","                  0.2983751846381093,\n","                  0.20412371134020624,\n","                  0.24719567926879937,\n","                  0.2276295133437991,\n","                  0.2225433526011561,\n","                  0.232820834640728,\n","                  0.2236328125,\n","                  0.28296067848882034,\n","                  0.2786579683131407,\n","                  0.231006160164271,\n","                  0.28774703557312253,\n","                  0.2647392290249433,\n","                  0.30930930930930933,\n","                  0.33333333333333337,\n","                  0.23619631901840488,\n","                  0.2200181708055724,\n","                  0.23076923076923073,\n","                  0.219118696153213,\n","                  0.19692737430167595,\n","                  0.11510031678986277,\n","                  0.1342031686859273,\n","                  0.05167958656330751,\n","                  0.06382978723404253],\n","                 [0.2114821124361158,\n","                  0.18031885651456847,\n","                  0.30265848670756645,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.28846153846153844,\n","                  0.2983751846381093,\n","                  0.20412371134020624,\n","                  0.24719567926879937,\n","                  0.2276295133437991,\n","                  0.2167630057803468,\n","                  0.232820834640728,\n","                  0.2041015625,\n","                  0.28296067848882034,\n","                  0.2712022367194781,\n","                  0.2306639288158795,\n","                  0.28774703557312253,\n","                  0.26077097505668934,\n","                  0.3078078078078078,\n","                  0.3222222222222222,\n","                  0.23389570552147243,\n","                  0.21835251362810415,\n","                  0.23076923076923073,\n","                  0.2175821763705208,\n","                  0.19878957169459965,\n","                  0.11615628299894398,\n","                  0.1351351351351351,\n","                  0.054263565891472854,\n","                  0.06382978723404253],\n","                 [0.20940374787052807,\n","                  0.17757009345794394,\n","                  0.30265848670756645,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.28846153846153844,\n","                  0.29542097488921715,\n","                  0.20412371134020624,\n","                  0.24054840049854587,\n","                  0.22605965463108324,\n","                  0.2109826589595376,\n","                  0.232820834640728,\n","                  0.2041015625,\n","                  0.25944487278334616,\n","                  0.26374650512581543,\n","                  0.2265571526351814,\n","                  0.28537549407114626,\n","                  0.26077097505668934,\n","                  0.3003003003003003,\n","                  0.3222222222222222,\n","                  0.23006134969325154,\n","                  0.2154754694124773,\n","                  0.23076923076923073,\n","                  0.21571640234868028,\n","                  0.2011173184357542,\n","                  0.12249208025343195,\n","                  0.13699906803355077,\n","                  0.056847545219638196,\n","                  0.0644946808510638],\n","                 [0.20824531516183986,\n","                  0.17757009345794394,\n","                  0.30265848670756645,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.28846153846153844,\n","                  0.29542097488921715,\n","                  0.20412371134020624,\n","                  0.24054840049854587,\n","                  0.22605965463108324,\n","                  0.2109826589595376,\n","                  0.232820834640728,\n","                  0.2041015625,\n","                  0.25944487278334616,\n","                  0.2320596458527493,\n","                  0.2265571526351814,\n","                  0.28537549407114626,\n","                  0.26190476190476186,\n","                  0.3003003003003003,\n","                  0.3111111111111111,\n","                  0.22929447852760731,\n","                  0.21436503129416518,\n","                  0.23076923076923073,\n","                  0.21478351533775997,\n","                  0.2011173184357542,\n","                  0.12249208025343195,\n","                  0.13699906803355077,\n","                  0.056847545219638196,\n","                  0.0644946808510638],\n","                 [0.20514480408858604,\n","                  0.17757009345794394,\n","                  0.30265848670756645,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.28846153846153844,\n","                  0.29542097488921715,\n","                  0.20412371134020624,\n","                  0.24054840049854587,\n","                  0.22605965463108324,\n","                  0.2109826589595376,\n","                  0.232820834640728,\n","                  0.203125,\n","                  0.25944487278334616,\n","                  0.2320596458527493,\n","                  0.19541409993155368,\n","                  0.28537549407114626,\n","                  0.24773242630385484,\n","                  0.3003003003003003,\n","                  0.30000000000000004,\n","                  0.23006134969325154,\n","                  0.21032707450030286,\n","                  0.23076923076923073,\n","                  0.2103934588157823,\n","                  0.2011173184357542,\n","                  0.12249208025343195,\n","                  0.13699906803355077,\n","                  0.056847545219638196,\n","                  0.0644946808510638],\n","                 [0.20330494037478708,\n","                  0.17757009345794394,\n","                  0.30265848670756645,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.28846153846153844,\n","                  0.29542097488921715,\n","                  0.20412371134020624,\n","                  0.2401329455754051,\n","                  0.22605965463108324,\n","                  0.2109826589595376,\n","                  0.232820834640728,\n","                  0.2021484375,\n","                  0.25944487278334616,\n","                  0.2320596458527493,\n","                  0.19541409993155368,\n","                  0.24268774703557316,\n","                  0.24319727891156462,\n","                  0.3003003003003003,\n","                  0.3111111111111111,\n","                  0.23006134969325154,\n","                  0.20765192812436906,\n","                  0.23076923076923073,\n","                  0.20721066783734843,\n","                  0.2011173184357542,\n","                  0.12249208025343195,\n","                  0.13699906803355077,\n","                  0.056847545219638196,\n","                  0.0644946808510638],\n","                 [0.20183986371379903,\n","                  0.1759208356239692,\n","                  0.2985685071574642,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.2692307692307693,\n","                  0.29394387001477107,\n","                  0.20247422680412375,\n","                  0.2401329455754051,\n","                  0.22477522477522482,\n","                  0.2109826589595376,\n","                  0.22936931283338557,\n","                  0.20068359375,\n","                  0.25944487278334616,\n","                  0.22926374650512582,\n","                  0.1926762491444216,\n","                  0.24268774703557316,\n","                  0.21882086167800452,\n","                  0.2987987987987988,\n","                  0.3222222222222222,\n","                  0.23159509202453987,\n","                  0.20613769432667073,\n","                  0.23076923076923073,\n","                  0.2053448938155079,\n","                  0.2001862197392924,\n","                  0.12143611404435062,\n","                  0.136067101584343,\n","                  0.06459948320413433,\n","                  0.06781914893617025],\n","                 [0.20115843270868827,\n","                  0.1759208356239692,\n","                  0.2985685071574642,\n","                  0.25997842502696866,\n","                  0.1767676767676768,\n","                  0.2692307692307693,\n","                  0.29394387001477107,\n","                  0.20247422680412375,\n","                  0.2401329455754051,\n","                  0.22477522477522482,\n","                  0.2109826589595376,\n","                  0.22936931283338557,\n","                  0.20068359375,\n","                  0.25944487278334616,\n","                  0.22926374650512582,\n","                  0.1926762491444216,\n","                  0.24268774703557316,\n","                  0.217687074829932,\n","                  0.26876876876876876,\n","                  0.3222222222222222,\n","                  0.23082822085889576,\n","                  0.2055824752675146,\n","                  0.23076923076923073,\n","                  0.2047412610437359,\n","                  0.2001862197392924,\n","                  0.12143611404435062,\n","                  0.136067101584343,\n","                  0.06459948320413433,\n","                  0.06781914893617025],\n","                 [0.20085178875638843,\n","                  0.1759208356239692,\n","                  0.2965235173824131,\n","                  0.25997842502696866,\n","                  0.17508417508417506,\n","                  0.2692307692307693,\n","                  0.292466765140325,\n","                  0.20164948453608245,\n","                  0.23971749065226422,\n","                  0.22448979591836737,\n","                  0.2109826589595376,\n","                  0.22936931283338557,\n","                  0.20068359375,\n","                  0.25905936777178107,\n","                  0.22926374650512582,\n","                  0.1923340177960301,\n","                  0.24268774703557316,\n","                  0.2159863945578231,\n","                  0.26876876876876876,\n","                  0.2222222222222222,\n","                  0.23082822085889576,\n","                  0.2055824752675146,\n","                  0.23076923076923073,\n","                  0.2047412610437359,\n","                  0.19972067039106145,\n","                  0.12143611404435062,\n","                  0.1351351351351351,\n","                  0.06459948320413433,\n","                  0.06781914893617025],\n","                 [0.19965928449744463,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.25997842502696866,\n","                  0.1717171717171717,\n","                  0.2692307692307693,\n","                  0.292466765140325,\n","                  0.19876288659793817,\n","                  0.23971749065226422,\n","                  0.22448979591836737,\n","                  0.2109826589595376,\n","                  0.22936931283338557,\n","                  0.19970703125,\n","                  0.2579028527370856,\n","                  0.22926374650512582,\n","                  0.1923340177960301,\n","                  0.24268774703557316,\n","                  0.21485260770975056,\n","                  0.26876876876876876,\n","                  0.2222222222222222,\n","                  0.20398773006134974,\n","                  0.20401776700989305,\n","                  0.23076923076923073,\n","                  0.2047412610437359,\n","                  0.1936685288640596,\n","                  0.12143611404435062,\n","                  0.1332712022367195,\n","                  0.06459948320413433,\n","                  0.06648936170212771],\n","                 [0.1918568994889267,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.1717171717171717,\n","                  0.23076923076923073,\n","                  0.276218611521418,\n","                  0.19876288659793817,\n","                  0.23971749065226422,\n","                  0.21778221778221774,\n","                  0.2109826589595376,\n","                  0.22246626921870094,\n","                  0.197265625,\n","                  0.25173477255204313,\n","                  0.21155638397017706,\n","                  0.19096509240246407,\n","                  0.24268774703557316,\n","                  0.21485260770975056,\n","                  0.2627627627627628,\n","                  0.2222222222222222,\n","                  0.20398773006134974,\n","                  0.1924591156874621,\n","                  0.2596153846153846,\n","                  0.1968940350107008,\n","                  0.19320297951582865,\n","                  0.12038014783526929,\n","                  0.13886300093196646,\n","                  0.06718346253229979,\n","                  0.06183510638297873],\n","                 [0.19161839863713803,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.1717171717171717,\n","                  0.23076923076923073,\n","                  0.276218611521418,\n","                  0.19876288659793817,\n","                  0.23971749065226422,\n","                  0.2174967889253604,\n","                  0.2109826589595376,\n","                  0.22026984625039225,\n","                  0.197265625,\n","                  0.25173477255204313,\n","                  0.21155638397017706,\n","                  0.19096509240246407,\n","                  0.24268774703557316,\n","                  0.2142857142857143,\n","                  0.2627627627627628,\n","                  0.2222222222222222,\n","                  0.20322085889570551,\n","                  0.19210579446799925,\n","                  0.1923076923076923,\n","                  0.19656478077155248,\n","                  0.19320297951582865,\n","                  0.12038014783526929,\n","                  0.13886300093196646,\n","                  0.06718346253229979,\n","                  0.06183510638297873],\n","                 [0.18827938671209543,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.15656565656565657,\n","                  0.23076923076923073,\n","                  0.27474150664697194,\n","                  0.19876288659793817,\n","                  0.23971749065226422,\n","                  0.21735407449693167,\n","                  0.19075144508670516,\n","                  0.21619077502353312,\n","                  0.19287109375,\n","                  0.25057825751734775,\n","                  0.20969245107176138,\n","                  0.19096509240246407,\n","                  0.24268774703557316,\n","                  0.20861678004535145,\n","                  0.2627627627627628,\n","                  0.2222222222222222,\n","                  0.20322085889570551,\n","                  0.18943064809206545,\n","                  0.1826923076923077,\n","                  0.19113208582560504,\n","                  0.18668528864059586,\n","                  0.13093980992608234,\n","                  0.13979496738117425,\n","                  0.07493540051679581,\n","                  0.06183510638297873],\n","                 [0.18759795570698468,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.15656565656565657,\n","                  0.23076923076923073,\n","                  0.27326440177252587,\n","                  0.19835051546391758,\n","                  0.23764021603656005,\n","                  0.2163550734979306,\n","                  0.19075144508670516,\n","                  0.21587700031377466,\n","                  0.19140625,\n","                  0.24903623747108716,\n","                  0.20969245107176138,\n","                  0.19096509240246407,\n","                  0.24189723320158107,\n","                  0.20861678004535145,\n","                  0.2627627627627628,\n","                  0.21111111111111114,\n","                  0.20322085889570551,\n","                  0.18801736321421358,\n","                  0.1826923076923077,\n","                  0.19041870164078367,\n","                  0.17830540037243947,\n","                  0.129883843717001,\n","                  0.13979496738117425,\n","                  0.07493540051679581,\n","                  0.06183510638297873],\n","                 [0.1871550255536627,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.15656565656565657,\n","                  0.23076923076923073,\n","                  0.27326440177252587,\n","                  0.19835051546391758,\n","                  0.23764021603656005,\n","                  0.21507064364207218,\n","                  0.19075144508670516,\n","                  0.21587700031377466,\n","                  0.19140625,\n","                  0.2447956823438705,\n","                  0.20969245107176138,\n","                  0.19096509240246407,\n","                  0.24189723320158107,\n","                  0.20691609977324266,\n","                  0.2627627627627628,\n","                  0.21111111111111114,\n","                  0.20092024539877296,\n","                  0.1872097718554412,\n","                  0.1826923076923077,\n","                  0.18965044174943757,\n","                  0.17830540037243947,\n","                  0.11615628299894398,\n","                  0.13979496738117425,\n","                  0.07493540051679581,\n","                  0.06183510638297873],\n","                 [0.18575809199318571,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.15656565656565657,\n","                  0.23076923076923073,\n","                  0.27326440177252587,\n","                  0.194639175257732,\n","                  0.23597839634399664,\n","                  0.21507064364207218,\n","                  0.19075144508670516,\n","                  0.2127392532161908,\n","                  0.1875,\n","                  0.24325366229760992,\n","                  0.20969245107176138,\n","                  0.19096509240246407,\n","                  0.24189723320158107,\n","                  0.20578231292517002,\n","                  0.2627627627627628,\n","                  0.21111111111111114,\n","                  0.20092024539877296,\n","                  0.1857460125176661,\n","                  0.1826923076923077,\n","                  0.18838830049936894,\n","                  0.17830540037243947,\n","                  0.09820485744456176,\n","                  0.11742777260018644,\n","                  0.07493540051679581,\n","                  0.06183510638297873],\n","                 [0.18562180579216359,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.15656565656565657,\n","                  0.23076923076923073,\n","                  0.27326440177252587,\n","                  0.194639175257732,\n","                  0.235147486497715,\n","                  0.21507064364207218,\n","                  0.19075144508670516,\n","                  0.2127392532161908,\n","                  0.1865234375,\n","                  0.24325366229760992,\n","                  0.20969245107176138,\n","                  0.19096509240246407,\n","                  0.24189723320158107,\n","                  0.2046485260770975,\n","                  0.2627627627627628,\n","                  0.21111111111111114,\n","                  0.20092024539877296,\n","                  0.18544316575812636,\n","                  0.1826923076923077,\n","                  0.18822367337979473,\n","                  0.17830540037243947,\n","                  0.09820485744456176,\n","                  0.11742777260018644,\n","                  0.06459948320413433,\n","                  0.06183510638297873],\n","                 [0.1851788756388416,\n","                  0.17372182517866963,\n","                  0.29243353783231085,\n","                  0.24163969795037754,\n","                  0.15656565656565657,\n","                  0.23076923076923073,\n","                  0.27326440177252587,\n","                  0.194639175257732,\n","                  0.2318238471125883,\n","                  0.21492792921364345,\n","                  0.19075144508670516,\n","                  0.21148415437715717,\n","                  0.1865234375,\n","                  0.24286815728604472,\n","                  0.20969245107176138,\n","                  0.19096509240246407,\n","                  0.24189723320158107,\n","                  0.2029478458049887,\n","                  0.2627627627627628,\n","                  0.21111111111111114,\n","                  0.20092024539877296,\n","                  0.18503937007874016,\n","                  0.1826923076923077,\n","                  0.18734566207539927,\n","                  0.17830540037243947,\n","                  0.09820485744456176,\n","                  0.11742777260018644,\n","                  0.04134366925064603,\n","                  0.059175531914893664]],\n"," 'track_rejects': [1],\n"," 'train_errors': [[0.28344674513010715,\n","                   0.5546695662679715,\n","                   0.5438673068529027,\n","                   0.5312569521690768,\n","                   0.5147113594040968,\n","                   0.437984496124031,\n","                   0.42611894543225015,\n","                   0.38288740754269013,\n","                   0.3556405353728489,\n","                   0.3521507025832975,\n","                   0.34404761904761905,\n","                   0.34098577571948396,\n","                   0.33964226289517474,\n","                   0.33831230148510605,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.328375,\n","                   0.3203002815139193,\n","                   0.3174224343675418,\n","                   0.3038952316991269,\n","                   0.3037614639210936,\n","                   0.2985347985347986,\n","                   0.2980504681946462,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.25689962325866655,\n","                   0.11537996858765254,\n","                   0.5438673068529027,\n","                   0.5312569521690768,\n","                   0.5147113594040968,\n","                   0.437984496124031,\n","                   0.42611894543225015,\n","                   0.28380969774449827,\n","                   0.31903851406719475,\n","                   0.3126955881450574,\n","                   0.34404761904761905,\n","                   0.3062520674826331,\n","                   0.276518302828619,\n","                   0.31899733882736714,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.322125,\n","                   0.3203002815139193,\n","                   0.2935560859188544,\n","                   0.2718267293485561,\n","                   0.2742148295552864,\n","                   0.2985347985347986,\n","                   0.2712694982701005,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.24992698811366487,\n","                   0.11537996858765254,\n","                   0.12701876909646448,\n","                   0.5312569521690768,\n","                   0.5147113594040968,\n","                   0.437984496124031,\n","                   0.42611894543225015,\n","                   0.2703862660944206,\n","                   0.31266502777019034,\n","                   0.3021108179419525,\n","                   0.34404761904761905,\n","                   0.30069467416473705,\n","                   0.2268094841930116,\n","                   0.3154777234097348,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.3175,\n","                   0.3203002815139193,\n","                   0.2816229116945107,\n","                   0.25520483546004025,\n","                   0.26653616542654435,\n","                   0.2985347985347986,\n","                   0.2648812686716969,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.23750036505943173,\n","                   0.11537996858765254,\n","                   0.12701876909646448,\n","                   0.1526140155728587,\n","                   0.5147113594040968,\n","                   0.437984496124031,\n","                   0.42611894543225015,\n","                   0.26719021093964024,\n","                   0.29882545752526635,\n","                   0.28483770019021903,\n","                   0.34404761904761905,\n","                   0.2903076414158121,\n","                   0.21006655574043265,\n","                   0.2592497210060949,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.31037499999999996,\n","                   0.3203002815139193,\n","                   0.26730310262529833,\n","                   0.2432840832773674,\n","                   0.2522278940993251,\n","                   0.2985347985347986,\n","                   0.25123100357788086,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.22879734820828834,\n","                   0.11537996858765254,\n","                   0.12701876909646448,\n","                   0.1526140155728587,\n","                   0.07076350093109873,\n","                   0.437984496124031,\n","                   0.42611894543225015,\n","                   0.20345173956716278,\n","                   0.28025129745971045,\n","                   0.2733325151868442,\n","                   0.34404761904761905,\n","                   0.26635792259345026,\n","                   0.20902662229617308,\n","                   0.2583054339428277,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.28937500000000005,\n","                   0.3203002815139193,\n","                   0.26730310262529833,\n","                   0.23052384150436533,\n","                   0.24114249870219762,\n","                   0.2985347985347986,\n","                   0.24120585213787238,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.2274612306883561,\n","                   0.11537996858765254,\n","                   0.12701876909646448,\n","                   0.1526140155728587,\n","                   0.07076350093109873,\n","                   0.08333333333333337,\n","                   0.42611894543225015,\n","                   0.2021733175052507,\n","                   0.2775198033324229,\n","                   0.27109283917285387,\n","                   0.34404761904761905,\n","                   0.2659609659278862,\n","                   0.20746672212978368,\n","                   0.25718945832260276,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.2885,\n","                   0.3203002815139193,\n","                   0.26491646778042954,\n","                   0.2293485560779046,\n","                   0.2397473611351445,\n","                   0.2985347985347986,\n","                   0.23991875966795373,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.2211749072749044,\n","                   0.11537996858765254,\n","                   0.12701876909646448,\n","                   0.1526140155728587,\n","                   0.07076350093109873,\n","                   0.08333333333333337,\n","                   0.1621704475781729,\n","                   0.19258515204090954,\n","                   0.2651370299553856,\n","                   0.2681475118119899,\n","                   0.34404761904761905,\n","                   0.25881574594773404,\n","                   0.2063227953410982,\n","                   0.2539273757404069,\n","                   0.33788187372708756,\n","                   0.3360871472437491,\n","                   0.3357338820301783,\n","                   0.277,\n","                   0.3203002815139193,\n","                   0.2601431980906921,\n","                   0.22246474143720618,\n","                   0.23303123377747015,\n","                   0.2985347985347986,\n","                   0.2327511896749206,\n","                   0.19896559003588765,\n","                   0.11613205338328259,\n","                   0.11183819155264718,\n","                   0.06400839454354668,\n","                   0.05395284327323158],\n","                  [0.21621009900411792,\n","                   0.11634650235592603,\n","                   0.14185945002182454,\n","                   0.1526140155728587,\n","                   0.07076350093109873,\n","                   0.1027131782945736,\n","                   0.1621704475781729,\n","                   0.13049036617660492,\n","                   0.25694254757352275,\n","                   0.26366815978400937,\n","                   0.3291666666666667,\n","                   0.25881574594773404,\n","                   0.2063227953410982,\n","                   0.2539273757404069,\n","                   0.3293279022403258,\n","                   0.3351367158941365,\n","                   0.3175582990397805,\n","                   0.271875,\n","                   0.3137316233969346,\n","                   0.2577565632458234,\n","                   0.21642041638683684,\n","                   0.22750475860875585,\n","                   0.2985347985347986,\n","                   0.22798068180476339,\n","                   0.19263246780662868,\n","                   0.11308826972605945,\n","                   0.10945865556216539,\n","                   0.06400839454354668,\n","                   0.05339805825242716],\n","                  [0.20871177827750353,\n","                   0.11707140268213123,\n","                   0.14185945002182454,\n","                   0.1526140155728587,\n","                   0.07076350093109873,\n","                   0.1027131782945736,\n","                   0.171367259350092,\n","                   0.13021641859190947,\n","                   0.16343439861604303,\n","                   0.26366815978400937,\n","                   0.31607142857142856,\n","                   0.24809791597750575,\n","                   0.20081114808652245,\n","                   0.24834749763928232,\n","                   0.3177189409368636,\n","                   0.323439099283521,\n","                   0.30401234567901236,\n","                   0.26,\n","                   0.29371285580231465,\n","                   0.24105011933174225,\n","                   0.21087978509066485,\n","                   0.21916637826613605,\n","                   0.2802197802197802,\n","                   0.2200691959804929,\n","                   0.18672155372598687,\n","                   0.11308826972605945,\n","                   0.10866547689867145,\n","                   0.0645330535152151,\n","                   0.05256588072122048],\n","                  [0.1898382056598814,\n","                   0.13350247674278115,\n","                   0.1671759057180271,\n","                   0.1526140155728587,\n","                   0.10689013035381756,\n","                   0.1027131782945736,\n","                   0.1808706315144083,\n","                   0.14080905853346726,\n","                   0.16343439861604303,\n","                   0.18435908449407867,\n","                   0.24464285714285716,\n","                   0.22123718160767447,\n","                   0.19165973377703827,\n","                   0.2270581165765302,\n","                   0.274949083503055,\n","                   0.24119023249013016,\n","                   0.29441015089163236,\n","                   0.229375,\n","                   0.23709727869878006,\n","                   0.21718377088305485,\n","                   0.20449966420416388,\n","                   0.19680091711368752,\n","                   0.20146520146520142,\n","                   0.19772810466777668,\n","                   0.1796495672366477,\n","                   0.11519550456567551,\n","                   0.10688082490581008,\n","                   0.06400839454354668,\n","                   0.05423023578363384],\n","                  [0.1878376799742998,\n","                   0.13350247674278115,\n","                   0.1671759057180271,\n","                   0.1526140155728587,\n","                   0.10689013035381756,\n","                   0.1027131782945736,\n","                   0.1808706315144083,\n","                   0.14080905853346726,\n","                   0.1628880997905855,\n","                   0.1834079892004663,\n","                   0.08154761904761909,\n","                   0.21812768772742308,\n","                   0.18479617304492513,\n","                   0.22482616533608035,\n","                   0.274949083503055,\n","                   0.24119023249013016,\n","                   0.29441015089163236,\n","                   0.228375,\n","                   0.23709727869878006,\n","                   0.21002386634844872,\n","                   0.20349227669576897,\n","                   0.19474606333275657,\n","                   0.20146520146520142,\n","                   0.1957561373056077,\n","                   0.1796495672366477,\n","                   0.11519550456567551,\n","                   0.10688082490581008,\n","                   0.06400839454354668,\n","                   0.05423023578363384],\n","                  [0.18216465640606294,\n","                   0.13350247674278115,\n","                   0.1671759057180271,\n","                   0.1526140155728587,\n","                   0.10689013035381756,\n","                   0.1027131782945736,\n","                   0.190680564071122,\n","                   0.14080905853346726,\n","                   0.1632522990075571,\n","                   0.18334662821378167,\n","                   0.09166666666666667,\n","                   0.16672179953688393,\n","                   0.18479617304492513,\n","                   0.22482616533608035,\n","                   0.2625254582484725,\n","                   0.23365989179704638,\n","                   0.28532235939643347,\n","                   0.21875,\n","                   0.2349077259931186,\n","                   0.2004773269689738,\n","                   0.19459368703828073,\n","                   0.18815971621387784,\n","                   0.11721611721611724,\n","                   0.18968672877773451,\n","                   0.17584969389909222,\n","                   0.11496136736127371,\n","                   0.10430299424945466,\n","                   0.0645330535152151,\n","                   0.05395284327323158],\n","                  [0.17958003562980052,\n","                   0.13350247674278115,\n","                   0.1907463989524225,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.1027131782945736,\n","                   0.190680564071122,\n","                   0.14080905853346726,\n","                   0.1632522990075571,\n","                   0.18334662821378167,\n","                   0.10297619047619044,\n","                   0.16672179953688393,\n","                   0.14798252911813647,\n","                   0.22482616533608035,\n","                   0.24521384928716905,\n","                   0.2318321392016377,\n","                   0.28292181069958844,\n","                   0.21437499999999998,\n","                   0.23334375977478883,\n","                   0.19570405727923623,\n","                   0.18922095366017466,\n","                   0.18504499048278245,\n","                   0.11721611721611724,\n","                   0.187136160213492,\n","                   0.17394975723031458,\n","                   0.11355654413486305,\n","                   0.10251834225659329,\n","                   0.06348373557187825,\n","                   0.05395284327323158],\n","                  [0.17544026167460058,\n","                   0.13495227739519144,\n","                   0.1907463989524225,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.10658914728682167,\n","                   0.19681177191906807,\n","                   0.14080905853346726,\n","                   0.1644359464627151,\n","                   0.18236485242682698,\n","                   0.10833333333333328,\n","                   0.16672179953688393,\n","                   0.14798252911813647,\n","                   0.17615245943857838,\n","                   0.23340122199592672,\n","                   0.22408246819710487,\n","                   0.2772633744855967,\n","                   0.20799999999999996,\n","                   0.22583672192680637,\n","                   0.16229116945107402,\n","                   0.1905641370047011,\n","                   0.18104343311991689,\n","                   0.11721611721611724,\n","                   0.18169258563888202,\n","                   0.1712054042643023,\n","                   0.11004448606883632,\n","                   0.10113027959547893,\n","                   0.06400839454354668,\n","                   0.05298196948682388],\n","                  [0.17303086942554247,\n","                   0.13495227739519144,\n","                   0.1907463989524225,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.10658914728682167,\n","                   0.19681177191906807,\n","                   0.14080905853346726,\n","                   0.1644359464627151,\n","                   0.18236485242682698,\n","                   0.10833333333333328,\n","                   0.16672179953688393,\n","                   0.14735856905158073,\n","                   0.17615245943857838,\n","                   0.1661914460285132,\n","                   0.22408246819710487,\n","                   0.2772633744855967,\n","                   0.20737499999999998,\n","                   0.22583672192680637,\n","                   0.15990453460620524,\n","                   0.18821356615177975,\n","                   0.17818826786641284,\n","                   0.11721611721611724,\n","                   0.17922467438922152,\n","                   0.1712054042643023,\n","                   0.11004448606883632,\n","                   0.10113027959547893,\n","                   0.06400839454354668,\n","                   0.05298196948682388],\n","                  [0.1651820916445197,\n","                   0.13495227739519144,\n","                   0.1907463989524225,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.10658914728682167,\n","                   0.19681177191906807,\n","                   0.14080905853346726,\n","                   0.1644359464627151,\n","                   0.18236485242682698,\n","                   0.10833333333333328,\n","                   0.1639431028779358,\n","                   0.14642262895174707,\n","                   0.17615245943857838,\n","                   0.1661914460285132,\n","                   0.1454891065945314,\n","                   0.2772633744855967,\n","                   0.186875,\n","                   0.22583672192680637,\n","                   0.14558472553699287,\n","                   0.1821692411014103,\n","                   0.1693415815885101,\n","                   0.11721611721611724,\n","                   0.169707275024502,\n","                   0.1712054042643023,\n","                   0.11004448606883632,\n","                   0.10113027959547893,\n","                   0.06400839454354668,\n","                   0.05298196948682388],\n","                  [0.16197686983440907,\n","                   0.13495227739519144,\n","                   0.1907463989524225,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.10658914728682167,\n","                   0.19681177191906807,\n","                   0.14080905853346726,\n","                   0.16334334881180002,\n","                   0.18236485242682698,\n","                   0.10833333333333328,\n","                   0.16672179953688393,\n","                   0.14642262895174707,\n","                   0.17615245943857838,\n","                   0.1661914460285132,\n","                   0.148559730954818,\n","                   0.19478737997256512,\n","                   0.17862500000000003,\n","                   0.22583672192680637,\n","                   0.14797136038186154,\n","                   0.18132975151108122,\n","                   0.16570773490223223,\n","                   0.11721611721611724,\n","                   0.16552717654421578,\n","                   0.1712054042643023,\n","                   0.11004448606883632,\n","                   0.10113027959547893,\n","                   0.06400839454354668,\n","                   0.05298196948682388],\n","                  [0.15999094652609447,\n","                   0.13567717772139665,\n","                   0.19161938018332603,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.11046511627906974,\n","                   0.20202329858982215,\n","                   0.14099169025659752,\n","                   0.16334334881180002,\n","                   0.18046266183960236,\n","                   0.10833333333333328,\n","                   0.1630830301025471,\n","                   0.14756655574043265,\n","                   0.17615245943857838,\n","                   0.16517311608961305,\n","                   0.14870595116245067,\n","                   0.19650205761316875,\n","                   0.144625,\n","                   0.22489834219580862,\n","                   0.14319809069212408,\n","                   0.17897918065815988,\n","                   0.1640314068177885,\n","                   0.10439560439560436,\n","                   0.16360244193323648,\n","                   0.15948912814017313,\n","                   0.10910793725122925,\n","                   0.10013880626611149,\n","                   0.07082896117523607,\n","                   0.05228848821081833],\n","                  [0.15886656347653394,\n","                   0.13567717772139665,\n","                   0.19161938018332603,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.11046511627906974,\n","                   0.20202329858982215,\n","                   0.14099169025659752,\n","                   0.16334334881180002,\n","                   0.18046266183960236,\n","                   0.10833333333333328,\n","                   0.1630830301025471,\n","                   0.14756655574043265,\n","                   0.17615245943857838,\n","                   0.16517311608961305,\n","                   0.14870595116245067,\n","                   0.19650205761316875,\n","                   0.14437500000000003,\n","                   0.17672818267125434,\n","                   0.13365155131264916,\n","                   0.1774680993955675,\n","                   0.162917459768126,\n","                   0.10439560439560436,\n","                   0.16277586878741723,\n","                   0.15948912814017313,\n","                   0.10910793725122925,\n","                   0.10013880626611149,\n","                   0.07082896117523607,\n","                   0.05228848821081833],\n","                  [0.15861102187436116,\n","                   0.13567717772139665,\n","                   0.19249236141422954,\n","                   0.1523915461624027,\n","                   0.10763500931098702,\n","                   0.11046511627906974,\n","                   0.20110361741263028,\n","                   0.14080905853346726,\n","                   0.1629791495948284,\n","                   0.18030925937289066,\n","                   0.10833333333333328,\n","                   0.1630830301025471,\n","                   0.14756655574043265,\n","                   0.17589492660314188,\n","                   0.16476578411405296,\n","                   0.14870595116245067,\n","                   0.19615912208504804,\n","                   0.14449999999999996,\n","                   0.17672818267125434,\n","                   0.05011933174224348,\n","                   0.1774680993955675,\n","                   0.162917459768126,\n","                   0.10439560439560436,\n","                   0.1626341705338482,\n","                   0.1593835761030188,\n","                   0.10910793725122925,\n","                   0.09994051160023798,\n","                   0.07082896117523607,\n","                   0.052011095700416066],\n","                  [0.15777138518150757,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.1523915461624027,\n","                   0.11210428305400377,\n","                   0.11046511627906974,\n","                   0.20110361741263028,\n","                   0.1411743219797279,\n","                   0.1629791495948284,\n","                   0.18030925937289066,\n","                   0.10833333333333328,\n","                   0.1630830301025471,\n","                   0.1468386023294509,\n","                   0.17400635247660745,\n","                   0.16435845213849287,\n","                   0.14914461178534877,\n","                   0.19598765432098764,\n","                   0.14400000000000002,\n","                   0.178292148889584,\n","                   0.05011933174224348,\n","                   0.15815983881799867,\n","                   0.16177106765876448,\n","                   0.10439560439560436,\n","                   0.16264597872164555,\n","                   0.1573780873970868,\n","                   0.10863966284242565,\n","                   0.10132857426135233,\n","                   0.0713536201469045,\n","                   0.052011095700416066],\n","                  [0.15777138518150757,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.11210428305400377,\n","                   0.1647286821705426,\n","                   0.22532188841201717,\n","                   0.1411743219797279,\n","                   0.1629791495948284,\n","                   0.18767257777505064,\n","                   0.10833333333333328,\n","                   0.17466093284816409,\n","                   0.15848585690515804,\n","                   0.19787106189372483,\n","                   0.18879837067209781,\n","                   0.16530194472876147,\n","                   0.19633058984910834,\n","                   0.14400000000000002,\n","                   0.19705974350954014,\n","                   0.054892601431980936,\n","                   0.15815983881799867,\n","                   0.1617494376189652,\n","                   0.13003663003663002,\n","                   0.16277586878741723,\n","                   0.15104496516782773,\n","                   0.10536174198080073,\n","                   0.10093198492960542,\n","                   0.06873032528856249,\n","                   0.050901525658807234],\n","                  [0.15730410910896298,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.11210428305400377,\n","                   0.1647286821705426,\n","                   0.22532188841201717,\n","                   0.1411743219797279,\n","                   0.16188655194391333,\n","                   0.187243050868258,\n","                   0.10833333333333328,\n","                   0.17042672841548134,\n","                   0.15848585690515804,\n","                   0.19787106189372483,\n","                   0.18879837067209781,\n","                   0.16530194472876147,\n","                   0.19633058984910834,\n","                   0.14312499999999995,\n","                   0.19705974350954014,\n","                   0.054892601431980936,\n","                   0.1576561450638012,\n","                   0.16111135144488664,\n","                   0.012820512820512775,\n","                   0.16223269214873592,\n","                   0.15104496516782773,\n","                   0.10536174198080073,\n","                   0.10093198492960542,\n","                   0.06873032528856249,\n","                   0.050901525658807234],\n","                  [0.15891037060833502,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.13929236499068898,\n","                   0.1647286821705426,\n","                   0.2375843041079092,\n","                   0.1411743219797279,\n","                   0.1629791495948284,\n","                   0.187243050868258,\n","                   0.125,\n","                   0.17565332451207405,\n","                   0.1639975041597338,\n","                   0.2026783414885398,\n","                   0.19307535641547857,\n","                   0.16530194472876147,\n","                   0.19478737997256512,\n","                   0.15649999999999997,\n","                   0.19705974350954014,\n","                   0.06921241050119331,\n","                   0.15815983881799867,\n","                   0.16324191036511504,\n","                   0.08424908424908428,\n","                   0.16479506890077578,\n","                   0.15188938146506226,\n","                   0.11285413252165766,\n","                   0.10688082490581008,\n","                   0.08079748163693601,\n","                   0.054785020804438256],\n","                  [0.15709237463859116,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.1389199255121043,\n","                   0.1647286821705426,\n","                   0.2375843041079092,\n","                   0.14126563784129298,\n","                   0.1630701993990713,\n","                   0.18583174817451065,\n","                   0.1255952380952381,\n","                   0.17565332451207405,\n","                   0.16337354409317806,\n","                   0.20113314447592068,\n","                   0.19307535641547857,\n","                   0.16530194472876147,\n","                   0.19461591220850483,\n","                   0.15637500000000004,\n","                   0.19705974350954014,\n","                   0.05011933174224348,\n","                   0.15799194089993285,\n","                   0.16169536251946703,\n","                   0.08424908424908428,\n","                   0.16283490972640424,\n","                   0.12634578847371758,\n","                   0.11261999531725586,\n","                   0.10688082490581008,\n","                   0.08079748163693601,\n","                   0.054785020804438256],\n","                  [0.15590958207996264,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.1389199255121043,\n","                   0.1647286821705426,\n","                   0.2375843041079092,\n","                   0.14126563784129298,\n","                   0.1630701993990713,\n","                   0.18454316745413268,\n","                   0.1255952380952381,\n","                   0.17565332451207405,\n","                   0.16160565723793674,\n","                   0.1958107992102326,\n","                   0.19307535641547857,\n","                   0.16530194472876147,\n","                   0.19461591220850483,\n","                   0.153625,\n","                   0.19705974350954014,\n","                   0.05011933174224348,\n","                   0.15681665547347212,\n","                   0.16059223048970406,\n","                   0.08424908424908428,\n","                   0.16151239269309337,\n","                   0.12634578847371758,\n","                   0.07468976820416762,\n","                   0.10688082490581008,\n","                   0.08079748163693601,\n","                   0.054785020804438256],\n","                  [0.1558803773254286,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.1389199255121043,\n","                   0.1647286821705426,\n","                   0.2375843041079092,\n","                   0.14090037439503245,\n","                   0.1628880997905855,\n","                   0.18454316745413268,\n","                   0.1255952380952381,\n","                   0.1738008600727754,\n","                   0.16056572379367717,\n","                   0.19907288179242855,\n","                   0.19307535641547857,\n","                   0.16530194472876147,\n","                   0.19461591220850483,\n","                   0.15425,\n","                   0.19705974350954014,\n","                   0.05011933174224348,\n","                   0.15681665547347212,\n","                   0.16050571033050698,\n","                   0.08424908424908428,\n","                   0.16150058450529592,\n","                   0.12634578847371758,\n","                   0.10044486068836334,\n","                   0.08427523299623241,\n","                   0.08079748163693601,\n","                   0.054785020804438256],\n","                  [0.15499693350077393,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.1389199255121043,\n","                   0.1647286821705426,\n","                   0.2375843041079092,\n","                   0.14090037439503245,\n","                   0.16234180096512796,\n","                   0.18417500153402466,\n","                   0.1255952380952381,\n","                   0.17313926563016868,\n","                   0.16035773710482526,\n","                   0.19812859472916133,\n","                   0.19307535641547857,\n","                   0.16530194472876147,\n","                   0.19461591220850483,\n","                   0.14887499999999998,\n","                   0.19705974350954014,\n","                   0.05011933174224348,\n","                   0.1566487575554063,\n","                   0.1597053988579339,\n","                   0.08424908424908428,\n","                   0.16036699847674374,\n","                   0.12634578847371758,\n","                   0.10044486068836334,\n","                   0.08427523299623241,\n","                   0.017313746065057756,\n","                   0.054785020804438256],\n","                  [0.1545734645600304,\n","                   0.1355563610003625,\n","                   0.1903099083369707,\n","                   0.19933259176863183,\n","                   0.1389199255121043,\n","                   0.1647286821705426,\n","                   0.2375843041079092,\n","                   0.14080905853346726,\n","                   0.1615223527269416,\n","                   0.18454316745413268,\n","                   0.1255952380952381,\n","                   0.17174991730069467,\n","                   0.16035773710482526,\n","                   0.19864366040003434,\n","                   0.19307535641547857,\n","                   0.16530194472876147,\n","                   0.19461591220850483,\n","                   0.14849999999999997,\n","                   0.19705974350954014,\n","                   0.05250596658711215,\n","                   0.15614506380120885,\n","                   0.15926198304204875,\n","                   0.08424908424908428,\n","                   0.1600009446550238,\n","                   0.12634578847371758,\n","                   0.10044486068836334,\n","                   0.08427523299623241,\n","                   0.0598111227701994,\n","                   0.035506241331484056]],\n"," 'update_node_indices_tracking_rejects': [0],\n"," 'update_nodes': [<__main__.PointerDecisionListNode object at 0x7f4c96db0a10>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96db02d0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96db0c90>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96db0ad0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96db0410>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96db0790>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96db07d0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96db09d0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96db06d0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96da9910>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96da9fd0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96da9150>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96da9b90>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96da93d0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96da9e10>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96da97d0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96da9210>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96cb2e90>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96cb2b10>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96cb2650>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96cb2910>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96cb2550>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96cb2ad0>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c976df810>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c976df250>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4cb36ed210>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96e95110>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c96e95990>,\n","                  <__main__.PointerDecisionListNode object at 0x7f4c97f6f510>]}\n"]}]},{"cell_type":"code","source":["#f = content"],"metadata":{"id":"mG_E3j_QOEat","executionInfo":{"status":"ok","timestamp":1648310449161,"user_tz":240,"elapsed":133,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"LDKlep9hyo2c"},"source":["# Analysis of Your Final Model"]},{"cell_type":"markdown","source":["## Part 1"],"metadata":{"id":"JStSHzwi_zx_"}},{"cell_type":"markdown","source":["1. How does your final model perform? On both the validation set and the training data, calculate f's error rates on each of the groups you identified, calculate the error rates of the initial model on each of the groups you identified, and compare them by taking their difference. Hint: you can use the helper function `bountyHuntWrapper.measure_group_error(model, g, x, y)` to get the error of f on x and y restricted to just the datapoints in a group g, and you can use `metrics.zero_one_loss` for the initial model (which is just a DL so you can directly use the scikit.learn functions on it)."],"metadata":{"id":"B2KB2yMGrdiE"}},{"cell_type":"markdown","source":["My final model achieves an overall accuracy of 0.8454 on the training data and 0.8148 on the validation data. To see the breakdown of the error rates per group, please see the relevant cells below in the Data Exploration section of this notebook."],"metadata":{"id":"VNVStlclrhEQ"}},{"cell_type":"markdown","source":["#### Error Output from Training Data"],"metadata":{"id":"oTaxkwdZ-QNu"}},{"cell_type":"markdown","source":["~~~~~~~~~~~~~~~~~\n","EDU\n","model error: 0.1355563610003625\n","stump error: 0.5546695662679715\n","error difference: -0.419113205267609\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","PRT\n","model error: 0.1389199255121043\n","stump error: 0.5147113594040968\n","error difference: -0.37579143389199254\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CMS\n","model error: 0.1903099083369707\n","stump error: 0.5438673068529027\n","error difference: -0.35355739851593204\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","ENT\n","model error: 0.19933259176863183\n","stump error: 0.5312569521690768\n","error difference: -0.33192436040044493\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","LGL\n","model error: 0.1647286821705426\n","stump error: 0.437984496124031\n","error difference: -0.2732558139534884\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","oceanian\n","model error: 0.05250596658711215\n","stump error: 0.3174224343675418\n","error difference: -0.26491646778042965\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","local govt worker\n","model error: 0.14080905853346726\n","stump error: 0.38288740754269013\n","error difference: -0.24207834900922287\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","SCI\n","model error: 0.1255952380952381\n","stump error: 0.34404761904761905\n","error difference: -0.21845238095238095\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","MIL\n","model error: 0.08424908424908428\n","stump error: 0.2985347985347986\n","error difference: -0.2142857142857143\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","part time\n","model error: 0.06750504211650254\n","stump error: 0.26780559180606633\n","error difference: -0.2003005496895638\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime white\n","model error: 0.15157661810805823\n","stump error: 0.3459339848792181\n","error difference: -0.1943573667711599\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","associates degree\n","model error: 0.1615223527269416\n","stump error: 0.3556405353728489\n","error difference: -0.1941181826459073\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","RPR\n","model error: 0.2375843041079092\n","stump error: 0.42611894543225015\n","error difference: -0.18853464132434095\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","over time\n","model error: 0.14849999999999997\n","stump error: 0.328375\n","error difference: -0.179875\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","non profit worker\n","model error: 0.16035773710482526\n","stump error: 0.33964226289517474\n","error difference: -0.17928452579034948\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime black\n","model error: 0.1498559077809798\n","stump error: 0.32564841498559083\n","error difference: -0.17579250720461104\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","SAL\n","model error: 0.16530194472876147\n","stump error: 0.3360871472437491\n","error difference: -0.17078520251498763\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","cow minorities\n","model error: 0.17174991730069467\n","stump error: 0.34098577571948396\n","error difference: -0.1692358584187893\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","bachelors degree\n","model error: 0.18454316745413268\n","stump error: 0.3521507025832975\n","error difference: -0.16760753512916482\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced black women\n","model error: 0.2351598173515982\n","stump error: 0.4018264840182648\n","error difference: -0.16666666666666663\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single asian women\n","model error: 0.13140643623361148\n","stump error: 0.2949940405244339\n","error difference: -0.16358760429082242\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","african\n","model error: 0.1428571428571429\n","stump error: 0.30434782608695654\n","error difference: -0.16149068322981364\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single black women\n","model error: 0.1062124248496994\n","stump error: 0.2645290581162325\n","error difference: -0.1583166332665331\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","twenties\n","model error: 0.0792342133703875\n","stump error: 0.2327312027173074\n","error difference: -0.1534969893469199\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","other race\n","model error: 0.13979979288919575\n","stump error: 0.29288919571971006\n","error difference: -0.1530894028305143\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","elderly\n","model error: 0.18570410828781758\n","stump error: 0.3360246972215626\n","error difference: -0.150320588933745\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","black men\n","model error: 0.1585704371963914\n","stump error: 0.30707841776544065\n","error difference: -0.14850798056904924\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","black\n","model error: 0.15614506380120885\n","stump error: 0.3038952316991269\n","error difference: -0.14775016789791806\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","black women\n","model error: 0.1538711776187378\n","stump error: 0.3009108653220559\n","error difference: -0.1470396877033181\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime asian\n","model error: 0.1293402777777778\n","stump error: 0.27604166666666663\n","error difference: -0.14670138888888884\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","european\n","model error: 0.1607629427792916\n","stump error: 0.3073569482288828\n","error difference: -0.14659400544959122\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single white women\n","model error: 0.1104080987029421\n","stump error: 0.255773489402088\n","error difference: -0.1453653906991459\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","BUS\n","model error: 0.19307535641547857\n","stump error: 0.33788187372708756\n","error difference: -0.14480651731160898\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","US born\n","model error: 0.15926198304204875\n","stump error: 0.3037614639210936\n","error difference: -0.14449948087904485\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","white men\n","model error: 0.16199960342814335\n","stump error: 0.30648394985569194\n","error difference: -0.1444843464275486\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single women\n","model error: 0.10784092447975446\n","stump error: 0.2517943393671286\n","error difference: -0.14395341488737412\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CON\n","model error: 0.19461591220850483\n","stump error: 0.3357338820301783\n","error difference: -0.14111796982167346\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","married men\n","model error: 0.17857500500700985\n","stump error: 0.3182705788103345\n","error difference: -0.13969557380332465\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","self employed worker\n","model error: 0.19864366040003434\n","stump error: 0.33831230148510605\n","error difference: -0.1396686410850717\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged black\n","model error: 0.2038523274478331\n","stump error: 0.34253611556982344\n","error difference: -0.13868378812199034\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","white\n","model error: 0.1600009446550238\n","stump error: 0.2980504681946462\n","error difference: -0.1380495235396224\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced women\n","model error: 0.20487019521030392\n","stump error: 0.33930368283356815\n","error difference: -0.13443348762326424\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged white\n","model error: 0.18341244725738393\n","stump error: 0.3175545007032349\n","error difference: -0.13414205344585095\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single\n","model error: 0.11327407778334164\n","stump error: 0.24632782507093975\n","error difference: -0.1330537472875981\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","male\n","model error: 0.1587538527456428\n","stump error: 0.29157855454658543\n","error difference: -0.13282470180094264\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","native american men\n","model error: 0.13808975834292292\n","stump error: 0.2692750287686997\n","error difference: -0.13118527042577677\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","white women\n","model error: 0.1576925034352893\n","stump error: 0.288309837650771\n","error difference: -0.13061733421548172\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","married\n","model error: 0.17462287036778856\n","stump error: 0.30458968853693225\n","error difference: -0.12996681816914368\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","Total\n","model error: 0.1545734645600304\n","stump error: 0.28344674513010715\n","error difference: -0.12887328057007674\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","EXT\n","model error: 0.32558139534883723\n","stump error: 0.4534883720930233\n","error difference: -0.12790697674418605\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","female\n","model error: 0.14989243650658535\n","stump error: 0.2743410768730751\n","error difference: -0.12444864036648973\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single men\n","model error: 0.1179437439379244\n","stump error: 0.24162948593598443\n","error difference: -0.12368574199806004\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","sixties\n","model error: 0.19452415112386423\n","stump error: 0.3180296508847441\n","error difference: -0.12350549976087988\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","FIN\n","model error: 0.19705974350954014\n","stump error: 0.3203002815139193\n","error difference: -0.12324053800437917\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","high school grad\n","model error: 0.1614503816793893\n","stump error: 0.28396946564885495\n","error difference: -0.12251908396946565\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged native american\n","model error: 0.1701346389228886\n","stump error: 0.2925336597307222\n","error difference: -0.1223990208078336\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","forties\n","model error: 0.16650678866587953\n","stump error: 0.2885182998819362\n","error difference: -0.12201151121605669\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced_native_american_women\n","model error: 0.12195121951219512\n","stump error: 0.24390243902439024\n","error difference: -0.12195121951219512\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","native american\n","model error: 0.14217156568686262\n","stump error: 0.2639472105578884\n","error difference: -0.12177564487102577\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","fifties\n","model error: 0.18364688856729383\n","stump error: 0.303328509406657\n","error difference: -0.11968162083936318\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","married women\n","model error: 0.16965578000125858\n","stump error: 0.2873953810332893\n","error difference: -0.11773960103203074\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced\n","model error: 0.19510624597553128\n","stump error: 0.31270122343850615\n","error difference: -0.11759497746297487\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single native american women\n","model error: 0.10738255033557043\n","stump error: 0.22483221476510062\n","error difference: -0.1174496644295302\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","thirties\n","model error: 0.1723421926910299\n","stump error: 0.2888289036544851\n","error difference: -0.1164867109634552\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian men\n","model error: 0.15625823451910403\n","stump error: 0.2718489240228371\n","error difference: -0.11559068950373308\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian_women\n","model error: 0.15853027427979993\n","stump error: 0.27298602725547694\n","error difference: -0.114455752975677\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","advanced degree\n","model error: 0.12800548455021787\n","stump error: 0.24083051760442686\n","error difference: -0.11282503305420899\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","natie american women\n","model error: 0.14661654135338342\n","stump error: 0.25814536340852134\n","error difference: -0.11152882205513792\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","full time\n","model error: 0.1762782487919211\n","stump error: 0.28379486289148026\n","error difference: -0.10751661409955915\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced white women\n","model error: 0.19161327897495628\n","stump error: 0.29702970297029707\n","error difference: -0.10541642399534079\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian\n","model error: 0.16258448229251143\n","stump error: 0.26774804001081376\n","error difference: -0.10516355771830233\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian\n","model error: 0.16258448229251143\n","stump error: 0.26774804001081376\n","error difference: -0.10516355771830233\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","for profit worker\n","model error: 0.14701058761839947\n","stump error: 0.24843528611114207\n","error difference: -0.1014246984927426\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","mar minorities\n","model error: 0.16230045026606632\n","stump error: 0.2627916496111339\n","error difference: -0.10049119934506756\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","MGR\n","model error: 0.17568294980487154\n","stump error: 0.27526577849549183\n","error difference: -0.0995828286906203\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged asian\n","model error: 0.1702953052014884\n","stump error: 0.26458712690998343\n","error difference: -0.09429182170849504\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime native american\n","model error: 0.1839080459770115\n","stump error: 0.27586206896551724\n","error difference: -0.09195402298850575\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","MED\n","model error: 0.15573199609973531\n","stump error: 0.24571667363142502\n","error difference: -0.0899846775316897\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced asian women\n","model error: 0.18008948545861303\n","stump error: 0.26957494407158833\n","error difference: -0.0894854586129753\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","multiple race\n","model error: 0.1276367986902588\n","stump error: 0.21245513506706126\n","error difference: -0.08481833637680247\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","PRD\n","model error: 0.18551150269211947\n","stump error: 0.26692772067221404\n","error difference: -0.08141621798009457\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CMM\n","model error: 0.13857170505128702\n","stump error: 0.2144377782078576\n","error difference: -0.07586607315657057\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","american\n","model error: 0.12841646534981765\n","stump error: 0.20259580313580594\n","error difference: -0.07417933778598829\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","TRN\n","model error: 0.12634578847371758\n","stump error: 0.19896559003588765\n","error difference: -0.07261980156217007\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","ENG\n","model error: 0.12488493402884315\n","stump error: 0.19300398895366677\n","error difference: -0.06811905492482362\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","OFF\n","model error: 0.21330379829304835\n","stump error: 0.28014854387908006\n","error difference: -0.0668447455860317\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","some school\n","model error: 0.09503908366225522\n","stump error: 0.14031421716585402\n","error difference: -0.0452751335035988\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","no school\n","model error: 0.0972972972972973\n","stump error: 0.1409563409563409\n","error difference: -0.043659043659043606\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CLN\n","model error: 0.08427523299623241\n","stump error: 0.11183819155264718\n","error difference: -0.027562958556414774\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","EAT\n","model error: 0.035506241331484056\n","stump error: 0.05395284327323158\n","error difference: -0.01844660194174752\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","PRS\n","model error: 0.10044486068836334\n","stump error: 0.11613205338328259\n","error difference: -0.01568719269491925\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","HLS\n","model error: 0.09564628919467633\n","stump error: 0.1026392961876833\n","error difference: -0.006993006993006978\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","FFF\n","model error: 0.0598111227701994\n","stump error: 0.06400839454354668\n","error difference: -0.0041972717733472775\n","~~~~~~~~~~~~~~~~~"],"metadata":{"id":"b22bVxt1-UGp"}},{"cell_type":"markdown","source":["#### Error Output from Validation Data"],"metadata":{"id":"xJc8sq1K_eg9"}},{"cell_type":"markdown","source":["~~~~~~~~~~~~~~~~~\n","EDU\n","model error: 0.17372182517866963\n","stump error: 0.5519516217702034\n","error difference: -0.3782297965915338\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","PRT\n","model error: 0.15656565656565657\n","stump error: 0.48484848484848486\n","error difference: -0.3282828282828283\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","ENT\n","model error: 0.24163969795037754\n","stump error: 0.5544768069039914\n","error difference: -0.31283710895361383\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CMS\n","model error: 0.29243353783231085\n","stump error: 0.5603271983640081\n","error difference: -0.2678936605316973\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime native american\n","model error: 0.17142857142857137\n","stump error: 0.4285714285714286\n","error difference: -0.25714285714285723\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","local govt worker\n","model error: 0.194639175257732\n","stump error: 0.385979381443299\n","error difference: -0.19134020618556702\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single native american women\n","model error: 0.09523809523809523\n","stump error: 0.2698412698412699\n","error difference: -0.17460317460317465\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","LGL\n","model error: 0.23076923076923073\n","stump error: 0.40384615384615385\n","error difference: -0.17307692307692313\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime white\n","model error: 0.1831470335339639\n","stump error: 0.352536543422184\n","error difference: -0.1693895098882201\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","EXT\n","model error: 0.25\n","stump error: 0.41666666666666663\n","error difference: -0.16666666666666663\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","part time\n","model error: 0.10673024023473321\n","stump error: 0.2684760682193288\n","error difference: -0.16174582798459558\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","RPR\n","model error: 0.27326440177252587\n","stump error: 0.431314623338257\n","error difference: -0.15805022156573112\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","non profit worker\n","model error: 0.1865234375\n","stump error: 0.33642578125\n","error difference: -0.14990234375\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","oceanian\n","model error: 0.21111111111111114\n","stump error: 0.3555555555555555\n","error difference: -0.14444444444444438\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","over time\n","model error: 0.2029478458049887\n","stump error: 0.3378684807256236\n","error difference: -0.13492063492063489\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","MIL\n","model error: 0.1826923076923077\n","stump error: 0.3173076923076923\n","error difference: -0.13461538461538458\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","SCI\n","model error: 0.19075144508670516\n","stump error: 0.32369942196531787\n","error difference: -0.1329479768786127\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","associates degree\n","model error: 0.2318238471125883\n","stump error: 0.36352305774823435\n","error difference: -0.13169921063564605\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","cow minorities\n","model error: 0.21148415437715717\n","stump error: 0.342955757765924\n","error difference: -0.13147160338876684\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","bachelors degree\n","model error: 0.21492792921364345\n","stump error: 0.3462252033680605\n","error difference: -0.13129727415441705\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","SAL\n","model error: 0.19096509240246407\n","stump error: 0.3189596167008898\n","error difference: -0.12799452429842573\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","natie american women\n","model error: 0.18497109826589597\n","stump error: 0.3121387283236994\n","error difference: -0.1271676300578034\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","black men\n","model error: 0.20341614906832295\n","stump error: 0.327639751552795\n","error difference: -0.12422360248447206\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime black\n","model error: 0.20731707317073167\n","stump error: 0.3292682926829268\n","error difference: -0.12195121951219512\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","twenties\n","model error: 0.10664538595473017\n","stump error: 0.22576900754497964\n","error difference: -0.11912362159024947\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single white women\n","model error: 0.13270676691729322\n","stump error: 0.25037593984962403\n","error difference: -0.11766917293233081\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single asian women\n","model error: 0.17861975642760486\n","stump error: 0.29364005412719896\n","error difference: -0.1150202976995941\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","native american\n","model error: 0.17528735632183912\n","stump error: 0.29022988505747127\n","error difference: -0.11494252873563215\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","US born\n","model error: 0.18503937007874016\n","stump error: 0.2994144962648899\n","error difference: -0.11437512618614976\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","european\n","model error: 0.18652849740932642\n","stump error: 0.30051813471502586\n","error difference: -0.11398963730569944\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","BUS\n","model error: 0.20969245107176138\n","stump error: 0.32339235787511644\n","error difference: -0.11369990680335507\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","white men\n","model error: 0.18994933305759487\n","stump error: 0.30255402750491156\n","error difference: -0.11260469444731669\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","self employed worker\n","model error: 0.24286815728604472\n","stump error: 0.35389360061680797\n","error difference: -0.11102544333076325\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single women\n","model error: 0.13267243197637624\n","stump error: 0.24277578569921954\n","error difference: -0.1101033537228433\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","black\n","model error: 0.20092024539877296\n","stump error: 0.30828220858895705\n","error difference: -0.1073619631901841\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","white\n","model error: 0.18734566207539927\n","stump error: 0.29457279262470504\n","error difference: -0.10722713054930577\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","married men\n","model error: 0.21121562090725088\n","stump error: 0.3168234313608763\n","error difference: -0.10560781045362544\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged white\n","model error: 0.21049933353839845\n","stump error: 0.31518507126012507\n","error difference: -0.10468573772172662\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","native american men\n","model error: 0.1657142857142857\n","stump error: 0.26857142857142857\n","error difference: -0.10285714285714287\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","white women\n","model error: 0.1844013096351731\n","stump error: 0.2855472404115996\n","error difference: -0.10114593077642653\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","married\n","model error: 0.20583436663838672\n","stump error: 0.3052274358807022\n","error difference: -0.0993930692423155\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged native american\n","model error: 0.2325581395348837\n","stump error: 0.33139534883720934\n","error difference: -0.09883720930232565\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single\n","model error: 0.14089848308051345\n","stump error: 0.23842862699338774\n","error difference: -0.09753014391287429\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","male\n","model error: 0.19039875105704807\n","stump error: 0.28660638782280623\n","error difference: -0.09620763676575816\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","high school grad\n","model error: 0.1817295188556567\n","stump error: 0.27673927178153446\n","error difference: -0.09500975292587777\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","Total\n","model error: 0.1851788756388416\n","stump error: 0.2796252129471891\n","error difference: -0.0944463373083475\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","other race\n","model error: 0.17191489361702128\n","stump error: 0.26553191489361705\n","error difference: -0.09361702127659577\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","FIN\n","model error: 0.2627627627627628\n","stump error: 0.3558558558558559\n","error difference: -0.0930930930930931\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","female\n","model error: 0.17943764756385494\n","stump error: 0.2719467696930672\n","error difference: -0.09250912212921225\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","fifties\n","model error: 0.21246411079209593\n","stump error: 0.30434048302651584\n","error difference: -0.09187637223441991\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","married women\n","model error: 0.1993067590987868\n","stump error: 0.29116117850953205\n","error difference: -0.09185441941074524\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","black women\n","model error: 0.1984848484848485\n","stump error: 0.2893939393939394\n","error difference: -0.09090909090909094\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","forties\n","model error: 0.1923549009293355\n","stump error: 0.28195686480799576\n","error difference: -0.08960196387866026\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single black women\n","model error: 0.14984709480122327\n","stump error: 0.23853211009174313\n","error difference: -0.08868501529051986\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced women\n","model error: 0.23463687150837986\n","stump error: 0.3230912476722533\n","error difference: -0.08845437616387342\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single men\n","model error: 0.14793433158939207\n","stump error: 0.234710445607072\n","error difference: -0.08677611401767993\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","thirties\n","model error: 0.1955307262569832\n","stump error: 0.2807661612130886\n","error difference: -0.08523543495610542\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","sixties\n","model error: 0.23776610450649704\n","stump error: 0.3184959911528892\n","error difference: -0.08072988664639213\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","MGR\n","model error: 0.20781696854146803\n","stump error: 0.28852875754687\n","error difference: -0.08071178900540199\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CON\n","model error: 0.24189723320158107\n","stump error: 0.32252964426877473\n","error difference: -0.08063241106719365\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian_women\n","model error: 0.19291014014839236\n","stump error: 0.2732893652102226\n","error difference: -0.08037922506183026\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian men\n","model error: 0.1868962620747585\n","stump error: 0.2654346913061739\n","error difference: -0.07853842923141541\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","full time\n","model error: 0.2030904079880721\n","stump error: 0.2777300862964803\n","error difference: -0.07463967830840823\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","elderly\n","model error: 0.273109243697479\n","stump error: 0.34663865546218486\n","error difference: -0.07352941176470584\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","advanced degree\n","model error: 0.16821777570963237\n","stump error: 0.23825034899953468\n","error difference: -0.07003257328990231\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced\n","model error: 0.2278860569715142\n","stump error: 0.29760119940029983\n","error difference: -0.06971514242878563\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged black\n","model error: 0.25113464447806355\n","stump error: 0.3207261724659607\n","error difference: -0.06959152798789714\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","for profit worker\n","model error: 0.1716066191872644\n","stump error: 0.23936950146627567\n","error difference: -0.06776288227901128\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian\n","model error: 0.2030893897189162\n","stump error: 0.2699417574069385\n","error difference: -0.06685236768802227\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian\n","model error: 0.2030893897189162\n","stump error: 0.2699417574069385\n","error difference: -0.06685236768802227\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged asian\n","model error: 0.20376055257099002\n","stump error: 0.2705295471987721\n","error difference: -0.06676899462778207\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced white women\n","model error: 0.2198198198198198\n","stump error: 0.2828828828828829\n","error difference: -0.06306306306306309\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","african\n","model error: 0.18840579710144922\n","stump error: 0.25120772946859904\n","error difference: -0.06280193236714982\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced asian women\n","model error: 0.23469387755102045\n","stump error: 0.29591836734693877\n","error difference: -0.061224489795918324\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","PRD\n","model error: 0.19192688499619193\n","stump error: 0.2490479817212491\n","error difference: -0.05712109672505716\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","mar minorities\n","model error: 0.20837209302325577\n","stump error: 0.2641860465116279\n","error difference: -0.05581395348837215\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced_native_american_women\n","model error: 0.2777777777777778\n","stump error: 0.33333333333333337\n","error difference: -0.05555555555555558\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime asian\n","model error: 0.2449799196787149\n","stump error: 0.29718875502008035\n","error difference: -0.052208835341365445\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced black women\n","model error: 0.32098765432098764\n","stump error: 0.37037037037037035\n","error difference: -0.04938271604938271\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","multiple race\n","model error: 0.1669052390495276\n","stump error: 0.20870312052676787\n","error difference: -0.041797881477240284\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CMM\n","model error: 0.14674441205053446\n","stump error: 0.1885325558794947\n","error difference: -0.041788143828960234\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","ENG\n","model error: 0.14204545454545459\n","stump error: 0.18181818181818177\n","error difference: -0.03977272727272718\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","OFF\n","model error: 0.22432762836185816\n","stump error: 0.2634474327628362\n","error difference: -0.039119804400978064\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","TRN\n","model error: 0.17830540037243947\n","stump error: 0.2094972067039106\n","error difference: -0.031191806331471117\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","american\n","model error: 0.16924778761061943\n","stump error: 0.1975663716814159\n","error difference: -0.02831858407079646\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","MED\n","model error: 0.21038790269559504\n","stump error: 0.23668639053254437\n","error difference: -0.026298487836949325\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","some school\n","model error: 0.12393465909090906\n","stump error: 0.14666193181818177\n","error difference: -0.022727272727272707\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","PRS\n","model error: 0.09820485744456176\n","stump error: 0.11087645195353746\n","error difference: -0.012671594508975703\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","FFF\n","model error: 0.04134366925064603\n","stump error: 0.05167958656330751\n","error difference: -0.01033591731266148\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","no school\n","model error: 0.1216216216216216\n","stump error: 0.12355212355212353\n","error difference: -0.0019305019305019266\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CLN\n","model error: 0.11742777260018644\n","stump error: 0.11742777260018644\n","error difference: 0.0\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","EAT\n","model error: 0.059175531914893664\n","stump error: 0.05452127659574468\n","error difference: 0.004654255319148981\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","HLS\n","model error: 0.13844621513944222\n","stump error: 0.12151394422310757\n","error difference: 0.016932270916334646\n","~~~~~~~~~~~~~~~~~"],"metadata":{"id":"71py6mGC_hSa"}},{"cell_type":"markdown","source":["## Part 2"],"metadata":{"id":"88ppdOlH_6Vr"}},{"cell_type":"markdown","source":["2. Say instead you used bootstrapped fairness to postprocess equal error rates on the initial model over the groups you discovered (assuming you had a way to identify those groups ahead of time). How much would you need to inflate each groups' error to get them equal?"],"metadata":{"id":"r54z2DSkrfmV"}},{"cell_type":"markdown","source":["The group that I identified with the highest group error from the initial decision stump model were the people who have occupations in the field of education (group name EDU). It had a group error of 0.5546695662679715. This means that I would have to inflate each identified group's error by the following amounts detailed below."],"metadata":{"id":"0xXZ0PIiAhD_"}},{"cell_type":"markdown","source":["~~~~~~~~~~~~~~~~~\n","EAT\n","error inflation: 0.5007167229947399\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","FFF\n","error inflation: 0.4906611717244248\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","HLS\n","error inflation: 0.4520302700802882\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CLN\n","error inflation: 0.4428313747153243\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","PRS\n","error inflation: 0.4385375128846889\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","some school\n","error inflation: 0.41435534910211747\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","no school\n","error inflation: 0.4137132253116306\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","ENG\n","error inflation: 0.3616655773143047\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","TRN\n","error inflation: 0.35570397623208383\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","american\n","error inflation: 0.35207376313216554\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","multiple race\n","error inflation: 0.3422144312009102\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CMM\n","error inflation: 0.3402317880601139\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single native american women\n","error inflation: 0.32983735150287086\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","twenties\n","error inflation: 0.3219383635506641\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","advanced degree\n","error inflation: 0.3138390486635446\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single men\n","error inflation: 0.31304008033198705\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced_native_american_women\n","error inflation: 0.31076712724358124\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","MED\n","error inflation: 0.30895289263654646\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single\n","error inflation: 0.3083417411970317\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","for profit worker\n","error inflation: 0.3062342801568294\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single women\n","error inflation: 0.3028752269008429\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single white women\n","error inflation: 0.2988960768658835\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","natie american women\n","error inflation: 0.29652420285945014\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","mar minorities\n","error inflation: 0.2918779166568376\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","native american\n","error inflation: 0.2907223557100831\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single black women\n","error inflation: 0.290140508151739\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged asian\n","error inflation: 0.29008243935798805\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","PRD\n","error inflation: 0.28774184559575744\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian\n","error inflation: 0.2869215262571577\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian\n","error inflation: 0.2869215262571577\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","part time\n","error inflation: 0.28686397446190515\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","native american men\n","error inflation: 0.2853945374992718\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced asian women\n","error inflation: 0.28509462219638315\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian men\n","error inflation: 0.28282064224513437\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian_women\n","error inflation: 0.28168353901249454\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","female\n","error inflation: 0.2803284893948964\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","MGR\n","error inflation: 0.27940378777247965\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime native american\n","error inflation: 0.27880749730245424\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime asian\n","error inflation: 0.27862789960130485\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","OFF\n","error inflation: 0.2745210223888914\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","Total\n","error inflation: 0.27122282113786433\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","full time\n","error inflation: 0.2708747033764912\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","high school grad\n","error inflation: 0.27070010061911653\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","married women\n","error inflation: 0.26727418523468216\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","white women\n","error inflation: 0.2663597286172005\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","forties\n","error inflation: 0.26615126638603526\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","thirties\n","error inflation: 0.2658406626134864\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","male\n","error inflation: 0.26309101172138605\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged native american\n","error inflation: 0.2621359065372493\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","other race\n","error inflation: 0.2617803705482614\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single asian women\n","error inflation: 0.2596755257435376\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced white women\n","error inflation: 0.2576398632976744\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","white\n","error inflation: 0.2566190980733253\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","MIL\n","error inflation: 0.2561347677331729\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","black women\n","error inflation: 0.25375870094591557\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","fifties\n","error inflation: 0.25134105686131447\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","US born\n","error inflation: 0.2509081023468779\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","black\n","error inflation: 0.25077433456884457\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","african\n","error inflation: 0.25032174018101494\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","married\n","error inflation: 0.25007987773103924\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","white men\n","error inflation: 0.24818561641227954\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","black men\n","error inflation: 0.24759114850253083\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","european\n","error inflation: 0.24731261803908866\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced\n","error inflation: 0.24196834282946533\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","oceanian\n","error inflation: 0.23724713190042968\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged white\n","error inflation: 0.2371150655647366\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","sixties\n","error inflation: 0.23663991538322737\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","married men\n","error inflation: 0.236398987457637\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","FIN\n","error inflation: 0.23436928475405217\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime black\n","error inflation: 0.22902115128238065\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","over time\n","error inflation: 0.2262945662679715\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CON\n","error inflation: 0.2189356842377932\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","elderly\n","error inflation: 0.2186448690464089\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","SAL\n","error inflation: 0.21858241902422237\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","BUS\n","error inflation: 0.21678769254088393\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","self employed worker\n","error inflation: 0.21635726478286543\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced women\n","error inflation: 0.21536588343440333\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","non profit worker\n","error inflation: 0.21502730337279674\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","cow minorities\n","error inflation: 0.21368379054848752\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged black\n","error inflation: 0.21213345069814804\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","SCI\n","error inflation: 0.21062194722035243\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime white\n","error inflation: 0.20873558138875337\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","bachelors degree\n","error inflation: 0.20251886368467398\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","associates degree\n","error inflation: 0.19902903089512258\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","local govt worker\n","error inflation: 0.17178215872528135\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced black women\n","error inflation: 0.15284308224970666\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","RPR\n","error inflation: 0.12855062083572133\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","LGL\n","error inflation: 0.11668507014394047\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","EXT\n","error inflation: 0.1011811941749482\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","PRT\n","error inflation: 0.039958206863874635\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","ENT\n","error inflation: 0.023412614098894724\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CMS\n","error inflation: 0.010802259415068738\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","EDU\n","error inflation: 0.0\n","~~~~~~~~~~~~~~~~~"],"metadata":{"id":"mR3fI0qkBUTJ"}},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"lLaQNLYxyo2c"},"source":["# Data Exploration\n","\n","In order to find promising groups, you may find it helpful to do some data exploration. Please include any code or visualizations that you did to do so here. To get you started, here are some things you may find useful:"]},{"cell_type":"markdown","metadata":{"id":"Af4yUzxDyo2c"},"source":["1. How to grab the predictions of the current PDL on the training data: because f.predict takes a single value as input, you have to use an apply function for this."]},{"cell_type":"code","source":["# decision stump to replicate our initial model\n","initial_model = DecisionTreeClassifier(max_depth = 1, random_state=0)\n","initial_model.fit(train_x.values, train_y) # added .values to train_x and this fixed a warning printout I was getting\n","stump = build_initial_pdl(initial_model, train_x, train_y, validation_x, validation_y) #bountyHuntWrapper.build_initial_pdl(initial_model, train_x, train_y, validation_x, validation_y)"],"metadata":{"id":"g-9O7aSMwH4r","executionInfo":{"status":"ok","timestamp":1648676189142,"user_tz":240,"elapsed":20669,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","execution_count":39,"metadata":{"pycharm":{"name":"#%%\n"},"id":"g1d2VR7cyo2d","executionInfo":{"status":"ok","timestamp":1648674888164,"user_tz":240,"elapsed":75129,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"outputs":[],"source":["# if loading the function from storage, then replace f with content since f refers to the model we trained (and content is the name we give to the model when we load it)\n","preds = train_x.apply(f.predict, axis=1)"]},{"cell_type":"code","source":["val_preds = validation_x.apply(f.predict, axis=1)"],"metadata":{"id":"3hkQeGMcqEm8","executionInfo":{"status":"ok","timestamp":1648674900581,"user_tz":240,"elapsed":12426,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rpPONVJYyo2d"},"source":["2. Getting the zero-one loss of a model restricted to a group you have defined."]},{"cell_type":"code","source":["# here are all the groups that successfully led to updates of the PDL (there are 28 of them)\n","f.pred_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U1HWS0qUsuxH","executionInfo":{"status":"ok","timestamp":1648675210618,"user_tz":240,"elapsed":140,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}},"outputId":"8a6dc100-fe3d-467a-e0c3-8c9ea2dba57a"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Total',\n"," 'EDU',\n"," 'CMS',\n"," 'ENT',\n"," 'PRT',\n"," 'LGL',\n"," 'RPR',\n"," 'local govt worker',\n"," 'associates degree',\n"," 'bachelors degree',\n"," 'SCI',\n"," 'cow minorities',\n"," 'non profit worker',\n"," 'self employed worker',\n"," 'BUS',\n"," 'SAL',\n"," 'CON',\n"," 'over time',\n"," 'FIN',\n"," 'oceanian',\n"," 'black',\n"," 'US born',\n"," 'MIL',\n"," 'white',\n"," 'TRN',\n"," 'PRS',\n"," 'CLN',\n"," 'FFF',\n"," 'EAT']"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["# get all the group functions and their names into a single list\n","groups = []\n","for func, name, _ in ranking:\n","  groups.append({'function': func, 'name': name})\n","for func, name in manual_functions:\n","  groups.append({'function': func, 'name': name})"],"metadata":{"id":"66D5zqH_sAJc","executionInfo":{"status":"ok","timestamp":1648676255404,"user_tz":240,"elapsed":123,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["# TRAIN\n","# get the group based breakdown of the current model error, initial model error, and their differences\n","error_data_train = []\n","for d in groups:\n","  model_error = measure_group_error(f, d['function'], train_x, train_y)\n","  stump_error = measure_group_error(stump, d['function'], train_x, train_y)\n","  error_diff = model_error - stump_error\n","  error_data_train.append({'function': d['function'], 'name': d['name'], 'model_error': model_error, 'stump_error': stump_error, 'error_diff': error_diff})\n","\n","error_data_train"],"metadata":{"id":"pc8sdvkdsEmW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VALIDATION\n","# do the same for validation\n","error_data_val = []\n","for d in groups:\n","  model_error = measure_group_error(f, d['function'], validation_x, validation_y)\n","  stump_error = measure_group_error(stump, d['function'], validation_x, validation_y)\n","  error_diff = model_error - stump_error\n","  error_data_val.append({'function': d['function'], 'name': d['name'], 'model_error': model_error, 'stump_error': stump_error, 'error_diff': error_diff})\n","\n","error_data_val"],"metadata":{"id":"k3CH2oblvcFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# total error of our model\n","g = lambda x: 1 #here we define a group that just is all the data, replace as you see fit.\n","\n","model_error_train_total = measure_group_error(f, g, train_x, train_y)\n","stump_error_train_total = measure_group_error(stump, g, train_x, train_y)\n","model_error_val_total = measure_group_error(f, g, validation_x, validation_y)\n","stump_error_val_total = measure_group_error(stump, g, validation_x, validation_y)\n","\n","error_data_train.append({'function': g, 'name': 'Total', 'model_error': model_error_train_total, 'stump_error': stump_error_train_total, 'error_diff': model_error_train_total - stump_error_train_total})\n","error_data_val.append({'function': g, 'name': 'Total', 'model_error': model_error_val_total, 'stump_error': stump_error_val_total, 'error_diff': model_error_val_total - stump_error_val_total})"],"metadata":{"id":"cdBHef0vxrp3","executionInfo":{"status":"ok","timestamp":1648678299921,"user_tz":240,"elapsed":160,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["error_data_tr = [{'name': d['name'], 'model_error': d['model_error'], 'stump_error': d['stump_error'], 'error_diff': d['error_diff']} for d in error_data_train]\n","error_data_v = [{'name': d['name'], 'model_error': d['model_error'], 'stump_error': d['stump_error'], 'error_diff': d['error_diff']} for d in error_data_val]              "],"metadata":{"id":"ScY_S7IW7Iz1","executionInfo":{"status":"ok","timestamp":1648679177963,"user_tz":240,"elapsed":164,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# write these to file for future reference\n","\n","with open('train_data_errors.json', 'w') as f1:\n","  json.dump(error_data_tr, f1)\n","\n","with open('validation_data_errors.json', 'w') as f2:\n","  json.dump(error_data_v, f2)"],"metadata":{"id":"n0RjwYm4vFgw","executionInfo":{"status":"ok","timestamp":1648679187047,"user_tz":240,"elapsed":133,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["error_data_tr.sort(key= lambda d: d['error_diff'])\n","for d in error_data_tr:\n","  print('~~~~~~~~~~~~~~~~~')\n","  print(d['name'])\n","  print('model error: ' + str(d['model_error']))\n","  print('stump error: ' + str(d['stump_error']))\n","  print('error difference: ' + str(d['error_diff']))\n","  print('~~~~~~~~~~~~~~~~~')"],"metadata":{"id":"azgzRwcl8tEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["error_data_v.sort(key= lambda d: d['error_diff'])\n","for d in error_data_v:\n","  print('~~~~~~~~~~~~~~~~~')\n","  print(d['name'])\n","  print('model error: ' + str(d['model_error']))\n","  print('stump error: ' + str(d['stump_error']))\n","  print('error difference: ' + str(d['error_diff']))\n","  print('~~~~~~~~~~~~~~~~~')"],"metadata":{"id":"29X5aFEg9R9K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["error_data_tr.sort(key= lambda d: d['stump_error'])"],"metadata":{"id":"EtnKvXfFBZ1E","executionInfo":{"status":"ok","timestamp":1648680680292,"user_tz":240,"elapsed":159,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["error_data_tr[len(error_data_tr)-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czO9Yv4HBsJ8","executionInfo":{"status":"ok","timestamp":1648680733802,"user_tz":240,"elapsed":135,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}},"outputId":"6afcd0fc-77e6-4b4c-d35b-559f6bcfa87e"},"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'error_diff': -0.419113205267609,\n"," 'model_error': 0.1355563610003625,\n"," 'name': 'EDU',\n"," 'stump_error': 0.5546695662679715}"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["for d in error_data_tr:\n","  print('~~~~~~~~~~~~~~~~~')\n","  print(d['name'])\n","  print('error inflation: ' + str(0.5546695662679715 - d['stump_error']))\n","  print('~~~~~~~~~~~~~~~~~')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6dkuLFfKBl_A","executionInfo":{"status":"ok","timestamp":1648680762343,"user_tz":240,"elapsed":140,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}},"outputId":"7d4ba61e-69e5-4ee3-e7ac-fc9116f3df36"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["~~~~~~~~~~~~~~~~~\n","EAT\n","error inflation: 0.5007167229947399\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","FFF\n","error inflation: 0.4906611717244248\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","HLS\n","error inflation: 0.4520302700802882\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CLN\n","error inflation: 0.4428313747153243\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","PRS\n","error inflation: 0.4385375128846889\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","some school\n","error inflation: 0.41435534910211747\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","no school\n","error inflation: 0.4137132253116306\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","ENG\n","error inflation: 0.3616655773143047\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","TRN\n","error inflation: 0.35570397623208383\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","american\n","error inflation: 0.35207376313216554\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","multiple race\n","error inflation: 0.3422144312009102\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CMM\n","error inflation: 0.3402317880601139\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single native american women\n","error inflation: 0.32983735150287086\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","twenties\n","error inflation: 0.3219383635506641\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","advanced degree\n","error inflation: 0.3138390486635446\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single men\n","error inflation: 0.31304008033198705\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced_native_american_women\n","error inflation: 0.31076712724358124\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","MED\n","error inflation: 0.30895289263654646\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single\n","error inflation: 0.3083417411970317\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","for profit worker\n","error inflation: 0.3062342801568294\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single women\n","error inflation: 0.3028752269008429\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single white women\n","error inflation: 0.2988960768658835\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","natie american women\n","error inflation: 0.29652420285945014\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","mar minorities\n","error inflation: 0.2918779166568376\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","native american\n","error inflation: 0.2907223557100831\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single black women\n","error inflation: 0.290140508151739\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged asian\n","error inflation: 0.29008243935798805\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","PRD\n","error inflation: 0.28774184559575744\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian\n","error inflation: 0.2869215262571577\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian\n","error inflation: 0.2869215262571577\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","part time\n","error inflation: 0.28686397446190515\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","native american men\n","error inflation: 0.2853945374992718\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced asian women\n","error inflation: 0.28509462219638315\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian men\n","error inflation: 0.28282064224513437\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","asian_women\n","error inflation: 0.28168353901249454\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","female\n","error inflation: 0.2803284893948964\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","MGR\n","error inflation: 0.27940378777247965\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime native american\n","error inflation: 0.27880749730245424\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime asian\n","error inflation: 0.27862789960130485\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","OFF\n","error inflation: 0.2745210223888914\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","Total\n","error inflation: 0.27122282113786433\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","full time\n","error inflation: 0.2708747033764912\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","high school grad\n","error inflation: 0.27070010061911653\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","married women\n","error inflation: 0.26727418523468216\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","white women\n","error inflation: 0.2663597286172005\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","forties\n","error inflation: 0.26615126638603526\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","thirties\n","error inflation: 0.2658406626134864\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","male\n","error inflation: 0.26309101172138605\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged native american\n","error inflation: 0.2621359065372493\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","other race\n","error inflation: 0.2617803705482614\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","single asian women\n","error inflation: 0.2596755257435376\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced white women\n","error inflation: 0.2576398632976744\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","white\n","error inflation: 0.2566190980733253\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","MIL\n","error inflation: 0.2561347677331729\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","black women\n","error inflation: 0.25375870094591557\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","fifties\n","error inflation: 0.25134105686131447\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","US born\n","error inflation: 0.2509081023468779\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","black\n","error inflation: 0.25077433456884457\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","african\n","error inflation: 0.25032174018101494\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","married\n","error inflation: 0.25007987773103924\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","white men\n","error inflation: 0.24818561641227954\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","black men\n","error inflation: 0.24759114850253083\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","european\n","error inflation: 0.24731261803908866\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced\n","error inflation: 0.24196834282946533\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","oceanian\n","error inflation: 0.23724713190042968\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged white\n","error inflation: 0.2371150655647366\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","sixties\n","error inflation: 0.23663991538322737\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","married men\n","error inflation: 0.236398987457637\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","FIN\n","error inflation: 0.23436928475405217\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime black\n","error inflation: 0.22902115128238065\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","over time\n","error inflation: 0.2262945662679715\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CON\n","error inflation: 0.2189356842377932\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","elderly\n","error inflation: 0.2186448690464089\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","SAL\n","error inflation: 0.21858241902422237\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","BUS\n","error inflation: 0.21678769254088393\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","self employed worker\n","error inflation: 0.21635726478286543\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced women\n","error inflation: 0.21536588343440333\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","non profit worker\n","error inflation: 0.21502730337279674\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","cow minorities\n","error inflation: 0.21368379054848752\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","middle aged black\n","error inflation: 0.21213345069814804\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","SCI\n","error inflation: 0.21062194722035243\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","overtime white\n","error inflation: 0.20873558138875337\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","bachelors degree\n","error inflation: 0.20251886368467398\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","associates degree\n","error inflation: 0.19902903089512258\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","local govt worker\n","error inflation: 0.17178215872528135\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","divorced black women\n","error inflation: 0.15284308224970666\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","RPR\n","error inflation: 0.12855062083572133\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","LGL\n","error inflation: 0.11668507014394047\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","EXT\n","error inflation: 0.1011811941749482\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","PRT\n","error inflation: 0.039958206863874635\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","ENT\n","error inflation: 0.023412614098894724\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","CMS\n","error inflation: 0.010802259415068738\n","~~~~~~~~~~~~~~~~~\n","~~~~~~~~~~~~~~~~~\n","EDU\n","error inflation: 0.0\n","~~~~~~~~~~~~~~~~~\n"]}]},{"cell_type":"code","source":["'''\n","# get errors from initial model\n","preds = train_x.apply(f.predict, axis=1)\n","merged_train = train_x.copy()\n","merged_train['train_y'] = train_y\n","merged_train['preds'] = preds\n","# get all the values that we currently mislabel\n","mistakes = merged_train[merged_train['preds'] != merged_train['train_y']]\n","xs = mistakes.drop(columns=['train_y', 'preds'])\n","\n","categories1 = {\n","    #'AGEP': xs['AGEP'].unique(), # self-defined categories\n","    'COW': xs['COW'].unique(),\n","    #'SCHL': xs['SCHL'].unique(), # self-defined categories\n","    'MAR': xs['MAR'].unique(),\n","    #'OCCP': xs['OCCP'].unique(), # self-defined categories\n","    #'POBP': xs['POBP'].unique(), # self-defined categories\n","    #'WKHP': xs['WKHP'].unique(), # self-defined categories\n","    'SEX': xs['SEX'].unique(),\n","    'RAC1P': xs['RAC1P'].unique()\n","}\n","\n","initial_error_rates = {}\n","for key in categories1.keys():\n","  for val in categories1[key]:\n","    initial_error_rates[(key, val)] = {\n","        'train_x count' : len(train_x[train_x[key] == val]),\n","        'error count' : len(xs[xs[key] == val]),\n","        'group error rate' : len(xs[xs[key] == val]) / len(train_x[train_x[key] == val])}\n","\n","#initial_error_rates\n","'''"],"metadata":{"id":"M1rK_C-FXit5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"ytlJluh0yo2d"},"source":["3. You can view the training data by calling `train_x`. If you want to only view the data for a single group defined by your group function, you can run the following:"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"wugX6UtOyo2d"},"outputs":[],"source":["# replace g with whatever your group is\n","indices = train_x.apply(g, axis=1) == 1\n","xs = train_x[indices]\n","ys = train_y[indices]"]},{"cell_type":"markdown","metadata":{"id":"Rd6oP_Akyo2e"},"source":["4. Inspecting the existing PDL: The PDL is stored as an object, and tracks its training errors, validation set errors, and the group functions that are used in lists where the ith element is the group errors of all groups discovered so far on the ith node in the PDL. If you are more curious about the implementation, you can look at the model.py file in the codebase, which doesn't contain anything you can use to adaptively modify your code. (But lives in the same folder as the rest of the codebase just to make importing things easier)"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"R-_-igjwyo2e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647909345364,"user_tz":240,"elapsed":152,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}},"outputId":"0801993f-b04b-49c9-86eb-ac1fc634ac19"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.28344674513010715]]\n","[0.28344674513010715]\n","[[0.2796252129471891]]\n","[<function DecisionlistNode.__init__.<locals>.<lambda> at 0x7f6112a78560>]\n","[<bound method BaseDecisionTree.predict of DecisionTreeClassifier(max_depth=1, random_state=0)>]\n","['Total']\n"]}],"source":["# f is the current model\n","print(f.train_errors) # group errors on training set.\n","print(f.train_errors[0]) # this is the group error of each group on the initial PDL. The ith element of f.train_errors is the group error of each group on the ith version of the PDL.\n","print(f.test_errors) # group errors on validation set\n","print(f.predicates) # all of the group functions that have been appended so far\n","print(f.leaves) # all of the h functions appended so far\n","print(f.pred_names) # the names you passed in for each of the group functions, to more easily understand which are which."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"YF_X2h1vyo2e"},"source":["5. Looking at the group error of the ith group over each round of updates: Say you found a group at round 5 and you want to know how its group error looked at previous or subsequent rounds. To do so, you can pull `f.train_errors` or `f.test_errors` and look at the ith element of each list as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"HURAx2gJyo2e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647909345365,"user_tz":240,"elapsed":6,"user":{"displayName":"Francesca Marini","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13526885581607466268"}},"outputId":"e5f4852b-85f6-4e3c-c18b-ec485606fef1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.28344674513010715]"]},"metadata":{},"execution_count":21}],"source":["target_group = 0 # this sets the group whose error you want to look at at each round to the initial model. If I wanted to look at the 1st group introduced, would change to a 1, e.g.\n","group_errs = [f.train_errors[i][target_group] for i in range(len(f.train_errors))]\n","group_errs"]},{"cell_type":"markdown","source":["# Submission File"],"metadata":{"id":"wIsEG54Sj-_n"}},{"cell_type":"markdown","source":["For details about my groups, please see the section of the notebook titled Bounty Hunting. It should be very straightforward how this was achieved. I generated groups, ordered them according to how the initial model performed on them (those with the highest error being considered first), and then passed them into the simple update function. I did this for single-variable constraints. I also additionally thought of some more complex intersectional groups that were likely to be treated unfairly and passed them into the simple update function as well. \n","\n","I have included the definition of the groups in the submissions file for completeness, but everything is really straightforwardly laid out in the notebook, and I don't have separate models h defined for each group since I was using simple update. "],"metadata":{"id":"gj2dw7N7oYtP"}},{"cell_type":"code","source":["[train_x, train_y, _, _] = get_data() #bountyHuntData.get_data()\n","\n","# Here, I define all of the g's tested that actually were integrated into my PDL\n","# possible group definitions\n","\n","# class of worker\n","def for_profit_worker(x):\n","  if x['COW'] == 1:\n","      return 1\n","  else:\n","      return 0\n","\n","def non_profit_worker(x):\n","  if x['COW'] == 2:\n","      return 1\n","  else:\n","      return 0\n","\n","def local_govt_worker(x):\n","  if x['COW'] == 3:\n","      return 1\n","  else:\n","      return 0\n","\n","def self_employed_worker(x):\n","  if x['COW'] == 6:\n","      return 1\n","  else:\n","      return 0\n","\n","def cow_minorities(x):\n","  if x['COW'] == 4:\n","      return 1\n","  elif x['COW'] == 5:\n","      return 1\n","  elif x['COW'] == 7:\n","      return 1\n","  elif x['COW'] == 8:\n","      return 1\n","  else:\n","      return 0\n","\n","# marital status\n","def married(x):\n","  if x['MAR'] == 1:\n","      return 1\n","  else:\n","      return 0\n","\n","def divorced(x):\n","  if x['MAR'] == 3:\n","      return 1\n","  else:\n","      return 0\n","\n","def mar_minorities(x):\n","  if x['MAR'] == 2:\n","      return 1\n","  elif x['MAR'] == 4:\n","      return 1\n","  else:\n","      return 0\n","\n","def single(x):\n","  if x['MAR'] == 5:\n","      return 1\n","  else:\n","      return 0\n","\n","# sex\n","def male(x):\n","  if x['SEX'] == 1:\n","      return 1\n","  else:\n","      return 0\n","\n","def female(x):\n","  if x['SEX'] == 2:\n","      return 1\n","  else:\n","      return 0\n","\n","# race\n","def white(x):\n","  if x['RAC1P'] == 1:\n","      return 1\n","  else:\n","      return 0\n","\n","def black(x):\n","  if x['RAC1P'] == 2:\n","      return 1\n","  else:\n","      return 0\n","\n","def other_race(x):\n","  if x['RAC1P'] == 9:\n","      return 1\n","  else:\n","      return 0\n","\n","def asian(x):\n","  if x['RAC1P'] == 6:\n","      return 1\n","  else:\n","      return 0\n","\n","def multiple_race(x):\n","  if x['RAC1P'] == 8:\n","      return 1\n","  else:\n","      return 0\n","\n","def native_american(x):\n","  if x['RAC1P'] == 3:\n","      return 1\n","  elif x['RAC1P'] == 4:\n","      return 1\n","  elif x['RAC1P'] == 5:\n","      return 1\n","  elif x['RAC1P'] == 7:\n","      return 1\n","  else:\n","      return 0\n","\n","# age groups\n","def twenties(x):\n","  if x['AGEP'] < 30:\n","      return 1\n","  else:\n","      return 0\n","\n","def thirties(x):\n","  if x['AGEP'] < 40 and x['AGEP'] >= 30:\n","      return 1\n","  else:\n","      return 0\n","\n","def forties(x):\n","  if x['AGEP'] < 50 and x['AGEP'] >= 40:\n","      return 1\n","  else:\n","      return 0\n","\n","def fifties(x):\n","  if x['AGEP'] < 60 and x['AGEP'] >= 50:\n","      return 1\n","  else:\n","      return 0\n","\n","def sixties(x):\n","  if x['AGEP'] < 70 and x['AGEP'] >= 60:\n","      return 1\n","  else:\n","      return 0\n","\n","def elderly(x):\n","  if x['AGEP'] >= 70:\n","      return 1\n","  else:\n","      return 0\n","\n","# education level\n","def no_school(x):\n","  if x['SCHL'] == 1:\n","    return 1\n","  else:\n","    return 0\n","\n","def some_school(x):\n","  if x['SCHL'] > 1 and x['SCHL'] <= 15:\n","    return 1\n","  else:\n","    return 0\n","\n","def high_school_grad(x):\n","  if x['SCHL'] > 15 and x['SCHL'] <= 19:\n","    return 1\n","  else:\n","    return 0\n","\n","def assoc_degree(x):\n","  if x['SCHL'] == 20:\n","    return 1\n","  else:\n","    return 0\n","\n","def assoc_degree(x):\n","  if x['SCHL'] == 20:\n","    return 1\n","  else:\n","    return 0\n","\n","def bachelor_degree(x):\n","  if x['SCHL'] == 21:\n","    return 1\n","  else:\n","    return 0\n","\n","def advanced_degree(x):\n","  if x['SCHL'] > 21:\n","    return 1\n","  else:\n","    return 0\n","\n","# work hours\n","def part_time(x):\n","  if x['WKHP'] < 30:\n","      return 1\n","  else:\n","      return 0\n","\n","def full_time(x):\n","  if x['WKHP'] < 60 and x['WKHP'] >= 30:\n","      return 1\n","  else:\n","      return 0\n","\n","def over_time(x):\n","  if x['WKHP'] >= 60:\n","      return 1\n","  else:\n","      return 0\n","\n","# occupation\n","def MGR(x):\n","  if x['OCCP'] <= 440:\n","    return 1\n","  else:\n","    return 0\n","\n","def BUS(x):\n","  if x['OCCP'] >= 500 and x['OCCP'] <= 750:\n","    return 1\n","  else:\n","    return 0\n","\n","def FIN(x):\n","  if x['OCCP'] >= 800 and x['OCCP'] <= 960:\n","    return 1\n","  else:\n","    return 0 \n","\n","def CMM(x):\n","  if x['OCCP'] >= 1005 and x['OCCP'] <= 1240:\n","    return 1\n","  else:\n","    return 0\n","\n","def ENG(x):\n","  if x['OCCP'] >= 1305 and x['OCCP'] <= 1560:\n","    return 1\n","  else:\n","    return 0\n","\n","def SCI(x):\n","  if x['OCCP'] >= 1600 and x['OCCP'] <= 1980:\n","    return 1\n","  else:\n","    return 0\n","\n","def CMS(x):\n","  if x['OCCP'] >= 2001 and x['OCCP'] <= 2060:\n","    return 1\n","  else:\n","    return 0\n","\n","def LGL(x):\n","  if x['OCCP'] >= 2105 and x['OCCP'] <= 2180:\n","    return 1\n","  else:\n","    return 0\n","\n","def EDU(x):\n","  if x['OCCP'] >= 2205 and x['OCCP'] <= 2555:\n","    return 1\n","  else:\n","    return 0\n","\n","def ENT(x):\n","  if x['OCCP'] >= 2600 and x['OCCP'] <= 2920:\n","    return 1\n","  else:\n","    return 0\n","\n","def MED(x):\n","  if x['OCCP'] >= 3000 and x['OCCP'] <= 3550:\n","    return 1\n","  else:\n","    return 0\n","\n","def HLS(x):\n","  if x['OCCP'] >= 3601 and x['OCCP'] <= 3655:\n","    return 1\n","  else:\n","    return 0\n","\n","def PRT(x):\n","  if x['OCCP'] >= 3700 and x['OCCP'] <= 3960:\n","    return 1\n","  else:\n","    return 0\n","\n","def EAT(x):\n","  if x['OCCP'] >= 4000 and x['OCCP'] <= 4160:\n","    return 1\n","  else:\n","    return 0\n","\n","def CLN(x):\n","  if x['OCCP'] >= 4200 and x['OCCP'] <= 4255:\n","    return 1\n","  else:\n","    return 0\n","\n","def PRS(x):\n","  if x['OCCP'] >= 4330 and x['OCCP'] <= 4655:\n","    return 1\n","  else:\n","    return 0\n","\n","def SAL(x):\n","  if x['OCCP'] >= 4700 and x['OCCP'] <= 4965:\n","    return 1\n","  else:\n","    return 0\n","\n","def OFF(x):\n","  if x['OCCP'] >= 5000 and x['OCCP'] <= 5940:\n","    return 1\n","  else:\n","    return 0\n","\n","def FFF(x):\n","  if x['OCCP'] >= 6005 and x['OCCP'] <= 6130:\n","    return 1\n","  else:\n","    return 0\n","\n","def CON(x):\n","  if x['OCCP'] >= 6200 and x['OCCP'] <= 6765:\n","    return 1\n","  else:\n","    return 0\n","\n","def EXT(x):\n","  if x['OCCP'] >= 6800 and x['OCCP'] <= 6950:\n","    return 1\n","  else:\n","    return 0\n","\n","def RPR(x):\n","  if x['OCCP'] >= 7000 and x['OCCP'] <= 7640:\n","    return 1\n","  else:\n","    return 0\n","\n","def PRD(x):\n","  if x['OCCP'] >= 7700 and x['OCCP'] <= 8990:\n","    return 1\n","  else:\n","    return 0\n","\n","def TRN(x):\n","  if x['OCCP'] >= 9005 and x['OCCP'] <= 9760:\n","    return 1\n","  else:\n","    return 0\n","\n","def MIL(x):\n","  if x['OCCP'] >= 9800 and x['OCCP'] <= 9830:\n","    return 1\n","  else:\n","    return 0\n","\n","# place of birth continents\n","def us_born(x):\n","  if x['POBP'] < 100:\n","    return 1\n","  else: \n","    return 0\n","\n","def european(x):\n","  if x['POBP'] < 200 and x['POBP'] >= 100:\n","    return 1\n","  else: \n","    return 0\n","\n","def asian(x):\n","  if x['POBP'] < 300 and x['POBP'] >= 200:\n","    return 1\n","  else: \n","    return 0\n","\n","def american(x):\n","  if x['POBP'] < 400 and x['POBP'] >= 300:\n","    return 1\n","  else: \n","    return 0\n","\n","def african(x):\n","  if x['POBP'] < 500 and x['POBP'] >= 400:\n","    return 1\n","  else: \n","    return 0\n","\n","def oceanian(x):\n","  if x['POBP'] < 600 and x['POBP'] >= 500:\n","    return 1\n","  else: \n","    return 0\n","\n","# manually defined groups that tend to be systematically mistreated and would be educated guesses to test on the model\n","\n","def black_men(x):\n","  if x['RAC1P'] == 2 and x['SEX'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def black_women(x):\n","  if x['RAC1P'] == 2 and x['SEX'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def asian_men(x):\n","  if x['RAC1P'] == 6 and x['SEX'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def asian_women(x):\n","  if x['RAC1P'] == 6 and x['SEX'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def white_men(x):\n","  if x['RAC1P'] == 1 and x['SEX'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def white_women(x):\n","  if x['RAC1P'] == 1 and x['SEX'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def native_american_men(x):\n","  if x['RAC1P'] == 3 and x['SEX'] == 1:\n","      return 1\n","  elif x['RAC1P'] == 4 and x['SEX'] == 1:\n","      return 1\n","  elif x['RAC1P'] == 5 and x['SEX'] == 1:\n","      return 1\n","  elif x['RAC1P'] == 7 and x['SEX'] == 1:\n","      return 1\n","  else:\n","      return 0\n","\n","def native_american_women(x):\n","  if x['RAC1P'] == 3 and x['SEX'] == 2:\n","      return 1\n","  elif x['RAC1P'] == 4 and x['SEX'] == 2:\n","      return 1\n","  elif x['RAC1P'] == 5 and x['SEX'] == 2:\n","      return 1\n","  elif x['RAC1P'] == 7 and x['SEX'] == 2:\n","      return 1\n","  else:\n","      return 0\n","\n","def middle_aged_black(x):\n","  if x['RAC1P'] == 2 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  else:\n","      return 0\n","\n","def middle_aged_asian(x):\n","  if x['RAC1P'] == 6 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  else:\n","      return 0\n","\n","def middle_aged_white(x):\n","  if x['RAC1P'] == 1 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  else:\n","      return 0\n","\n","def middle_aged_native_american(x):\n","  if x['RAC1P'] == 3 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  elif x['RAC1P'] == 4 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  elif x['RAC1P'] == 5 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  elif x['RAC1P'] == 7 and x['AGEP'] >= 40 and x['AGEP'] < 70:\n","      return 1\n","  else:\n","      return 0\n","\n","def overtime_white(x):\n","  if x['RAC1P'] == 1 and x['WKHP'] >= 60:\n","      return 1\n","  else:\n","      return 0\n","\n","def overtime_black(x):\n","  if x['RAC1P'] == 2 and x['WKHP'] >= 60:\n","      return 1\n","  else:\n","      return 0\n","\n","def overtime_asian(x):\n","  if x['RAC1P'] == 6 and x['WKHP'] >= 60:\n","      return 1\n","  else:\n","      return 0\n","\n","def overtime_native_american(x):\n","  if x['RAC1P'] == 3 and x['WKHP'] >= 60:\n","      return 1\n","  elif x['RAC1P'] == 4 and x['WKHP'] >= 60:\n","      return 1\n","  elif x['RAC1P'] == 5 and x['WKHP'] >= 60:\n","      return 1\n","  elif x['RAC1P'] == 7 and x['WKHP'] >= 60:\n","      return 1\n","  else:\n","      return 0\n","\n","def married_men(x):\n","  if x['MAR'] == 1 and x['SEX'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def married_women(x):\n","  if x['MAR'] == 1 and x['SEX'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def divorced_men(x):\n","  if x['MAR'] == 3 and x['SEX'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def divorced_women(x):\n","  if x['MAR'] == 3 and x['SEX'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def single_men(x):\n","  if x['MAR'] == 5 and x['SEX'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def single_women(x):\n","  if x['MAR'] == 5 and x['SEX'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def divorced_black_women(x):\n","  if x['MAR'] == 3 and x['SEX'] == 2 and x['RAC1P'] == 2:\n","    return 1\n","  else: \n","    return 0   \n","\n","def single_black_women(x):\n","  if x['MAR'] == 5 and x['SEX'] == 2 and x['RAC1P'] == 2:\n","    return 1\n","  else: \n","    return 0\n","\n","def divorced_white_women(x):\n","  if x['MAR'] == 3 and x['SEX'] == 2 and x['RAC1P'] == 1:\n","    return 1\n","  else: \n","    return 0   \n","\n","def single_white_women(x):\n","  if x['MAR'] == 5 and x['SEX'] == 2 and x['RAC1P'] == 1:\n","    return 1\n","  else: \n","    return 0\n","\n","def divorced_asian_women(x):\n","  if x['MAR'] == 3 and x['SEX'] == 2 and x['RAC1P'] == 6:\n","    return 1\n","  else: \n","    return 0   \n","\n","def single_asian_women(x):\n","  if x['MAR'] == 5 and x['SEX'] == 2 and x['RAC1P'] == 6:\n","    return 1\n","  else: \n","    return 0\n","\n","def divorced_native_american_women(x):\n","  if x['MAR'] == 3 and x['SEX'] == 2 and (x['RAC1P'] == 3 or x['RAC1P'] == 4 or x['RAC1P'] == 5 or x['RAC1P'] == 7):\n","    return 1\n","  else: \n","    return 0   \n","\n","def single_native_american_women(x):\n","  if x['MAR'] == 5 and x['SEX'] == 2 and (x['RAC1P'] == 3 or x['RAC1P'] == 4 or x['RAC1P'] == 5 or x['RAC1P'] == 7):\n","    return 1\n","  else: \n","    return 0\n","\n","# I used simple udate, so all of my h's followed this pattern\n","# h = build_model(train_x, train_y, g, dt_depth=10) #bountyHuntWrapper.build_model(train_x, train_y, g1, dt_depth=10)  # change this to be your first h"],"metadata":{"id":"x4ovkB17kAtd"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"19-Project.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}